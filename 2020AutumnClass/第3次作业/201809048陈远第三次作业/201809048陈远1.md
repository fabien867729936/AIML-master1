## 1，非线性回归 

### 1,激活函数 

所谓激活函数，就是在人工神经网络上运行的函数，负责将神经元的输入映射到输出。
激活函数它们将非线性特性引入带到网络中，在神经元中，输入经过加权求和，还要被作用于一个函数，就是激活函数，激活函数将神经网络赋予非线性的特性。如果没有激活函数，每一层网络乘以权值然后输出就相当于输入乘了一个矩阵，所以结果还是非线性的。
利用激活函数就会给神经网络增加非线性，可以让神经网络更加真实的逼近与非线性函数。

$$ 1.1 sigmoid函数 $$
sigmoid函数由于单增及其反函数单增的特性常用于神经网络的阀值函数公式如下：
f(x) = 1/(1+e(-x))
导数如下:
df(x)/dx = e(-x)/(1+e(-x)2) = f(x)(1-f(x))

sigmoid函数及其导数图像如下图所示：
![gjy14](gjy14.png)

函数特点：
优点：能把连续的输入值变换成0-1之间的值，可以引入非线性，容易求导。
缺点：1，当输入远离原点一定距离后，梯度会变得很小几乎为零。
2，函数输出不是以0为中心，导致权重更新效率变低。
3，sigmoid函数中有指数运算，对于较大的神经网络，会增加训练时间。

$$ 1.2 tanh函数 $$
tanh函数是双曲函数中的一个，tanh为双曲正切
函数公式如下：
f(x) = (ex-e(-x))/(ex+e(-x))
导数如下：
df(x)/dx = 2ex/(ex+e(-x))+f(x)2

tanh函数及其导数图像如下图所示：
![gjy16](gjy16.png)

tanh函数解决了sigmoid函数输出不是0均值的问题，但是没有解决梯度消失和指数运算的问题。

$$ 1.3 relu函数 $$
relu函数公式如下：
f(x) = max(0,x)
导数如下:
df(x)/dx = 0(x<=0) 1(x>0)

relu函数相比于sigmod函数和tanh函数，它有以下几个优点：
在输入为正数的时候，不存在梯度饱和问题。
计算速度要快很多。relu函数只有线性关系，不管是前向传播还是反向传播，都比sigmod和tanh要快很多。

缺点：
当输入是负数的时候，relu是完全不被激活的，这就表明一旦输入到了负数，relu就会死掉。这样在前向传播过程中，还不算什么问题，有的区域是敏感的，有的是不敏感的。但是到了反向传播过程中，输入负数，梯度就会完全到0，这个和sigmod函数、tanh函数有一样的问题。
relu函数的输出要么是0，要么是正数，这也就是说，ReLU函数也不是以0为中心的函数。
函数图像如下图所示：
![giy17](giy17.png)

$$ 1.4 elu函数 $$
elu函数公式如下：
f(x) = x(x>0) f(x) = a(ex-1)(x<=0)
导数如下所示：
df(x)/dx = 1(x>0) df(x)/dx = aex(x<=0)
elu函数是针对relu函数的一个改进型，相比于relu函数，在输入为负数的情况下，是有一定的输出的，而且这部分输出还具有一定的抗干扰能力。这样可以消除relu死掉的问题，不过还是有梯度饱和和指数运算的问题。
函数图像如下图所示：
![gjy18](gjy18.png)

$$ 1.5 leakyrelu函数 $$
leakyrelu函数公式如下：
f(x) = x(x=>0) f(x) = 1/a*x(x<0)
导数如下所示：
df(x)/dx = 1(x=>0) df(x)/dx = 1/a(x<0)
relu是将所有的负值都设为零，相反，leaky relu是给所有负值赋予一个非零斜率
函数图像如下图所示：
![gjy19](gjy19.png)

$$ 1.6 softplus函数 $$
softplus函数公式如下：
f(x) = log(1+ex)
导数如下所示：
df(x)/dx = ex/(1+ex)*lna(a是底数常数)
softplus可以看作是relu的平滑,而采用softplus激活函数，整个过程的计算量节省很多。
对于深层网络，softplus函数反向传播时，不容易会出现梯度消失的情况
softplus会使一部分神经元的输出为0，这样就造成了网络的稀疏性，并且减少了参数的相互依存关系，缓解了过拟合问题的发生.
函数图像如下图所示：
![gjy20](gjy20.png)

代码如下图所示：
import numpy as np

import matplotlib.pyplot as plt

from Activators.Relu import *(引入函数relu)

from Activators.Elu import *(引入函数elu)

from Activators.LeakyRelu import *(引入函数leakyrelu)

from Activators.Sigmoid import *(引入函数sigmoid)

from Activators.Softplus import *(引入函数softplus)

from Activators.Tanh import *(引入函数tanh)

def Draw(start,end,func,lable1,lable2):
    z = np.linspace(start, end, 200)(确定输入值z的范围及个数)
    a = func.forward(z)(利用前向传播联系a与z)
    da, dz = func.backward(z, a, 1)(利用反向传播联系导数与自身的关系)

    p1, = plt.plot(z,a)(表现z,a的变化趋势)
    p2, = plt.plot(z,da)(表现z,da的变化趋势)
    plt.legend([p1,p2], [lable1, lable2])
    plt.grid()(绘制网格)
    plt.xlabel("input : z")
    plt.ylabel("output : a")
    plt.title(lable1)
    plt.show()

if __name__ == '__main__':
    Draw(-7,7,CSigmoid(),"Sigmoid Function","Derivative of Sigmoid")(调用sigmoid函数画出sigmoid函数图像)
    Draw(-5,5,CTanh(),"tanh Function","Derivative of tanh")
    (调用tanh画出tanh函数图像)
    Draw(-5,5,CRelu(),"Relu Function","Derivative of Relu")
    (调用relu画出relu函数图像)
    Draw(-4,4,CElu(0.8),"ELU Function","Derivative of ELU")
    (调用elu画出elu函数图像)
    Draw(-5,5,CLeakyRelu(0.01),"Leaky Relu Function","Derivative of Leaky Relu")
    (调用leaky relu画出leaky relu函数图像)
    Draw(-5,5,CSoftplus(),"Softplus Function","Derivative of Softplus")
    (调用softplus画出softplus函数图像)

### 2，曲线拟合

### 用多项式回归法拟合正弦曲线

### 多项式回归的概念

多项式回归有几种形式：

#### 一元一次线性模型

因为只有一项，所以不能称为多项式了。它可以解决单变量的线性回归,其模型为：

$$z = x w + b \tag{1}$$

#### 多元一次多项式

多变量的线性回归，我们在第5章学习过相关内容。其模型为：

$$z = x_1 w_1 + x_2 w_2 + ...+ x_m w_m + b \tag{2}$$

这里的多变量，是指样本数据的特征值为多个，上式中的 $x_1,x_2,...,x_m$ 代表了m个特征值。

#### 一元多次多项式

一元多次项式用公式描述：

$$z = x w_1 + x^2 w_2 + ... + x^m w_m + b \tag{3}$$

上式中x是原有的唯一特征值，$x^m$ 是利用 $x$ 的 $m$ 次方作为额外的特征值，这样就把特征值的数量从 $1$ 个变为 $m$ 个。

换一种表达形式，令：$x_1 = x,x_2=x^2,\ldots,x_m=x^m$，则：

$$z = x_1 w_1 + x_2 w_2 + ... + x_m w_m + b \tag{4}$$

可以看到公式4和上面的公式2是一样的。

#### 多元多次多项式

多变量的非线性回归，其参数与特征组合繁复，但最终都可以归结为公式2和公式4的形式。

#### 正弦曲线的拟合

1.1隐层只有一个神经元的情况

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/9/sin_loss_1n.png" />

训练过程中损失函数值和准确率的变化

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/9/sin_result_1n.png" ch="500" />

一个神经元拟合效果

从图中可以看到损失值到0.04附近就很难下降了。从图中可以看到只有中间线性部分拟合了，两端的曲线部分没有拟合。

#### 隐层有两个神经元的情况

损失函数曲线和验证集精度曲线，都比较正常。而2个神经元的网络损失值可以达到0.004，少一个数量级。验证集精度到82%左右，而2个神经元的网络可以达到97%。

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/9/sin_loss_2n.png"/>

两个神经元的训练过程中损失函数值和准确率的变化

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/9/sin_result_2n.png"/>

两个神经元的拟合效果

### 代码如下：

class DataReaderEx(DataReader_1_3):
    def Add(self):
        X = self.XTrain[:,]**2(对x进行数组赋值)
        self.XTrain = np.hstack((self.XTrain, X))(将self.xtrain数组与x数组按水平方向进行叠加)
        X = self.XTrain[:,0:1]**3
        self.XTrain = np.hstack((self.XTrain, X))
        X = self.XTrain[:,0:1]**4
        self.XTrain = np.hstack((self.XTrain, X))


def ShowResult(net, dataReader, title):
    # draw train data
    X,Y = dataReader.XTrain, dataReader.YTrain
    plt.plot(X[:,0], Y[:,0], '.', c='b')
    # create and draw visualized validation data
    TX1 = np.linspace(0,1,100).reshape(100,1)
    TX = np.hstack((TX1, TX1[:,]**2))
    TX = np.hstack((TX, TX1[:,]**3))
    TX = np.hstack((TX, TX1[:,]**4))

    TY = net.inference(TX)
    plt.plot(TX1, TY, 'x', c='r')
    plt.title(title)
    plt.show()

### 复合函数的拟合

#### 隐层只有两个神经元的情况

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/9/complex_result_2n.png" ch="500" />

两个神经元的拟合效果

两个神经元的拟合效果图，拟合情况很不理想，和正弦曲线只用一个神经元的情况类似。

#### 隐层有三个神经元的情况

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/9/complex_loss_3n.png" />

三个神经元的训练过程中损失函数值和准确率的变化

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/9/complex_result_3n.png"/>

三个神经元的拟合效果

### 代码如下：

if __name__ == '__main__':
    dataReader = DataReader_2_0(train_data_name, test_data_name)
    dataReader.ReadData()
    (调用readdata函数读入数据)
    dataReader.GenerateValidationSet()
    (调用generatevalidationset函数进行数据验证)

    n_input, n_hidden, n_output = 1, 2, 1
    eta, batch_size, max_epoch = 0.05, 10, 5000
    eps = 0.001

    hp = HyperParameters_2_0(n_input, n_hidden, n_output, eta, max_epoch, batch_size, eps, NetType.Fitting, InitialMethod.Xavier)
    (超参数其中有输入，隐藏层，输出，每层神经元个数，学习速率，迭代次数)
    net = NeuralNet_2_0(hp, "sin_121")

    net.train(dataReader, 50, True)
    (网络训练)
    net.ShowTrainingHistory()
    ShowResult(net, dataReader, hp.toString())



## 2，非线性分类

### 非线性二分类

### 定义神经网络结构

非线性二分类的神经网络结构图如下所示：

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/10/xor_nn.png" />


- 输入层两个特征值$x_1,x_2$
  $$
  X=\begin{pmatrix}
    x_1 & x_2
  \end{pmatrix}
  $$
- 隐层$2\times 2$的权重矩阵$W1$
$$
  W1=\begin{pmatrix}
    w1_{11} & w1_{12} \\\\
    w1_{21} & w1_{22} 
  \end{pmatrix}
$$
- 隐层$1\times 2$的偏移矩阵$B1$

$$
  B1=\begin{pmatrix}
    b1_{1} & b1_{2}
  \end{pmatrix}
$$

- 隐层由两个神经元构成
$$
Z1=\begin{pmatrix}
  z1_{1} & z1_{2}
\end{pmatrix}
$$
$$
A1=\begin{pmatrix}
  a1_{1} & a1_{2}
\end{pmatrix}
$$
- 输出层$2\times 1$的权重矩阵$W2$
$$
  W2=\begin{pmatrix}
    w2_{11} \\\\
    w2_{21}  
  \end{pmatrix}
$$

- 输出层$1\times 1$的偏移矩阵$B2$

$$
  B2=\begin{pmatrix}
    b2_{1}
  \end{pmatrix}
$$

- 输出层有一个神经元使用Logistic函数进行分类
$$
  Z2=\begin{pmatrix}
    z2_{1}
  \end{pmatrix}
$$
$$
  A2=\begin{pmatrix}
    a2_{1}
  \end{pmatrix}
$$

一般的二分类的双层神经网络如下图所示：

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/10/binary_classifier.png" width="600" ch="500" />

输入特征值可以有很多，隐层单元也可以有很多，输出单元只有一个，且后面要接Logistic分类函数和二分类交叉熵损失函数。

### 前向计算

前向计算过程如下图所示：

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/10/binary_forward.png" />


#### 第一层

- 线性计算

$$
z1_{1} = x_{1} w1_{11} + x_{2} w1_{21} + b1_{1}
$$
$$
z1_{2} = x_{1} w1_{12} + x_{2} w1_{22} + b1_{2}
$$
$$
Z1 = X \cdot W1 + B1
$$

- 激活函数

$$
a1_{1} = Sigmoid(z1_{1})
$$
$$
a1_{2} = Sigmoid(z1_{2})
$$
$$
A1=\begin{pmatrix}
  a1_{1} & a1_{2}
\end{pmatrix}=Sigmoid(Z1)
$$

#### 第二层

- 线性计算

$$
z2_1 = a1_{1} w2_{11} + a1_{2} w2_{21} + b2_{1}
$$
$$
Z2 = A1 \cdot W2 + B2
$$

- 分类函数

$$a2_1 = Logistic(z2_1)$$
$$A2 = Logistic(Z2)$$

#### 损失函数

我们把异或问题归类成二分类问题，所以使用二分类交叉熵损失函数：

$$
loss = -Y \ln A2 + (1-Y) \ln (1-A2) \tag{12}
$$

在二分类问题中，$Y,A2$都是一个单一的数值，而非矩阵，但是为了前后统一，我们可以把它们看作是一个$1\times 1$的矩阵。

### 反向传播

反向传播的过程如下图所示：

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/10/binary_backward.png" />

#### 求损失函数对输出层的反向误差

对损失函数求导，可以得到损失函数对输出层的梯度值，上面图中的$Z2$部分。

根据公式12，求$A2$和$Z2$的导数：

$$
\begin{aligned}
\frac{\partial loss}{\partial Z2}&=\frac{\partial loss}{\partial A2}\frac{\partial A2}{\partial Z2} \\\\
&=\frac{A2-Y}{A2(1-A2)} \cdot A2(1-A2) \\\\
&=A2-Y \rightarrow dZ2
\end{aligned}
\tag{13}
$$

#### 求$W2$和$B2$的梯度

$$
\begin{aligned}
\frac{\partial loss}{\partial W2}&=\begin{pmatrix}
  \frac{\partial loss}{\partial w2_{11}} \\\\
  \frac{\partial loss}{\partial w2_{21}}
\end{pmatrix}
=\begin{pmatrix}
  \frac{\partial loss}{\partial Z2}\frac{\partial z2}{\partial w2_{11}} \\\\
  \frac{\partial loss}{\partial Z2}\frac{\partial z2}{\partial w2_{21}}
\end{pmatrix}
\\\\
&=\begin{pmatrix}
  dZ2 \cdot a1_{1} \\\\
  dZ2 \cdot a1_{2} 
\end{pmatrix}
=\begin{pmatrix}
  a1_{1} \\\\ a1_{2}
\end{pmatrix}dZ2
\\\\
&=A1^{\top} \cdot dZ2 \rightarrow dW2  
\end{aligned}
\tag{14}
$$
$$\frac{\partial{loss}}{\partial{B2}}=dZ2 \rightarrow dB2 \tag{15}$$

#### 求损失函数对隐层的反向误差

$$
\begin{aligned}  
\frac{\partial{loss}}{\partial{A1}} &= \begin{pmatrix}
  \frac{\partial loss}{\partial a1_{1}} & \frac{\partial loss}{\partial a1_{2}} 
\end{pmatrix}
\\\\
&=\begin{pmatrix}
\frac{\partial{loss}}{\partial{Z2}} \frac{\partial{Z2}}{\partial{a1_{1}}} & \frac{\partial{loss}}{\partial{Z2}}  \frac{\partial{Z2}}{\partial{a1_{2}}}  
\end{pmatrix}
\\\\
&=\begin{pmatrix}
dZ2 \cdot w2_{11} & dZ2 \cdot w2_{21}
\end{pmatrix}
\\\\
&=dZ2 \cdot \begin{pmatrix}
  w2_{11} & w2_{21}
\end{pmatrix}
\\\\
&=dZ2 \cdot W2^{\top}
\end{aligned}
\tag{16}
$$

$$
\frac{\partial A1}{\partial Z1}=A1 \odot (1-A1) \rightarrow dA1\tag{17}
$$

所以最后到达$Z1$的误差矩阵是：

$$
\begin{aligned}
\frac{\partial loss}{\partial Z1}&=\frac{\partial loss}{\partial A1}\frac{\partial A1}{\partial Z1}
\\\\
&=dZ2 \cdot W2^{\top} \odot dA1 \rightarrow dZ1 
\end{aligned}
\tag{18}
$$

有了$dZ1$后，再向前求$W1$和$B1$的误差，就和第5章中一样了，我们直接列在下面：

$$
dW1=X^{\top} \cdot dZ1 \tag{19}
$$
$$
dB1=dZ1 \tag{20}
$$

## 非线性多分类

### 定义神经网络结构

非线性多分类的网络结构如下图所示：

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/11/nn.png" />

- 输入层两个特征值$x_1, x_2$
$$
x=
\begin{pmatrix}
    x_1 & x_2
\end{pmatrix}
$$
- 隐层$2\times 3$的权重矩阵$W1$
$$
W1=
\begin{pmatrix}
    w1_{11} & w1_{12} & w1_{13} \\\\
    w1_{21} & w1_{22} & w1_{23}
\end{pmatrix}
$$

- 隐层$1\times 3$的偏移矩阵$B1$

$$
B1=\begin{pmatrix}
    b1_1 & b1_2 & b1_3 
\end{pmatrix}
$$

- 隐层由3个神经元构成
- 输出层$3\times 3$的权重矩阵$W2$
$$
W2=\begin{pmatrix}
    w2_{11} & w2_{12} & w2_{13} \\\\
    w2_{21} & w2_{22} & w2_{23} \\\\
    w2_{31} & w2_{32} & w2_{33} 
\end{pmatrix}
$$

- 输出层$1\times 1$的偏移矩阵$B2$

$$
B2=\begin{pmatrix}
    b2_1 & b2_2 & b2_3 
  \end{pmatrix}
$$

- 输出层有3个神经元使用Softmax函数进行分类

### 前向计算

前向计算过程：

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/11/multiple_forward.png" />


#### 第一层

- 线性计算

$$
z1_1 = x_1 w1_{11} + x_2 w1_{21} + b1_1
$$
$$
z1_2 = x_1 w1_{12} + x_2 w1_{22} + b1_2
$$
$$
z1_3 = x_1 w1_{13} + x_2 w1_{23} + b1_3
$$
$$
Z1 = X \cdot W1 + B1
$$

- 激活函数

$$
a1_1 = Sigmoid(z1_1) 
$$
$$
a1_2 = Sigmoid(z1_2) 
$$
$$
a1_3 = Sigmoid(z1_3) 
$$
$$
A1 = Sigmoid(Z1)
$$

#### 第二层

- 线性计算

$$
z2_1 = a1_1 w2_{11} + a1_2 w2_{21} + a1_3 w2_{31} + b2_1
$$
$$
z2_2 = a1_1 w2_{12} + a1_2 w2_{22} + a1_3 w2_{32} + b2_2
$$
$$
z2_3 = a1_1 w2_{13} + a1_2 w2_{23} + a1_3 w2_{33} + b2_3
$$
$$
Z2 = A1 \cdot W2 + B2
$$

- 分类函数

$$
a2_1 = \frac{e^{z2_1}}{e^{z2_1} + e^{z2_2} + e^{z2_3}}
$$
$$
a2_2 = \frac{e^{z2_2}}{e^{z2_1} + e^{z2_2} + e^{z2_3}}
$$
$$
a2_3 = \frac{e^{z2_3}}{e^{z2_1} + e^{z2_2} + e^{z2_3}}
$$
$$
A2 = Softmax(Z2)
$$

#### 损失函数

使用多分类交叉熵损失函数：
$$
loss = -(y_1 \ln a2_1 + y_2 \ln a2_2 + y_3 \ln a2_3)
$$
$$
J(w,b) = -\frac{1}{m} \sum^m_{i=1} \sum^n_{j=1} y_{ij} \ln (a2_{ij})
$$

$m$为样本数，$n$为类别数。

### 反向传播

反向传播的路径图如下图所示：

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/11/multiple_backward.png" />

Softmax与多分类交叉熵配合时的反向传播推导过程，最后是一个很简单的减法：

$$
\frac{\partial loss}{\partial Z2}=A2-y \rightarrow dZ2
$$

从Z2开始再向前推的话，和10.2节是一模一样的，所以直接把结论拿过来：

$$
\frac{\partial loss}{\partial W2}=A1^{\top} \cdot dZ2 \rightarrow dW2
$$
$$\frac{\partial{loss}}{\partial{B2}}=dZ2 \rightarrow dB2$$
$$
\frac{\partial A1}{\partial Z1}=A1 \odot (1-A1) \rightarrow dA1
$$
$$
\frac{\partial loss}{\partial Z1}=dZ2 \cdot W2^{\top} \odot dA1 \rightarrow dZ1 
$$
$$
dW1=X^{\top} \cdot dZ1
$$
$$
dB1=dZ1
$$

训练过程如下图所示：

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/11/loss.png" />

迭代了5000次，没有到达损失函数小于0.1的条件。

分类结果如下图所示：

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/11/result.png" ch="500" />

总结：
解决非线性回归问题方法之一是将非线性回归问题转化为线性回归的问题。
python代码实现的过程就是利用一些激活函数进行非线性问题转化为线性问题的映射。再利用线性回归
方法进行拟合。
在学习线性回归问题时，会遇到关于最小二乘法的讲解，LinearRegression就是普通的最小二乘回归线性问题。还有激活函数就是用来加入非线性因数，解决线性模型不能解决的问题在激活函数中我们是做输入和它们对应的权重的乘积之和，并将激活函数应用于获取该层的输出并将其作为输入反馈送到下一层。
非线性二分类是对每一层进行计算首先对第一层进行线性计算再通过激活函数进行计算再通过激活函数传递数据到第二层再进行线性计算再通过分类函数和损失函数得到最终结果。但是为了让数据更加准确我们就需要对结果进行验证所以就需要通过反向传播来去进行，通过损失函数求对输出层的反向误差和对隐藏层的反向误差的到最终的误差看是否在合理范围内，确定神经网络的训练是否合理。
非线性多分类相比于非线性二分类就是多了对层数分类多了几层通过更多的计算来对神经网络的训练以便最终的结果更加精确。


