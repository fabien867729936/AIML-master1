# 第三次作业

## 非线性回归
### 激活函数
### 激活函数的概论
### 激活函数的基本作用

图8-1是神经网络中的一个神经元，假设该神经元有三个输入，分别为$x_1,x_2,x_3$，那么：

$$z=x_1 w_1 + x_2 w_2 + x_3 w_3 +b \tag{1}$$
$$a = \sigma(z) \tag{2}$$

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/1/NeuranCell.png" width="500" />

图8-1 激活函数在神经元中的位置

激活函数的作用：
1.给神经网络增加非线性因素。
2.把公式1的计算结果压缩到$[0,1]$ 之间。

激活函数的基本性质：
+ 非线性：线性的激活函数和没有激活函数一样；
+ 可导性：做误差反向传播和梯度下降，必须要保证激活函数的可导性；
+ 单调性：单一的输入会得到单一的输出，较大值的输入得到较大值的输出。

### 激活函数的时间

激活函数是用在神经网络的层与层之间的连接，神经网络的最后一层不用激活。神经网络不管有多少层，最后的输出层决定了这个神经网络能干什么。

单层的神经网络的参数与功能

|网络|输入|输出|激活函数|分类函数|功能|
|---|---|---|---|---|---|
|单层|单变量|单输出|无|无|线性回归|
|单层|多变量|单输出|无|无|线性回归|
|单层|多变量|单输出|无|二分类函数|二分类|
|单层|多变量|多输出|无|多分类函数|多分类|

从这里可以知道：
1.神经网络最后一层不需要激活函数
2.激活函数只用于连接前后两层神经网络

### 挤压型激活函数

这一类函数的特点是，当输入值域的绝对值较大的时候，其输出在两端是饱和的，都具有S形的函数曲线以及压缩输入值域的作用，所以叫挤压型激活函数，又可以叫饱和型激活函数。

在神经网络中，有两个常用的Sigmoid函数，一个是Logistic函数，另一个Tanh函数。

### Logistic函数

Logistic函数即为对数w几率函数。

公式

$$Sigmoid(z) = \frac{1}{1 + e^{-z}} \rightarrow a \tag{1}$$

导数

$$Sigmoid'(z) = a(1 - a) \tag{2}$$

推导过程如下：
令：$u=1,v=1+e^{-z}$ 则：
$$
\begin{aligned}
Sigmoid'(z)&= (\frac{u}{v})'=\frac{u'v-v'u}{v^2} \\\\
&=\frac{0-(1+e^{-z})'}{(1+e^{-z})^2}=\frac{e^{-z}}{(1+e^{-z})^2} \\\\
&=\frac{1+e^{-z}-1}{(1+e^{-z})^2}=\frac{1}{1+e^{-z}}-(\frac{1}{1+e^{-z}})^2 \\\\
&=a-a^2=a(1-a)
\end{aligned}
$$

值域
- 输入值域：$(-\infty, \infty)$
- 输出值域：$(0,1)$
- 导数值域：$(0,0.25]$

#### 函数图像

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/8/sigmoid.png" ch="500" />

图8-3 Sigmoid函数图像

#### 优点

从数学上来看，Sigmoid函数对中央区的信号增益较大，对两侧区的信号增益小，在信号的特征空间映射上，有很好的效果。
从神经科学上来看，中央区酷似神经元的兴奋态，两侧区酷似神经元的抑制态，因而在神经网络学习方面，可以将重点特征推向中央区，将非重点特征推向两侧区。

#### 缺点

指数计算代价大。

### Tanh函数
TanHyperbolic，即双曲正切函数。

### 公式
$$Tanh(z) = \frac{e^{z} - e^{-z}}{e^{z} + e^{-z}} = (\frac{2}{1 + e^{-2z}}-1) \rightarrow a \tag{3}$$
即
$$Tanh(z) = 2 \cdot Sigmoid(2z) - 1 \tag{4}$$

### 导数公式

$$Tanh'(z) = (1 + a)(1 - a)$$

利用基本导数公式23，令：$u={e^{z}-e^{-z}}，v=e^{z}+e^{-z}$ 则有：
$$
\begin{aligned}
Tanh'(z)&=\frac{u'v-v'u}{v^2} \\\\
&=\frac{(e^{z}-e^{-z})'(e^{z}+e^{-z})-(e^{z}+e^{-z})'(e^{z}-e^{-z})}{(e^{z}+e^{-z})^2} \\\\
&=\frac{(e^{z}+e^{-z})(e^{z}+e^{-z})-(e^{z}-e^{-z})(e^{z}-e^ {-z})}{(e^{z}+e^{-z})^2} \\\\
&=\frac{(e^{z}+e^{-z})^2-(e^{z}-e^{-z})^2}{(e^{z}+e^{-z})^2} \\\\
&=1-(\frac{(e^{z}-e^{-z}}{e^{z}+e^{-z}})^2=1-a^2
\end{aligned}
$$

### 值域

- 输入值域：$(-\infty,\infty)$
- 输出值域：$(-1,1)$
- 导数值域：$(0,1)$

### 函数图像

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/8/tanh.png" ch="500" />

### 优点
具有Sigmoid的所有优点
同时在传递过程中，输入数据的均值并不会发生改变，这就使他在很多应用中能表现出比Sigmoid优异一些的效果。

### 缺点
exp指数计算代价大。梯度消失问题仍然存在。

### 代码测试



### 半线性激活函数

### ReLU函数

修正线性单元，线性整流函数，斜坡函数。

### 公式
$$ReLU(z) = max(0,z) = \begin{cases} 
  z, & z \geq 0 \\\\ 
  0, & z < 0 
\end{cases}$$

### 导数
$$ReLU'(z) = \begin{cases} 1 & z \geq 0 \\\\ 0 & z < 0 \end{cases}$$

### 值域
- 输入值域：$(-\infty, \infty)$
- 输出值域：$(0,\infty)$
- 导数值域：$\\{0,1\\}$

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/8/relu.png"/>

### 优点

- 反向导数恒等于1，更加有效率的反向传播梯度值，收敛速度快；
- 避免梯度消失问题；
- 计算简单，速度快；
- 活跃度的分散性使得神经网络的整体计算成本下降。

### 缺点
无界。

### 8.2.2 Leaky ReLU函数
LReLU，带泄露的线性整流函数。

### 公式
$$LReLU(z) = \begin{cases} z & z \geq 0 \\\\ \alpha \cdot z & z < 0 \end{cases}$$

### 导数
$$LReLU'(z) = \begin{cases} 1 & z \geq 0 \\\\ \alpha & z < 0 \end{cases}$$

### 值域
输入值域：$(-\infty, \infty)$
输出值域：$(-\infty,\infty)$
导数值域：$\\{\alpha,1\\}$

### 函数图像

函数图像如图所示。
<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/8/leakyRelu.png"/>

### 优点
继承了ReLU函数的优点，具有收敛快速和运算复杂度低的优点。

### Softplus函数
### 公式
$$Softplus(z) = \ln (1 + e^z)$$

### 导数
$$Softplus'(z) = \frac{e^z}{1 + e^z}$$

### 
输入值域：$(-\infty, \infty)$
输出值域：$(0,\infty)$
导数值域：$(0,1)$

### 函数图像

Softplus的函数图像如图8-8所示。
<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/8/softplus.png"/>

### 8.2.4 ELU函数

#### 公式
$$ELU(z) = \begin{cases} z & z \geq 0 \\ \alpha (e^z-1) & z < 0 \end{cases}$$

#### 导数
$$ELU'(z) = \begin{cases} 1 & z \geq 0 \\ \alpha e^z & z < 0 \end{cases}$$

#### 值域
输入值域：$(-\infty, \infty)$
输出值域：$(-\alpha,\infty)$
导数值域：$(0,1]$

#### 函数图像
ELU的函数图像如图8-9所示。

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/8/elu.png"/>

### 单入单出的双层神经网络-非线性回归

### 思考：
在思考解决非线性回归的问题时，首先可以明确自变量X和因变量Y之间不是线性关系。常用的传统的处理方法有线性迭代法、分段回归法、迭代最小二乘法等。在解决这一类问题中，我们需要思考一个隐层的两层神经网络。

### 回归模型的评价标准

回归问题主要是求值，评价标准主要是看求得值与实际结果的偏差有多大，所以，回归问题主要以下方法来评价模型。

### 用多项式回归法拟合正弦曲线

### 多项式回归的概念

多项式回归有几种形式：
一元一次线性模型、多元一次多项式、一元多次多项式、多元多次多项式

### 双层神经网络实现非线性回归

### 万能近似定理

万能近似定理(universal approximation theorem) $^{[1]}$，是深度学习最根本的理论依据。它证明了在给定网络具有足够多的隐藏单元的条件下，配备一个线性输出层和一个带有任何“挤压”性质的激活函数（如Sigmoid激活函数）的隐藏层的前馈神经网络，能够以任何想要的误差量近似任何从一个有限维度的空间映射到另一个有限维度空间的Borel可测的函数。

前馈网络的导数也可以以任意好地程度近似函数的导数。

### 定义神经网络结构

本节的目的是要用神经网络完成图9-1和图9-2中的曲线拟合。

根据万能近似定理的要求，我们定义一个两层的神经网络，输入层不算，一个隐藏层，含3个神经元，一个输出层。图9-7显示了此次用到的神经网络结构。

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/9/nn.png" />

图9-7 单入单出的双层神经网络

### 曲线拟合

### 正弦曲线的拟合
### 隐层只有一个神经元的情况

令`n_hidden=1`，并指定模型名称为`sin_111`，训练过程见图9-10。图9-11为拟合效果图。

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/9/sin_loss_1n.png" />

训练过程中损失函数值和准确率的变化

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/9/sin_result_1n.png" ch="500" />

一个神经元的拟合效果

损失值到0.04附近就很难下降了。

### 非线性回归的工作原理
### 多项式为何能拟合曲线
使用多项式回归法，它成功地用于正弦曲线和复合函数曲线的拟合，其基本工作原理是把单一特征值的高次方做为额外的特征值加入，使得神经网络可以得到附加的信息用于训练。

观察多项式回归的示意图

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/9/polynomial_concept.png"/>
多项式回归方法的特征值输入

单层神经网络的多项式回归法，需要$x,x^2,x^3$三个特征值，组成如下公式来得到拟合结果：

$$
z = x \cdot w_1 + x^2 \cdot w_2 + x^3 \cdot w_3 + b \tag{1}
$$

第一步 把X拆成两个线性序列z1和z2

假设原始值x有21个点，样本数据如表9-15所示。

表9-15

|id|0|1|2|...|19|20|21|
|--|--|--|--|--|--|--|--|
|x|0.|0.05|0.1|...|0.9|0.95|1.|

通过以下线性变换，被分成了两个线性序列，得到下表所示的隐层值：

$$
z1 = x \cdot w_{11} + b_{11} \tag{2}
$$
$$
z2 = x \cdot w_{12} + b_{12} \tag{3}
$$

其中：

- $w_{11} = -2.673$
- $b_{11} = 1.303$
- $w_{12} = -9.036$
- $b_{12} = 4.507$

隐层线性变化结果

||0|1|2|...|19|20|21|
|--|--|--|--|--|--|--|--|
|z1|1.303|1.169|1.035|...|-1.102|-1.236|-1.369|
|z2|4.507|4.055|3.603|...|-3.625|-4.077|-4.528|

三个线性序列如图9-19所示，黑色点是原始数据序列，红色和绿色点是拆分后的两个序列。

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/9/nn_concept_x_z1_z2.png" ch="500" />

从原始数据序列拆分成的两个数据序列

这个运算相当于把特征值分解成两个部分，不太容易理解。打个不太恰当的比喻，有一个浮点数12.34，你可以把它拆成12和0.34两个部分，然后去分别做一些运算。另外一个例子就是，一张彩色图片上的黄色，我们普通人看到的就是黄色，但是画家会想到是红色和绿色的组合。

第二步 计算z1的激活函数值a1

下表和下图分别展示了隐层对第一个特征值的计算结果数值和示意图。

第一个特征值及其激活函数结果数值

||0|1|2|...|19|20|21|
|--|--|--|--|--|--|--|--|
|z1|1.303|1.169|1.035|...|-1.102|-1.236|-1.369|
|a1|0.786|0.763|0.738|...|0.249|0.225|0.203|

第二行的a1值等于第1行的z1值的sigmoid函数值：

$$a1 = {1 \over 1+e^{-z1}} \tag{4}$$

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/9/nn_concept_x_z1_a1.png" ch="500" />

第一个特征值及其激活函数结果可视化

z1还是一条直线，但是经过激活函数后的a1已经不是一条直线了。上面这张图由于z1的跨度大，所以a1的曲线程度不容易看出来。

第三步 计算z2的激活函数值a2

下表和下图分别展示了隐层对第二个特征值的计算结果数值和示意图。

第二个特征值及其激活函数结果数值

||0|1|2|...|19|20|21|
|--|--|--|--|--|--|--|--|
|z2|4.507|4.055|3.603|...|-3.625|-4.077|-4.528|
|a2|0.989|0.983|0.973|...|0.026|0.017|0.011|

$$a2 = {1 \over 1+e^{-z2}} \tag{5}$$

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/9/nn_concept_x_z2_a2.png" ch="500" />

第二个特征值及其激活函数结果可视化

z2还是一条直线，但是经过激活函数后的a2已经明显看出是一条曲线了。

第四步 计算Z值

下表和下图分别展示了输出层对两个特征值的计算结果数值和示意图。

输出层的计算结果数值

||0|1|2|...|19|20|21|
|--|--|--|--|--|--|--|--|
|a1|0.786|0.763|0.738|...|0.249|0.225|0.203|
|a2|0.989|0.983|0.973|...|0.026|0.017|0.011|
|z|0.202|0.383|0.561|...|-0.580|-0.409|-0.235|

$$z = a1 \cdot w_{11} + a2 \cdot w_{21} + b \tag{6}$$

其中：

- $w_{11}=-9.374$
- $w_{21}=6.039$
- $b=1.599$
  
<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/9/nn_concept_a1_a2_z.png" ch="500" />

输出层的计算结果可视化

也就是说，相同x值的红点a1和绿点a2，经过公式6计算后得到蓝点z，而所有的蓝点就拟合出一条正弦曲线。

比较多项式回归和双层神经网络解法


多项式回归和神经网络的比较

||多项式回归|双层神经网络|
|---|---|---|
|特征提取方式|特征值的高次方|线性变换拆分|
|特征值数量级|高几倍的数量级|数量级与原特征值相同|
|训练效率|低，需要迭代次数多|高，比前者少好几个数量级|

### 总结
-综上可知多项式回归在各个方面均处于绝对的优势地位。

### 超参数优化的初步认识

通过学习可以得知在超参数优化中主要存在两方面的困难：

1.超参数优化是一个组合优化问题，无法无法像一般参数那样通过梯度下降方法来优化，也没有一种通用有效的优化方法。
2.评估一组超参数配置的时间代价很高，导致一些优化方法在超参数优化难以应用。

对于超参数的设置，比较简单的方法是人工搜索、网络搜索和随机搜索。

其中最终可以调整的参数只有三个：
- 隐层神经元数
- 学习率
- 批样本量

在使用中我们还需要做到避免权重矩阵初始化的影响

权重矩阵中的参数，是神经网络要学习的参数，所以不能称作超参数。

权重矩阵初始化是神经网络训练非常重要的环节之一，不同的初始化方法，甚至是相同的方法但不同的随机值，都会给结果带来或多或少的影响。

在进行手动调整参数时，我们还必须了解超参数、训练误差、泛化误差和计算资源之间的相应关系。

# step4 总结

通过对双层神经网络的学习，学习了如何解决非线性问题。在两层神经网络之间，需要有激活函数的连接，从而提升神经网络的能力。通过单入淡出双层的非线性回归，分析了多项式回归法拟合正弦曲线和多项式回归法拟合复合函数曲线，同时进行了验证和测试，在神经网络非线性回归实现中，学习了万能近似定理，对其的可实现性进行了全面分析，分析了其实现中对其产生影响的因素，进行了论证，同时分析了使用曲线拟合的各各方面，了解了非线性回归的工作原理，最后学习了参数调优初步的使用。

## 10 非线性分类

### 10.0-多入单出双层-非线性二分类

二分类模型的评估标准
精确度也被称之为精度

混淆矩阵分成4部分来评估：

- 正例中被判断为正类的样本数（TP-True Positive）：521
- 正例中被判断为负类的样本数（FN-False Negative）：550-521=29
- 负例中被判断为负类的样本数（TN-True Negative）：435
- 负例中被判断为正类的样本数（FP-False Positive）：450-435=15

可以用下图来帮助理解。

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/10/TPFP.png"/>

用表格的方式描述矩阵的话是下表的样子。

四类样本的矩阵关系

|预测值|被判断为正类|被判断为负类|Total|
|---|---|---|---|
|样本实际为正例|TP-True Positive|FN-False Negative|Actual Positive=TP+FN|
|样本实际为负例|FP-False Positive|TN-True Negative|Actual Negative=FP+TN|
|Total|Predicated Postivie=TP+FP|Predicated Negative=FN+TN|

>准确率，越大越好。<br>
>分子为被判断为正类并且真的是正类的样本数，分母是被判断为正类的样本数。越大越好。<br>
>分子为被判断为正类并且真的是正类的样本数，分母是真的正类的样本数。越大越好。<br>
>分子为被判断为正类的负例样本数，分母为所有负类样本数。越小越好。<br>
>调和平均值越大越好

### 10.2 非线性二分类实现

定义完成非线性二分类的神经网络结构图，下图所示

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/10/xor_nn.png" />

- 输入层两个特征值$x_1,x_2$
  $$
  X=\begin{pmatrix}
    x_1 & x_2
  \end{pmatrix}
  $$

- 隐层$2\times 2$的权重矩阵$W1$
$$
  W1=\begin{pmatrix}
    w1_{11} & w1_{12} \\\\
    w1_{21} & w1_{22} 
  \end{pmatrix}
$$
- 隐层$1\times 2$的偏移矩阵$B1$

$$
  B1=\begin{pmatrix}
    b1_{1} & b1_{2}
  \end{pmatrix}
$$

- 隐层由两个神经元构成
$$
Z1=\begin{pmatrix}
  z1_{1} & z1_{2}
\end{pmatrix}
$$

$$
A1=\begin{pmatrix}
  a1_{1} & a1_{2}
\end{pmatrix}
$$

- 输出层$1\times 1$的偏移矩阵$B2$

$$
  B2=\begin{pmatrix}
    b2_{1}
  \end{pmatrix}
$$

- 输出层有一个神经元使用Logistic函数进行分类
$$
  Z2=\begin{pmatrix}
    z2_{1}
  \end{pmatrix}
$$
$$
  A2=\begin{pmatrix}
    a2_{1}
  \end{pmatrix}
$$

### 10.3 实现逻辑异或门

运行结果

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/10/xor_loss.png" />

### 10.4 逻辑异或门的工作原理

运行结果
<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/10/xor_result_2d.png" ch="500" />

### 10.5 实现双弧形二分类

运行结果
<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/10/sin_loss.png" />


## 10.6 双弧形二分类的工作原理

运行结果

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/10/sin_data_source.png" ch="500" />

# 第11章 多入多出的双层神经网络 - 非线性多分类

## 11.0 非线性多分类问题

### 11.0.1 提出问题：铜钱孔形问题

多分类问题数据样本
非线性多分类

|样本|$x_1$|$x_2$|$y$|
|---|---|---|---|
|1|0.22825111|-0.34587097|2|
|2|0.20982606|0.43388447|3|
|...|...|...|...|
|1000|0.38230143|-0.16455377|2|

还好这个数据只有两个特征，所以我们可以用可视化的方法展示，如图

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/11/data.png" ch="500" />

可视化样本数据

一共有3个类别：

1. 蓝色方点
2. 红色叉点
3. 绿色圆点

### 11.1 非线性多分类

非线性多分类的神经网络结构图
<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/11/nn.png" />

### 11.2 非线性多分类的工作原理

运行结果
|||
|---|---|
|<img src='../Images/11/multiple_3d_c1_1.png'/>|<img src='../Images/11/multiple_3d_c2_1.png'/>|
|红色：类别1样本区域|红色：类别2样本区域|
|<img src='../Images/11/multiple_3d_c3_1.png'/>|<img src='../Images/11/multiple_3d_c1_c2_1.png'/>|

### 12.0 多变量非线性多分类

学习率与批大小是对梯度下降影响最大的两个因子。 

梯度下降公式： $$ w_{t+1} = w_t - \frac{\eta}{m} \sum_i^m \nabla J(w,b) $$

其中:

1.$\eta$：学习率.
1.m：批大小。

# Sept总结
部分围绕非线性分类，介绍了各类门的工作原理及实现、非线性多分类的工作原理，实现方法。进一步了解了神经网络的工作原理，解决多分类问题，以及数据集的使用得到理想泛能力强的模型，而其都是由神经网络实现。