# 第一次作业
## ai-edu-step1学习过程
### 1.0人工智能发展简史
在判断智能的可能性下，英国科学家图灵提出了图灵测试，如果一台机器能够与人类展开对话而不能被辨别出其机器的身份，那么称这台机器具有智能。

关于聊天机器人是否智能，1980年有学者提出中文房间问题，在房间中一个不会中文，只会英语的人被关在封闭房间，房内有一本英文手册，指示如何处理收到的汉语信息并如何用汉语回复，房间内的人接收外部传入的中文问题，再通过查阅手册回复相应的中文问题，将答案递出。

当房间外的人无法分辨内部传出的中文信息是出自人还是机器之手时，这就被叫做智能。

### 1.1 人工智能的定义
人工智能的分类：
#### 由**期待**进行划分：
 **智能地把某件特定的事情做好，在某个领域增强人类的智慧，这种方式又叫做智能增强**这种人工智能针对于某一方面，在某一特定领域帮助人类完成简单的任务，属于“弱人工智能”，或者叫“狭义人工智能”
**像人类一样能认知，思考，判断：模拟人类的智能**
这种人工智能是人类所憧憬的人工智能，具有和人一样的智慧，思考，可以多维度的处理各种各样的事情，但是这种人工智能基于目前的编程无法实现，也有人对于这样的“强人工智能”怀有担心和疑虑，害怕AI超出人类的控制，从未对人类造成危害，不过目前我们的技术离这个还很远，不需要杞人忧天，很多科幻类影视作品喜欢以这个为题材，描绘人机大战。

#### 由**技术**进行分析
**因为目前所实施的大多是狭义的人工智能，所以优先考虑这种方式，通过对这种方式的分析，可以发现这种技术拥有一些共性：**

1.有一个模型结构（例如逻辑回归，决策树）
2.用训练数据输入模块，获得经验
3.通过不断执行任务并在执行结束后对结果进行分析，重复使结果不断优化，达到一个满意的值。

**对机器学习的方法可以划归三种类型：**
1.监督学习
2.无监督学习
3.强化学习

其中监督学习是通过标注的数据来学习，无监督学习是通过没有标注的数据进行学习，这种方式相较于上一种需要更强的分析能力，可以从数据种找到共同特性，发现不同数据间的联系。强化学习则是在无监督学习的基础上，选择与外界进行互动，环境给程序的反馈，成为对程序学习的实现的评价标准，程序需要学习到一个模型，持续的获得高评价，是程序不断地提升，不断地优化到评价满意的水平。

机器学习领域中，神经元是其中一个重要的方法，每个神经元都与其他神经元相连，同时会向其他神经元发送化学物质，改变其他神经元，当某个神经元的电位超过一定的阈值，它就会被激活，向其他神经元发送化学物质。将神经元搭建起来我们就会构建出一个神经网络。

随着数据的丰富和机器算力的增强，人们可以不断增加神经网络的层次数目，相邻层次之间的输入输出由非线性函数来控制，产生DNN（深度神经网络）。其近十年有巨大成果，包括图像分类、语音识别、自然语言处理等方面。

随着人们不断地调整网络结构，DNN又产生了许多新的结构，例如卷积神经网络，循环神经网络，长期短期记忆，生成对抗网络，迁移学习等。

#### 由取得成果来看
狭义人工智能在各个领域都取得了很大的成果。某些领域中，人工智能的某项能力取得的成绩已经超过了人类的最高水平

如：
-翻译领域（微软的中英翻译超过了人类）
-阅读理解（SQuAD比赛）
-下围棋（2016）德州扑克（2019）

AI在一些领域与其他科技的结合，如计算，数据，云端和物联网终端设备联系，搭建一个支持智能决定的系统，这也可以看作是未来基础设置建设的一个方向，称之为新基建。

在个人用户的角度来看，AI已经走进了我们的生活中，从从跨境旅游的翻译、照片的美颜，甚至是人工换脸等方面，AI已经完美的满足了我们的需求。

未来的软件开发中，程序的开发，和AI模型的开发进行有机的协同，达到一种更好的效果，为用户提供更好的服务，创作更大的价值。

### 1.2 范式的演化 
-**范式的演化阶段：**
1.第一阶段：经验
2.第二阶段：理论
3.第三阶段：计算仿真
4.第四阶段：数据探索

演变过程也是通过感官获取的经验，不断地积累而后获得相应的理论，在通过实验进行对理论地计算仿真，查看验证结果，反过来对理论进行验证，在数据探索阶段，通过数据的回收，通过分析数据，探索新的规律，对AI而言就是不断地学习，不断地总结，不断地获取能力，不断地强化AI，例如ALphaGo。

### 神经网络地基本工作原理介绍
-**神经网络细胞的数学模型**
输入 input
权重 weights
偏移 bias
求和计算 sum
激活函数 activation

**小结**
- 一个神经元可以有多个输入
- 一个神经元只能有一个输出，这个输出可以同时输入给多个神经元。
- 一个神经元的$w$的数量和输入的数量一致。
- 一个神经元只有一个 $b$。
- $w$ 和 $b$ 有人为的初始值，在训练过程中被不断修改。
- $A$ 可以等于 $Z$，即激活函数不是必须有的。
- 一层神经网络中的所有神经元的激活函数必须一致。

**单层神经网络模型**
这是一个单层的神经网络，假设有m个输入，有n个输出。在神经网络中，b到每一个神经元的权值来表示实际的偏移值，（b_1,b_2）,这样便于矩阵运算。也有些把b写成x_0,其实是同一个效果，即把偏移值看作神经元的一个输入。
x_1，x_2,x_3看作是样本数据的三个特征值
w_{11},w_{21},w_{31}是（x_1，x_2,x_3）到n1的权重
w_{12},w_{22},w_{32}是（x_1,x_2,x_3)到n2的权重
b_1是n1的偏移
b_2是n2的偏移

![](https://camo.githubusercontent.com/231032513bc8d84bf9a3f601e200279b1b0f3eee/68747470733a2f2f61696564756769746875623461322e626c6f622e636f72652e77696e646f77732e6e65742f61322d696d616765732f496d616765732f312f4f6e654c617965724e4e2e706e67)

**训练流程**
如图所示

![](https://camo.githubusercontent.com/292b2fbcd6e3351763cab081dd8c02e413f3ba81/68747470733a2f2f61696564756769746875623461322e626c6f622e636f72652e77696e646f77732e6e65742f61322d696d616765732f496d616765732f312f547261696e466c6f772e706e67)

**神经网络的主要功能**
回归/拟合：使得模拟测试出的y值与样本数据形成的曲线距离尽量的小，可以理解为是对样本数据的一种骨架式的抽象。

![](https://camo.githubusercontent.com/3069fea9664ce4873d7cc8c4cf5a17b8c287948a/68747470733a2f2f61696564756769746875623461322e626c6f622e636f72652e77696e646f77732e6e65742f61322d696d616765735c496d616765735c315c7367645f726573756c742e706e67)

分类

通过一个两层的神经网络得到一个非常相似的结果，使得分类误差在满意的范围之内。通过神经网络训练出来的分界线，可以比较完美地把两类样本分开，所以分类可以理解为是对两类或多类样本数据的边界的抽象。

激活函数

人体运动组织和神经网络组织的对比

|人体运动组织|神经网络组织|
|---|---|
|支撑骨骼|网络层次|
|关节|激活函数|
|肌肉韧带|权重参数|
|学习各种运动的动作|前向+反向训练过程|

![](https://camo.githubusercontent.com/497a1caa69069e20853c75c77dedd20bc9a8f978/68747470733a2f2f61696564756769746875623461322e626c6f622e636f72652e77696e646f77732e6e65742f61322d696d616765732f496d616765732f312f4c696e656172767341637469766174696f6e2e706e67)

激活函数相当于对得到的数据进行合理的拟合操作

### 神经网络的三个基本概念
这三个概念是：反向传播，梯度下降，损失函数。

反向传播与梯度下降的基本工作原理：

1. 初始化；
2. 正向计算；
3. 损失函数为我们提供了计算损失的方法；
4. 梯度下降是在损失函数基础上向着损失最小的点靠近而指引了网络权重调整的方向；
5. 反向传播把损失值反向传给神经网络的每一层，让每一层都根据损失值反向调整权重；
6. Go to 2，直到精度足够好（比如损失函数值小于 0.001）。

#### 线性反向传播

代码：
import numpy as np

def target_function(w,b):  
    x = 2*w+3*b  
    y=2*b+1  
    z=x*y  
    return x,y,z  
  
def single_variable(w,b,t):  
    print("\nsingle variable: b ----- ")  
    error = 1e-5  
    while(True):  
        x,y,z = target_function(w,b)  
        delta_z = z - t  
        print("w=%f,b=%f,z=%f,delta_z=%f"%(w,b,z,delta_z))  
        if abs(delta_z) < error:  
            break  
        delta_b = delta_z /63  
        print("delta_b=%f"%delta_b)  
        b = b - delta_b  
  
    print("done!")  
    print("final b=%f"%b)  
  
def single_variable_new(w,b,t):  
    print("\nsingle variable new: b ----- ")  
    error = 1e-5  
    while(True):  
        x,y,z = target_function(w,b)  
        delta_z = z - t  
        print("w=%f,b=%f,z=%f,delta_z=%f"%(w,b,z,delta_z))  
        if abs(delta_z) < error:  
            break  
        factor_b = 2*x+3*y  
        delta_b = delta_z/factor_b  
        print("factor_b=%f, delta_b=%f"%(factor_b, delta_b))  
        b = b - delta_b  
  
    print("done!")  
    print("final b=%f"%b)  


# this version has a bug
def double_variable(w,b,t):  
    print("\ndouble variable: w, b -----")  
    error = 1e-5  
    while(True):  
        x,y,z = target_function(w,b)  
        delta_z = z - t  
        print("w=%f,b=%f,z=%f,delta_z=%f"%(w,b,z,delta_z))  
        if abs(delta_z) < error:  
            break  
        delta_b = delta_z/63/2  
        delta_w = delta_z/18/2  
        print("delta_b=%f, delta_w=%f"%(delta_b,delta_w))  
        b = b - delta_b  
        w = w - delta_w  
    print("done!")  
    print("final b=%f"%b)  
    print("final w=%f"%w)  
  
# this is correct version  
def double_variable_new(w,b,t):  
    print("\ndouble variable new: w, b -----")  
    error = 1e-5  
    while(True):  
        x,y,z = target_function(w,b)  
        delta_z = z - t  
        print("w=%f,b=%f,z=%f,delta_z=%f"%(w,b,z,delta_z))  
        if abs(delta_z) < error:  
            break  
  
        factor_b, factor_w = calculate_wb_factor(x,y)  
        delta_b = delta_z/factor_b/2  
        delta_w = delta_z/factor_w/2  
        print("factor_b=%f, factor_w=%f, delta_b=%f,   delta_w=%f"%(factor_b, factor_w, delta_b,delta_w))  
        b = b - delta_b  
        w = w - delta_w  
    print("done!")  
    print("final b=%f"%b)  
    print("final w=%f"%w)  
  
def calculate_wb_factor(x,y):  
    factor_b = 2*x+3*y  
    factor_w = 2*y  
    return factor_b, factor_w  
  
if __name__ == '__main__':  
    w = 3  
    b = 4  
    t = 150  
    single_variable(w,b,t)  
    single_variable_new(w,b,t)  
    double_variable(w,b,t)  
    double_variable_new(w,b,t)  

测试过程：

![](https://i0.hdslb.com/bfs/album/60cb39e3bd70c840e0c8e0096c10723fe4ae2c30.png@518w_1e_1c.png)

![](https://i0.hdslb.com/bfs/album/f2e2e959a0d8dcf935947e9f1e9cb321b097a936.png@518w_1e_1c.png)


#### 非线性反向传播

代码：
import numpy as np  
import matplotlib.pyplot as plt  
  
def draw_fun(X,Y):  
    x = np.linspace(1.2,10)  
    a = x*x  
    b = np.log(a)  
    c = np.sqrt(b)  
    plt.plot(x,c)  
  
    plt.plot(X,Y,'x')  
  
    d = 1/(x*np.sqrt(np.log(x**2)))  
    plt.plot(x,d)  
    plt.show()  
  
  
def forward(x):  
    a = x*x  
    b = np.log(a)  
    c = np.sqrt(b)  
    return a,b,c  
  
def backward(x,a,b,c,y):  
    loss = c - y  
    delta_c = loss  
    delta_b = delta_c * 2 * np.sqrt(b)  
    delta_a = delta_b * a  
    delta_x = delta_a / 2 / x  
    return loss, delta_x, delta_a, delta_b, delta_c  
  
def update(x, delta_x):  
    x = x - delta_x  
    if x < 1:  
        x = 1.1  
    return x  
  
if __name__ == '__main__':  
    print("how to play: 1) input x, 2) calculate c, 3) input   target number but not faraway from c")  
    print("input x as initial number(1.2,10), you can try 1.3:")    
    line = input()  
    x = float(line)  
      
    a,b,c = forward(x)  
    print("c=%f" %c)  
    print("input y as target number(0.5,2), you can try 1.8:")  
    line = input()  
    y = float(line)  
  
    error = 1e-3  
  
    X,Y = [],[]  
  
    for i in range(20):  
        # forward  
        print("forward...")  
        a,b,c = forward(x)  
        print("x=%f,a=%f,b=%f,c=%f" %(x,a,b,c))  
        X.append(x)  
        Y.append(c)  
        # backward  
        print("backward...")  
        loss, delta_x, delta_a, delta_b, delta_c = backward(x,a,b,c,y)  
        if abs(loss) < error:  
            print("done!")  
            break  
        # update x  
        x = update(x, delta_x)  
        print("delta_c=%f, delta_b=%f, delta_a=%f,   delta_x=%f\n" %(delta_c, delta_b, delta_a, delta_x))  
  
      
    draw_fun(X,Y)  

测试过程：


![](https://i0.hdslb.com/bfs/album/cba784c388496e759f963cc67e52b48007a61b07.png@518w_1e_1c.png)



#### 梯度下降

梯度下降的三要素：
1. 当前点
2. 方向
3. 步长

梯度下降包含了两层含义：
1. 梯度：函数当前位置的最快上升点
2. 下降：与导数相反的方向，用数学语言描述就是那个减号。
   
代码：
import numpy as np  
import matplotlib.pyplot as plt  
  
def target_function(x):  
    y = x*x  
    return y  
  
def derivative_function(x):  
    return 2*x  
  
def draw_function():  
    x = np.linspace(-1.2,1.2)  
    y = target_function(x)  
    plt.plot(x,y)  
  
def draw_gd(X):  
    Y = []  
    for i in range(len(X)):  
        Y.append(target_function(X[i]))  
      
    plt.plot(X,Y)  
  
if __name__ == '__main__':  
    x = 1.2  
    eta = 0.3  
    error = 1e-3  
    X = []  
    X.append(x)  
    y = target_function(x)  
    while y > error:  
        x = x - eta * derivative_function(x)  
        X.append(x)  
        y = target_function(x)  
        print("x=%f, y=%f" %(x,y))  
  
  
    draw_function()  
    draw_gd(X)  
    plt.show()  

测试过程:

![](https://i0.hdslb.com/bfs/album/45153861645128f943fcc7aab3b2a99c20803b8f.png@518w_1e_1c.png)



import numpy as np  
import matplotlib.pyplot as plt  
from mpl_toolkits.mplot3d import Axes3D  
  
def target_function(x,y):  
    J = x**2 + np.sin(y)**2  
    return J  
  
def derivative_function(theta):  
    x = theta[0]  
    y = theta[1]  
    return np.array([2*x,2*np.sin(y)*np.cos(y)])  
  
def show_3d_surface(x, y, z):  
    fig = plt.figure()  
    ax = Axes3D(fig)  
   
    u = np.linspace(-3, 3, 100)  
    v = np.linspace(-3, 3, 100)  
    X, Y = np.meshgrid(u, v)  
    R = np.zeros((len(u), len(v)))  
    for i in range(len(u)):  
        for j in range(len(v)):  
            R[i, j] = X[i, j]**2 + np.sin(Y[i, j])**2  
  
    ax.plot_surface(X, Y, R, cmap='rainbow')  
    plt.plot(x,y,z,c='black')  
    plt.show()  
  
if __name__ == '__main__':  
    theta = np.array([3,1])  
    eta = 0.1  
    error = 1e-2  
  
    X = []  
    Y = []  
    Z = []  
    for i in range(100):  
        print(theta)  
        x=theta[0]  
        y=theta[1]    
        z=target_function(x,y)  
        X.append(x)  
        Y.append(y)  
        Z.append(z)  
        print("%d: x=%f, y=%f, z=%f" %(i,x,y,z))  
        d_theta = derivative_function(theta)  
        print("    ",d_theta)  
        theta = theta - eta * d_theta  
        if z < error:  
            break  
    show_3d_surface(X,Y,Z)  

测试过程：


![](https://i0.hdslb.com/bfs/album/e39cba21f5a6f052dc81ea9a97382fcd45019073.png@518w_1e_1c.png)

import numpy as np  
import matplotlib.pyplot as plt  
  
def targetFunction(x):  
    y = (x-1)**2 + 0.1  
    return y  
  
def derivativeFun(x):  
    y = 2*(x-1)  
    return   
  
def create_sample():  
    x = np.linspace(-1,3,num=100)  
    y = targetFunction(x)  
    return x,  
  
def draw_base():  
    x,y=create_sample()  
    plt.plot(x,y,'.')  
    plt.show()  
    return x,y  
     
def gd(eta):  
    x = -0.8  
    a = np.zeros((2,10))  
    for i in range(10):  
        a[0,i] = x  
        a[1,i] = targetFunction(x)  
        dx = derivativeFun(x)  
        x = x - eta*dx  
      
    plt.plot(a[0,:],a[1,:],'x')  
    plt.plot(a[0,:],a[1,:])  
    plt.title("eta=%f" %eta)  
    plt.show()  
  
if __name__ == '__main__':  
  
    eta = [1.1,1.,0.8,0.6,0.4,0.2,0.1]  
  
    for e in eta:  
        X,Y=create_sample()  
        plt.plot(X,Y,'.')  
        #plt.show()  
        gd(e)  


测试过程：

![](https://i0.hdslb.com/bfs/album/39e7884ddac8c365115f4bdac2dc3a7a86ea5bce.png@518w_1e_1c.png)

![](https://i0.hdslb.com/bfs/album/752f94ae7f9e9d297bb69533be9c5c183d1535da.png@518w_1e_1c.png)

![](https://i0.hdslb.com/bfs/album/453c1016f35ae25c14a855936eb9af82f4d18d65.png@518w_1e_1c.png)

![](https://i0.hdslb.com/bfs/album/2b249a039e5111e6fb2bac931d032e6b8c2202a0.png@518w_1e_1c.png)

![](https://i0.hdslb.com/bfs/album/83be5ca223d7a0f3d4fd68b3385d6f82f0761c02.png@518w_1e_1c.png)

![](https://i0.hdslb.com/bfs/album/69d883e91d3ce3b978d870efcc07f3e9529290a6.png@518w_1e_1c.png)

![](https://i0.hdslb.com/bfs/album/17d9ba030a07dda35f0036dc3043e3765e413b18.png@518w_1e_1c.png)

#### 损失函数

### 概念
损失函数可以看作是误差，偏差等等，所指的意思也是和准确值之间的偏差

### 损失函数的作用

损失函数的作用是计算神经网络每次迭代的前进计算结果与事实值之间的差距，从而指导下一步的训练向正确的方向进行。
使用步骤：
1. 随机值初始化前向计算公式的参数
2. 代入样本，计算输出的预测值
3. 用损失函数计算预测值和标签值的误差
4. 根据损失函数的导数，沿着梯度最小方向将误差回传，修正前向计算公式中各个权重值
5. 重复第二步，直到损失函数达到一个满意的值就停止迭代。

代码：

import numpy as np  
import matplotlib.pyplot as plt  
  
def target_function2(a,y):  
    p1 = y * np.log(a)  
    p2 = (1-y) * np.log(1-a)  
    y = -p1 - p2  
    return y  
  
if __name__ == '__main__':  
    err = 1e-2  # avoid invalid math caculation  
    a = np.linspace(0+err,1-err)  
    y = 0  
    z1 = target_function2(a,y)  
    y = 1  
    z2 = target_function2(a,y)  
    p1, = plt.plot(a,z1)  
    p2, = plt.plot(a,z2)  
    plt.grid()  
    plt.legend([p1,p2],["y=0","y=1"])  
    plt.xlabel("a")  
    plt.ylabel("Loss")  
    plt.show()  


![](https://i0.hdslb.com/bfs/album/54987895bdcc1e57f2da2d43447754a3e2dadfc5.png@518w_1e_1c.png)

### 总结
    均方差函数，主要用于回归
    交叉熵函数，主要用于分类
    二者都是非负函数，极值在底部，用梯度下降法可以求解

#### **学习总结：**
通过对第一章的学习，对AI有了更深一步的了解，了解了AI的基本概念，从图灵测试，到中文翻译房间，从狭义功能的ALFHAGO到只存在于科幻小说的具有和人一样独立思考的超级AI，很有趣，对于目前AI的发展，从获得数据的方向来看，中国的移动互联网覆盖产生的庞大数据，为以后狭义方向功能的AI开发提供了庞大而宝贵的数据，对于未来AI乃至于超级AI，我是持有乐观态度，软硬件的升级，无人驾驶已经是进入了黄金赛道，后期AI会为我们生活创造出巨大的价值，很看好。

对于AI的来说，神经网络是非常重要的一部分，而神经网络的三大概念是：反向传播，梯度下降，损失函数。我的理解是反向传播相当于程序进行对于结果评价后反向传给每一层神经网络，一层一层的偏导，最终锁定传入函数值的大小，梯度下降指的是在损失函数基础上向着损失最小的点靠近而指引了网络权重调整的方向，损失函数则是为我们提供了计算损失的方法，损失函数的评价分析是整个过程的开始和基础。