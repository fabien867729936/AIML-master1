1、	人工智能发展简史
 ![1.png](https://i.loli.net/2020/10/05/EAorqSpNHF3DUKm.png)
2、	人工智能定义
第一个层面，人们对人工智能的期待可以分为：
•	智能地把某件特定的事情做好，在某个领域增强人类的智慧，这种方式又叫做智能增强——像搜索引擎，自动语言翻译，某个领域的智能助手那样的程序，帮助人类完成某种特定任务。这也叫做“弱人工智能”，或者“狭义人工智能”。
•	像人类一样能认知，思考，判断：模拟人类的智能——像人类一样能认知，思考，判断的智能软件。这是人工智能学科一开始就有的梦想。这样的智能也叫做“通用人工智能”（Artificial General Intelligence， AGI）， 或“强人工智能”。对于这样的人工智能，科幻小说有很多描写，也有一些研究，但是在实际的应用还没有什么突破。有学者认为，AGI是不可能通过目前人们编程程序的方式实现的^{[1]}[1]。尽管如此，社会上还是有人担忧有一天电脑的AGI会超过人类的智能，人类再也赶不上电脑，从而永远受制于电脑。
第二个层面，从技术的特点来看。
要实现某种狭义的人工智能，我们很自然地想到，如果我们能让运行程序的电脑来学习并自动掌握某些规律，那该多好啊，这就是“机器学习”。机器学习在几十年的发展历史中，产生了很多技术，这些技术都有下面的共性：
如果一个程序解决任务（T）的效能（用P表示）随着经验（E）得到了提高，那么，这个程序就能从经验（E）中学到了关于任务（T）的知识，并让衡量值（P）得到提高。
1.	选择一个模型结构（例如逻辑回归，决策树等），这就是上面说的程序。
2.	用训练数据（输入和输出）输入模型。这就是上面的经验（E）。
3.	通过不断执行任务（T）并衡量结果（P），让P不断提高，直到达到一个满意的值。
那么，机器学习的各种方法是如何从经验中学习呢？我们可以大致地分为下面三种类型：
1.	监督学习（Supervised Learning）
通过标注的数据来学习，例如，程序通过学习标注了正确答案的手写数字的图像数据，它就能认识其他的手写数字。
2.	无监督学习（Unsupervised Learning）
通过没有标注的数据来学习。这种算法可以发现数据中自然形成的共同特性（聚类），可以用来发现不同数据之间的联系，例如，买了商品A的顾客往往也购买了商品B。
3.	强化学习（Reinforcement Learning）
我们可以让程序选择和它的环境互动（例如玩一个游戏），环境给程序的反馈是一些“奖励”（例如游戏中获得高分），程序要学习到一个模型，能在这种环境中得到高的分数，不仅是当前局面要得到高分，而且最终的结果也要是高分才行。
 ![2.png](https://i.loli.net/2020/10/05/Ia6ORrNqpYbV2xS.png)

3、	范式的演化
范式演化的四个阶段
第一阶段：经验
第二阶段：理论
第三阶段：计算仿真
第四阶段：数据探索
4、	神经网络的基本工作原理
神经元细胞的数学模型
神经网络由基本的神经元组成
 ![3.png](https://i.loli.net/2020/10/05/xdFc4Ja976gEl1z.png)
输入 input   权重 weights    偏移 bias
 激活函数 activation
求和之后，神经细胞已经处于兴奋状态了，已经决定要向下一个神经元传递信号了，但是要传递多强烈的信号，要由激活函数来确定：
A=σ(Z)
如果激活函数是一个阶跃信号的话，会像继电器开合一样咔咔的开启和闭合，在生物体中是不可能有这种装置的，而是一个渐渐变化的过程。所以一般激活函数都是有一个渐变的过程，也就是说是个曲线。
 
 ![4.png](https://i.loli.net/2020/10/05/SxcPF7EUu6GVY5h.png)
 #### 小结

- 一个神经元可以有多个输入。
- 一个神经元只能有一个输出，这个输出可以同时输入给多个神经元。
- 一个神经元的 $w$ 的数量和输入的数量一致。
- 一个神经元只有一个 $b$。
- $w$ 和 $b$ 有人为的初始值，在训练过程中被不断修改。
- $A$ 可以等于 $Z$，即激活函数不是必须有的。
- 一层神经网络中的所有神经元的激活函数必须一致。
#### 训练流程

从真正的“零”开始学习神经网络时，我没有看到过任何一个流程图来讲述训练过程，大神们写书或者博客时都忽略了这一点，图1-16是一个简单的流程图。

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/1/TrainFlow.png" />
神经网络的主要功能

#### 回归（Regression）或者叫做拟合（Fitting）

单层的神经网络能够模拟一条二维平面上的直线，从而可以完成线性分割任务。而理论证明，两层神经网络可以无限逼近任意连续函数。图1-18所示就是一个两层神经网络拟合复杂曲线的实例。

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images\Images\1\sgd_result.png">
所谓回归或者拟合，其实就是给出x值输出y值的过程，并且让y值与样本数据形成的曲线的距离尽量小，可以理解为是对样本数据的一种骨架式的抽象。
  蓝色的点是样本点，从中可以大致地看出一个轮廓或骨架，而红色的点所连成的线就是神经网络的学习结果，它可以“穿过”样本点群形成中心线，尽量让所有的样本点到中心线的距离的和最近。
  #### 分类（Classification）

如图1-19，二维平面中有两类点，红色的和蓝色的，用一条直线肯定不能把两者分开了。

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images\Images\1\Sample.png">

图1-19 分类示意图

我们使用一个两层的神经网络可以得到一个非常近似的结果，使得分类误差在满意的范围之内。图1-19中那条淡蓝色的曲线，本来并不存在，是通过神经网络训练出来的分界线，可以比较完美地把两类样本分开，所以分类可以理解为是对两类或多类样本数据的边界的抽象。

# 神经网络中的三个基本概念
这三大概念是：反向传播，梯度下降，损失函数。
神经网络训练的最基本的思想就是：先“猜”一个结果，称为预测结果 $a$，看看这个预测结果和事先标记好的训练集中的真实结果 $y$ 之间的差距，然后调整策略，再试一次，这一次就不是“猜”了，而是有依据地向正确的方向靠近。如此反复多次，一直到预测结果和真实结果之间相差无几，亦即 $|a-y|\rightarrow 0$，就结束训练。

在神经网络训练中，我们把“猜”叫做初始化，可以随机，也可以根据以前的经验给定初始值。即使是“猜”，也是有技术含量的。
简单总结一下反向传播与梯度下降的基本工作原理：

1. 初始化；
2. 正向计算；
3. 损失函数为我们提供了计算损失的方法；
4. 梯度下降是在损失函数基础上向着损失最小的点靠近而指引了网络权重调整的方向；
5. 反向传播把损失值反向传给神经网络的每一层，让每一层都根据损失值反向调整权重；
6. Go to 2，直到精度足够好（比如损失函数值小于 $0.001$）。
梯度下降的数学公式：

$$\theta_{n+1} = \theta_{n} - \eta \cdot \nabla J(\theta) \tag{1}$$

其中：

- $\theta_{n+1}$：下一个值；
- $\theta_n$：当前值；
- $-$：减号，梯度的反向；
- $\eta$：学习率或步长，控制每一步走的距离，不要太快以免错过了最佳景点，不要太慢以免时间太长；
- $\nabla$：梯度，函数当前位置的最快上升点；
- $J(\theta)$：函数。
#### 梯度下降的三要素

1. 当前点；
2. 方向；
3. 步长。
 ![5.png](https://i.loli.net/2020/10/05/XHw3c8fdSV7LJCB.png)
 ![6.png](https://i.loli.net/2020/10/05/5nO7q4jYQx2iFys.png)
 ![7.png](https://i.loli.net/2020/10/05/c4ghNfP9mGaJTKS.png)
 ![8.png](https://i.loli.net/2020/10/05/H4P3DTyqxMLZAgb.png)
 ![10.png](https://i.loli.net/2020/10/05/rZUHlc1BEY2X7PK.png)
 ![9.png](https://i.loli.net/2020/10/05/CoHSmnfkE6aQMXW.png)
 ![11.png](https://i.loli.net/2020/10/05/dYlUuZV2Xi4HBqP.png)
 损失函数
 

损失函数的作用
损失函数的作用，就是计算神经网络每次迭代的前向计算结果与真实值的差距，从而指导下一步的训练向正确的方向进行。
如何使用损失函数呢？具体步骤：
1.	用随机值初始化前向计算公式的参数；
2.	代入样本，计算输出的预测值；
3.	用损失函数计算预测值和标签值（真实值）的误差；
4.	根据损失函数的导数，沿梯度最小方向将误差回传，修正前向计算公式中的各个权重值；
5.	进入第2步重复, 直到损失函数值达到一个满意的值就停止迭代。
