# 0 人工智能发展史

-1950年，英国科学家图灵发表论文，提出图灵测试，人工智能第一次出现在正式的论文之中。

-人工智能于1956年作为一门学术学科创立，此后几年经历了几次乐观浪潮， 接着是失望和资金损失，接着是新的方法、成功和新的资金。 在其历史的大部分时间里，人工智能研究一直被划分为许多子领域，这些子领域之间往往无法相互沟通。

**中文房间问题**

-一个对中文一窍不通，只会英语的人被关在一个封闭房间中。房间里有一本英文手册，说明该如何处理收到的汉语信息。房间外的人向房间内输入中文问题，房间内的人便按照手册的说明，查找到合适的指示，将相应的中文字符组合成对问题的解答，并将答案输出。房间外面的人看到自己递进去的中文输入能得到回答，很可能就会认为房间内的人有智能，懂中文，就像现在的聊天机器人那样，那么这是“智能”么？


<img src="http://n1.itc.cn/img8/wb/smccloud/recom/2015/09/18/144254598237800138.JPEG" width="500" />

-*个人理解*：这一假设的关注点在于输入输出，类似一种函数问题，我们假设：
 *x=1* 
 输入x，那么我们就会得到1的回复，如果我们再次输入：
 *x=1,y=2,y+y=4* 的规则和
 *x+x* 的输入
 通常机器是否可以为我们解答，答案是否定的，换言之，没有“学习”和“应用”能力的机器并不智能

# 1 人工智能的定义

-第一个层面，人们对人工智能的期待可以分为:
 智能地把某件特定的事情做好，在某个领域增强人类的智慧，这种方式又叫智能增强。例如搜索引擎、自动翻译、智能助手等，帮助人类完成某种特定任务。这也叫“弱人工智能”或“狭义人工智能”。像人类一样能认知、 思考和判断，却模拟人类的智能，这是人工智能学科， 开始就希望达到的目标。这样的智能也叫“通用人工智能”或“强人工智能”。
-第二个层面，从技术的特点来看:
 如果能让运行程序的计算机来学习并自动掌握某些规律，某种程度上可以谈是实现了狭义的人工智能，这种方法即机器学习。

**机器学习**

-机器学习的具体过程：
 1.选择一个模型结构(例如逻辑回归、决策树等)，构建上文中的程序。
 2.将训练数据(包含输入和输出)输入模型中，不断学习，得到经验E.
 3.通过不断执行任务T并衡量效果P，让P不断提高，直到达到一个满意的值。

-机器学习类型：
1. 监督学习（Supervised Learning）

    通过标注的数据来学习，例如，程序通过学习标注了正确答案的手写数字的图像数据，它就能认识其他的手写数字。

2. 无监督学习（Unsupervised Learning）

    通过没有标注的数据来学习。这种算法可以发现数据中自然形成的共同特性（聚类），可以用来发现不同数据之间的联系，例如，买了商品A的顾客往往也购买了商品B。

3. 强化学习（Reinforcement Learning）

    我们可以让程序选择和它的环境互动（例如玩一个游戏），环境给程序的反馈是一些“奖励”（例如游戏中获得高分），程序要学习到一个模型，能在这种环境中得到高的分数，不仅是当前局面要得到高分，而且最终的结果也要是高分才行。

<img src="http://p7.itc.cn/q_70/images03/20200602/5b29b77965a04a28a74640640310cf78.jpeg" width="500" />

**神经网络**

-神经网络模型是一个重要的方法，它的原型在1943年就出现了，在生物神经网络中，每个神经元与其他神经元相连，当它兴奋时，就会像相邻的神经元发送化学物质，从而改变这些神经元内的电位;如果某神经元的电位超过一个國值，那么它就会被激活(兴奋)，向其他神经元发送化学物质。把许多这样的神经元按照定的层次结构连接起来，我们就构建了一个神经网络。

<img src="http://xilinx.eetrend.com/files-eetrend-xilinx/imagecache/image600/blog/201707/11644-30925-shen_du_xue_xi_2.jpg" width="500" />

# 2 范式的演化

-第一阶段：经验
 从几千年前到几百年前，人们通过观察自然现象来归纳总结一些规律。

-第二阶段：理论
 这一阶段，科学家们开始明确定义，速度是什么，质量是什么，化学元素是什么(不再是五行和燃素) ...也开始构建各种模型，在模型中尽量撇除次要和无关因素，例如我们在中学的物理实验中，要假设“斜面足够光滑，无摩擦力”，“空 气阻力可以忽略不计”，等等。在这个理论推导阶段，以伽利略、牛顿为代表的科学家，开启了现代科学之门。伽利略在比萨斜塔做的试验推翻了两千多年来人们想当然的“定律”。

-第三阶段：计算仿真
 从二十世纪中期开始，利用电子计算机对科学实验进行模拟仿真的模式得到迅速普及，人们可以对复杂现象通过模拟仿真，推演更复杂的现象，典型案例如模拟核试验、天气预报等。这样计算机仿真越来越多地取代实验，逐渐成为科研的常规方法。科学家先定义问题，确认假设，再利用数据进行分析和验证。

-第四阶段：数据探索
 最后我们到了“数据探索”阶段。在这个阶段，科学家收集数据，分析数据，探索新的规 律。在深度学习的浪潮中出现的许多结果就是基于海量数据学习得来的。有些数据并不是 从现实世界中收集而来，而是由计算机程序自己生成，例如，在ALPHAGO算法训练的过程 中，它和自己对弈了数百万局，这个数量大大超过了所有记录下来的职业选手棋谱的数量。


# 3 反向传播和梯度下降

-通俗的理解三大概念

例（打靶）：小明拿了一支没有准星的步枪，射击100米外的靶子（如下图），打 靶时会遇到各种干扰因素。 
 • 目的：打中靶心。 
 • 初始化：随便打一枪，能上靶就行，但是要记住当时的步枪的姿态。 
 • 前向计算：让子弹飞一会儿，击中靶子。 
 • 损失函数：环数，偏离角度。 
 • 反向传播：把靶子拉回来看。 
 • 梯度下降：根据本次的偏差，调整步枪的射击角度。

-总结反向传播与梯度下降的基本工作原理和步骤如下： 
 • 初始化。 
 • 正向计算。 
 • 损失函数：为我们提供了计算损失的方法。 
 • 梯度下降：在损失函数基础上向着损失最小的点靠近，从而指引了网络权重调整的方向。 
 • 反向传播：把损失值反向传给神经网络的各层，让各层都可以根据损失值反向调整权重。 
 • 重复正向计算过程，直到精度满足要求。

# 4 线性反向传播

-正向计算的实例

 假设有一个函数：
 
import numpy as np

def target_function(w,b):
    x = 2*w+3*b
    y=2*b+1
    z=x*y
    return x,y,z

def single_variable(w,b,t):
    print("\nsingle variable: b ----- ")
    error = 1e-5
    while(True):
        x,y,z = target_function(w,b)
        delta_z = z - t
        print("w=%f,b=%f,z=%f,delta_z=%f"%(w,b,z,delta_z))
        if abs(delta_z) < error:
            break
        delta_b = delta_z /63
        print("delta_b=%f"%delta_b)
        b = b - delta_b

    print("done!")
    print("final b=%f"%b)

def single_variable_new(w,b,t):
    print("\nsingle variable new: b ----- ")
    error = 1e-5
    while(True):
        x,y,z = target_function(w,b)
        delta_z = z - t
        print("w=%f,b=%f,z=%f,delta_z=%f"%(w,b,z,delta_z))
        if abs(delta_z) < error:
            break
        factor_b = 2*x+3*y
        delta_b = delta_z/factor_b
        print("factor_b=%f, delta_b=%f"%(factor_b, delta_b))
        b = b - delta_b

    print("done!")
    print("final b=%f"%b)

def double_variable_new(w,b,t):
    print("\ndouble variable new: w, b -----")
    error = 1e-5
    while(True):
        x,y,z = target_function(w,b)
        delta_z = z - t
        print("w=%f,b=%f,z=%f,delta_z=%f"%(w,b,z,delta_z))
        if abs(delta_z) < error:
            break

        factor_b, factor_w = calculate_wb_factor(x,y)
        delta_b = delta_z/factor_b/2
        delta_w = delta_z/factor_w/2
        print("factor_b=%f, factor_w=%f, delta_b=%f, delta_w=%f"%(factor_b, factor_w, delta_b,delta_w))
        b = b - delta_b
        w = w - delta_w
    print("done!")
    print("final b=%f"%b)
    print("final w=%f"%w)

def calculate_wb_factor(x,y):
    factor_b = 2*x+3*y
    factor_w = 2*y
    return factor_b, factor_w

if __name__ == '__main__':
    w = 3
    b = 4
    t = 150
    single_variable(w,b,t)
    single_variable_new(w,b,t)
    double_variable(w,b,t)
    double_variable_new(w,b,t)

# 5 非线性反向传播

import numpy as np
import matplotlib.pyplot as plt

def draw_fun(X,Y):
    x = np.linspace(1.2,10)
    a = x*x
    b = np.log(a)
    c = np.sqrt(b)
    plt.plot(x,c)

    plt.plot(X,Y,'x')

    d = 1/(x*np.sqrt(np.log(x**2)))
    plt.plot(x,d)
    plt.show()


def forward(x):
    a = x*x
    b = np.log(a)
    c = np.sqrt(b)
    return a,b,c

def backward(x,a,b,c,y):
    loss = c - y
    delta_c = loss
    delta_b = delta_c * 2 * np.sqrt(b)
    delta_a = delta_b * a
    delta_x = delta_a / 2 / x
    return loss, delta_x, delta_a, delta_b, delta_c

def update(x, delta_x):
    x = x - delta_x
    if x < 1:
        x = 1.1
    return x

if __name__ == '__main__':
    print("how to play: 1) input x, 2) calculate c, 3) input target number but not faraway from c")
    print("input x as initial number(1.2,10), you can try 1.3:")
    line = input()
    x = float(line)
    
    a,b,c = forward(x)
    print("c=%f" %c)
    print("input y as target number(0.5,2), you can try 1.8:")
    line = input()
    y = float(line)

    error = 1e-3

    X,Y = [],[]

    for i in range(20):
        # forward
        print("forward...")
        a,b,c = forward(x)
        print("x=%f,a=%f,b=%f,c=%f" %(x,a,b,c))
        X.append(x)
        Y.append(c)
        # backward
        print("backward...")
        loss, delta_x, delta_a, delta_b, delta_c = backward(x,a,b,c,y)
        if abs(loss) < error:
            print("done!")
            break
        # update x
        x = update(x, delta_x)
        print("delta_c=%f, delta_b=%f, delta_a=%f, delta_x=%f\n" %(delta_c, delta_b, delta_a, delta_x))

    
    draw_fun(X,Y)

# 6 梯度下降

-从自然现象中理解梯度下降

在大多数文章中，都以“一个人被困在山上，需要迅速下到谷底”来举例，这个人会“寻找当前所处位置最陡峭的地方向下走”。这个例子中忽略了安全因素，这个人不可能沿着最陡峭的方向走，要考虑坡度。

在自然界中，梯度下降的最好例子，就是泉水下山的过程：

1. 水受重力影响，会在当前位置，沿着最陡峭的方向流动，有时会形成瀑布（梯度下降）；
2. 水流下山的路径不是唯一的，在同一个地点，有可能有多个位置具有同样的陡峭程度，而造成了分流（可以得到多个解）；
3. 遇到坑洼地区，有可能形成湖泊，而终止下山过程（不能得到全局最优解，而是局部最优解）。
   
# 7 损失函数

-损失函数的作用

损失函数的作用，就是计算神经网络每次迭代的前向计算结果与真实值的差距，从而指导下一步的训练向正确的方向进行。

如何使用损失函数呢？具体步骤：

1. 用随机值初始化前向计算公式的参数；
2. 代入样本，计算输出的预测值；
3. 用损失函数计算预测值和标签值（真实值）的误差；
4. 根据损失函数的导数，沿梯度最小方向将误差回传，修正前向计算公式中的各个权重值；
5. 进入第2步重复, 直到损失函数值达到一个满意的值就停止迭代。