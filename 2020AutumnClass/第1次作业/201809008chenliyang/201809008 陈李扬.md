<style>
table {
margin: auto;
}
</style>



# Step 1

##  介绍

* **人工神经网络(Artificial Neural Netork,即ANN)** 是由简单神经元经过相互连接形成网状结构，通过调节各连接的权重值改变连接的强度，进而实现感知判断

* **反向传播(Back Propagation,BP)** 算法的提出进一步推动了神经网络的发展。目前，神经网络作为一种重要的数据挖掘方法，已在医学诊断、信用卡欺诈识别、手写数字识别以及发动机的故障诊断等领域得到了广泛的应用
  
## 神经网络

* ### **“神经网络”** 的图示:
<div align=center>
<img src="https://github.com/q706389036/photo/blob/master/400px-Network331.png?raw=true" width="30%" />
</div>


我们使用圆圈来表示神经网络的输入，标上“$+$ $1$”的圆圈被称为**偏置节点**，也就是截距项。神经网络最左边的一层叫做**输入层**，最右的一层叫做**输出层**（本例中，输出层只有一个节点）。中间所有节点组成的一层叫做**隐藏层**，因为我们不能在训练样本集中观测到它们的值。同时可以看到，以上神经网络的例子中有$3$个**输入单元**（偏置单元不计在内），$3$个**隐藏单元**及一个**输出单元**。

## **神经网络中的三个基本概念**

>### 反向传播

* **BP神经网络**是一种按误差反向传播(简称误差反传)训练的多层前馈网络，其算法称为BP算法，它的基本思想是梯度下降法，利用梯度搜索技术，以期使网络的实际输出值和期望输出值的误差均方差为最小。
  
* 假设有一个函数：

$$z = x \cdot y \tag{1}$$

其中:

$$x = 2w + 3b \tag{2}$$

$$y = 2b + 1 \tag{3}$$

注意这里 $x,y,z$ 不是变量，只是中间计算结果；$w,b$ 才是变量。因为在后面要学习的神经网络中，要最终求解的目标是 $w$ 和 $b$ 的值。
>### 梯度下降

* 在自然界中，梯度下降的最好例子，就是泉水下山的过程：

1. 水受重力影响，会在当前位置，沿着最陡峭的方向流动，有时会形成瀑布（梯度下降）；
2. 水流下山的路径不是唯一的，在同一个地点，有可能有多个位置具有同样的陡峭程度，而造成了分流（可以得到多个解）；
3. 遇到坑洼地区，有可能形成湖泊，而终止下山过程（不能得到全局最优解，而是局部最优解）。

“梯度下降”包含了两层含义：

1. 梯度：函数当前位置的最快上升点；
2. 下降：与导数相反的方向，用数学语言描述就是那个减号。

亦即与上升相反的方向运动，就是下降。

<div align=center>
<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/2/gd_concept.png" width="40%" />
</div>

  
#### **梯度下降的数学公式**：

$$\theta_{n+1} = \theta_{n} - \eta \cdot \nabla J(\theta) \tag{1}$$

#### 其中：

- $\theta_{n+1}$：下一个值；
- $\theta_n$：当前值；
- $-$：减号，梯度的反向；
- $\eta$：学习率或步长，控制每一步走的距离，不要太快以免错过了最佳景点，不要太慢以免时间太长；
- $\nabla$：梯度，函数当前位置的最快上升点；
- $J(\theta)$：函数。

#### **梯度下降的三要素**

1. 当前点；
2. 方向；
3. 步长。

### 一.**单变量梯度下降**

***假设单变量函数***

$$J(x) = x ^2$$

***其微分为***

$$J'(x) = 2x$$

***假设初始位置为***：

$$x_0=1.2$$

***假设学习率***：

$$\eta = 0.3$$

***假设终止条件为 $J(x)<0.01$，迭代过程是***：
```
x=0.480000, y=0.230400
x=0.192000, y=0.036864
x=0.076800, y=0.005898
x=0.030720, y=0.000944
```

<div align=center>
<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/2/gd_single_variable.png" width="55%" />
</div>

### 二.**双变量的梯度下降**

***假设一个双变量函数***：

$$J(x,y) = x^2 + \sin^2(y)$$

***两个一阶偏导数为***

$${\partial{J(x,y)} \over \partial{x}} = 2x$$
$${\partial{J(x,y)} \over \partial{y}} = 2 \sin y \cos y$$

$$双变量梯度下降的迭代过程$$

|迭代次数|x|y|J(x,y)|
|:----:|:----:|:----:|:----:|
|1|3|1|9.708073|
|2|2.4|0.909070|6.382415|
|...|...|...|...|
|15|0.105553|0.063481|0.015166|
|16|0.084442|0.050819|0.009711|

* 可视化结果：梯度下降的过程，从红色的高地一直沿着坡度向下走，直到蓝色的洼地。

|观察角度1|观察角度2|
|--|--|
|<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images\Images\2\gd_double_variable.png">|<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images\Images\2\gd_double_variable2.png">|

>### 损失函数

* **概念**：在各种材料中经常看到的中英文词汇有：误差，偏差，Error，Cost，Loss，损失，代价......意思都差不多，在本书中，使用“损失函数”和“Loss Function”这两个词汇，具体的损失函数符号用 $J$ 来表示，误差值用 $loss$ 表示。

***“损失”就是所有样本的“误差”的总和，亦即（$m$ 为样本数）***：

$$损失 = \sum^m_{i=1}误差_i$$

$$J = \sum_{i=1}^m loss_i$$

在黑盒子的例子中，我们如果说“某个样本的损失”是不对的，只能说“某个样本的误差”，因为样本是一个一个计算的。如果我们把神经网络的参数调整到完全满足独立样本的输出误差为 $0$，通常会令其它样本的误差变得更大，这样作为误差之和的损失函数值，就会变得更大。所以，我们通常会在根据某个样本的误差调整权重后，计算一下整体样本的损失函数值，来判定网络是不是已经训练到了可接受的状态。

* **损失函数的作用**

损失函数的作用，就是计算神经网络每次迭代的前向计算结果与真实值的差距，从而指导下一步的训练向正确的方向进行。

如何使用损失函数呢？具体步骤：

1. 用随机值初始化前向计算公式的参数；
2. 代入样本，计算输出的预测值；
3. 用损失函数计算预测值和标签值（真实值）的误差；
4. 根据损失函数的导数，沿梯度最小方向将误差回传，修正前向计算公式中的各个权重值；
5. 进入第2步重复, 直到损失函数值达到一个满意的值就停止迭代。

# 代码测试

## **Level1_BP_Linear**
<div align=center>
<img src="https://github.com/q706389036/photo/blob/master/1.png?raw=true" width="60%" />
</div>

## **Level2_BP_NoneLinear**
<div align=center>
<img src="https://github.com/q706389036/photo/blob/master/2.png?raw=true" width="60%" />
</div>

<div align=center>
<img src="https://github.com/q706389036/photo/blob/master/2.1.png?raw=true" width="60%" />
</div>

## **Level3_GDSingleVariable**
<div align=center>
<img src="https://github.com/q706389036/photo/blob/master/3.png?raw=true" width="60%" />
<img src="https://github.com/q706389036/photo/blob/master/3.1.png?raw=true" width="60%" />
</div>

## **Level4_GDDoubleVariable**
<div align=center>
<img src="https://github.com/q706389036/photo/blob/master/4.png?raw=true" width="60%" />
<img src="https://github.com/q706389036/photo/blob/master/4.1.png?raw=true" width="60%" />
</div>

## **Level5_LearningRate**
<div align=center>
<img src="https://github.com/q706389036/photo/blob/master/5.1.png?raw=true" width="60%" />
<img src="https://github.com/q706389036/photo/blob/master/5.2.png?raw=true" width="60%" />
<img src="https://github.com/q706389036/photo/blob/master/5.3.png?raw=true" width="60%" />
<img src="https://github.com/q706389036/photo/blob/master/5.4.png?raw=true" width="60%" />
<img src="https://github.com/q706389036/photo/blob/master/5.5.png?raw=true" width="60%" />
<img src="https://github.com/q706389036/photo/blob/master/5.6.png?raw=true" width="60%" />
<img src="https://github.com/q706389036/photo/blob/master/5.7.png?raw=true" width="60%" />
</div>

# **心得**

第一章主要学习了几个常用软件：GitHub、python、vscode、git。这使我的学习能力更上一层楼。虽然之前没有学过python，但是我们学过C语言和Java，基础的python也大体上能看懂一部分，剩下的一部分就要自己学习。合理的运用这些软件，会让学习变得更加简单。

这门课主要是和人工智能有关。通过一个星期的学习，大致了解到了神经网络的基本工作原理，什么是反向传播和梯度下降，这些在人工智能领域都是非常重要的一部分。不懂的地方还有很多，还需要继续学习。
# <font color="red">加油！！！</font>