
### 人工智能的定义：

1. 能地把某件特定的事情做好，在某个领域增强人类的智慧，这种方式又叫做智能增强。
2. 像人类一样能认知，思考，判断：模拟人类的智能。
### 机器学习：
1. 监督学习（Supervised Learning）
   
通过标注的数据来学习，例如，程序通过学习标注了正确答案的手写数字的图像数据，它就能认识其他的手写数字。

2. 无监督学习（Unsupervised Learning）
   
通过没有标注的数据来学习。这种算法可以发现数据中自然形成的共同特性（聚类），可以用来发现不同数据之间的联系，例如，买了商品A的顾客往往也购买了商品B。

3. 强化学习（Reinforcement Learning）
   
我们可以让程序选择和它的环境互动（例如玩一个游戏），环境给程序的反馈是一些“奖励”（例如游戏中获得高分），程序要学习到一个模型，能在这种环境中得到高的分数，不仅是当前局面要得到高分，而且最终的结果也要是高分才行。
### 人工智能的发展
![](2020-10-05-21-19-47.png)
###范式演化
范式演化的四个阶段
- 第一阶段：经验
- 第二阶段：理论
- 第三阶段：计算仿真
- 第四阶段：数据探索
  
### 神经网络的基本工作原理简介
1. 神经元细胞的数学模型
- 输入
- 权重
- 偏移
- 求和计算
- 激活函数
  ![](2020-10-05-23-04-29.png)
1. 激活函数 

   激活函数就相当于关节

    如果我们不运用激活函数的话，则输出信号将仅仅是一个简单的线性函数。线性函数一个一级多项式。线性方程是很容易解决的，但是它们的复杂性有限，并且从数据中学习复杂函数映射的能力更小。一个没有激活函数的神经网络将只不过是一个线性回归模型罢了，不能解决现实世界中的大多数非线性问题。

    没有激活函数，我们的神经网络将无法学习和模拟其他复杂类型的数据，例如图像、视频、音频、语音等。这就是为什么我们要使用人工神经网络技术，诸如深度学习，来理解一些复杂的事情，一些相互之间具有很多隐藏层的非线性问题。

常见的激活函数：
![](2020-10-05-23-05-53.png)
### 神经网络中的三个基本概念
三大概念：
- 反向传播
- 梯度下降
- 损失函数
  
1. 初始化；
2. 正向计算；
3. 损失函数为我们提供了计算损失的方法；
4. 梯度下降是在损失函数基础上向着损失最小的点靠近而指引了网络权重调整的方向；
5. 反向传播把损失值反向传给神经网络的每一层，让每一层都根据损失值反向调整权重；
6. Go to 2，直到精度足够好；

### 线性反向传播
![](2020-10-05-23-12-15.png)
### 梯度下降


梯度下降的三要素
当前点、方向、步长。


- 梯度：函数当前位置的最快上升点；
- 下降：与导数相反的方向，用数学语言描述就是那个减号。
  
### 损失函数概论
1. 概念
   在各种材料中经常看到的中英文词汇有：误差，偏差，Error，Cost，Loss，损失，代价......意思都差不多，在本书中，使用“损失函数”和“Loss Function”这两个词汇，具体的损失函数符号用 $J$ 来表示，误差值用 $loss$ 表示。
 ![](2020-10-05-23-39-21.png)
2. 神经网络中常用的损失函数
- 均方差函数，主要用于回归
- 交叉熵函数，主要用于分类
  二者都是非负函数，极值在底部，用梯度下降法可以求解。
### 均方差函数
  要想得到预测值 $a$ 与真实值 $y$ 的差距，最朴素的想法就是用 $Error=a_i-y_i$。

对于单个样本来说，这样做没问题，但是多个样本累计时，$a_i-y_i$ 可能有正有负，误差求和时就会导致相互抵消，从而失去价值。所以有了绝对值差的想法，即 $Error=|a_i-y_i|$ 。
### 交叉熵损失函数
交叉熵（Cross Entropy）是Shannon信息论中一个重要概念，主要用于度量两个概率分布间的差异性信息。在信息论中，交叉熵是表示两个概率分布 $p,q$ 的差异，其中 $p$ 表示真实分布，$q$ 表示预测分布，那么 $H(p,q)$ 就称为交叉熵：

$$H(p,q)=\sum_i p_i \cdot \ln {1 \over q_i} = - \sum_i p_i \ln q_i \tag{1}$$
![](2020-10-05-23-46-36.png)