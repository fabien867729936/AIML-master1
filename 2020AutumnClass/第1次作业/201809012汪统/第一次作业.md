#人工智能的定义
##三个层面
####第一个层面，人们对人工智能的期待可以分为：
1.智能地把某件特定的事情做好，在某个领域增强人类的智慧，这种方式又叫做智能增强。
2.像人类一样能认知，思考，判断：模拟人类的智能。
####第二个层面，从技术的特点来看。
如果一个程序解决任务（T）的效能（用P表示）随着经验（E）得到了提高，那么，这个程序就能从经验（E）中学到了关于任务（T）的知识，并让衡量值（P）得到提高。^{[2]} 
[2]
 

选择一个模型结构（例如逻辑回归，决策树等），这就是上面说的程序。
用训练数据（输入和输出）输入模型。这就是上面的经验（E）。
通过不断执行任务（T）并衡量结果（P），让P
不断提高，直到达到一个满意的值。
####第三个层面，从应用的角度来看，我们看到狭义人工智能在各个领域都取得了很大的成果
翻译领域（微软的中英翻译超过人类）
阅读理解（SQuAD 比赛）
下围棋（2016）德州扑克（2019）麻将（2019）
#神经网络的基本定义
##1.神经元
它是神经网络的基本单位。它获得一定数量的输入和一个偏置值。当信号值到达时会乘以一个权值。如果神经元有4个输入，那么就有4个权值，权重可以在训练时调整。
##2、连接
将一个神经元连接到另一层或同一层的另一个神经元。连接伴随着与之相关联的权值。训练的目标是更新此权值以减少损失（即错误）。
##3、偏移
它是神经元的额外输入，它始终为1，并具有自己的连接权重。这确保即使所有的输入都为空（全部为0），神经元也会激活。
##4、传递函数
激活函数用于将非线性引入神经网络。我们可以把它看作格式化输出结果，将结果变成我们可以使用的一种符号或者数字。
##5、输入层 
神经网络中的第一层。它需要输入信号（值）并将它们传递到下一层。它不对输入信号（值）做任何操作，并且没有关联的权重和偏置值。
#官方定义
神经网络是一种运算模型，由大量的节点（或称神经元）之间相互联接构成。每个节点代表一种特定的输出函数，称为激励函数（activation function）。每两个节点间的连接都代表一个对于通过该连接信号的加权值，称之为权重，这相当于人工神经网络的记忆。网络的输出则依网络的连接方式，权重值和激励函数的不同而不同。而网络自身通常都是对自然界某种算法或者函数的逼近，也可能是对一种逻辑策略的表达
#一、反向传播
反向传播定义：我们使用神经网络时，我们输入某个向量x，然后网络产生一个输出y，这个输入向量通过每一层隐含层，直到输出层。这个方向的流动叫做正向传播。然后，代价通过反向算法返回到网络中，调整权重并计算梯度
####1、线性反向传播
####2、非线性反向传播
#二、梯度下降
基本定义：梯度下降是迭代法的一种,可以用于求解最小二乘问题(线性和非线性都可以)。在求解机器学习算法的模型参数，即无约束优化问题时，梯度下降（Gradient Descent）是最常采用的方法之一。在机器学习中，基于基本的梯度下降法发展了两种梯度下降方法，分别为随机梯度下降法和批量梯度下降法。
梯度下降的目的：梯度下降的目的就是使得x值向极值点逼近。
#三、损失函数
基本定义：如果我们把神经网络的参数调整到完全满足独立样本的输出误差为 00，通常会令其它样本的误差变得更大，这样作为误差之和的损失函数值，就会变得更大。所以，我们通常会在根据某个样本的误差调整权重后，计算一下整体样本的损失函数值，来判定网络是不是已经训练到了可接受的状态。
损失函数的作用：损失函数的作用，就是计算神经网络每次迭代的前向计算结果与真实值的差距，从而指导下一步的训练向正确的方向进行。
使用步骤：
1.用随机值初始化前向计算公式的参数；
2.代入样本，计算输出的预测值；
3.用损失函数计算预测值和标签值（真实值）的误差；
4.根据损失函数的导数，沿梯度最小方向将误差回传，修正前向计 算公式中的各个权重值；
5.进入第2步重复, 直到损失函数值达到一个满意的值就停止迭代。
####均方差损失函数
基本定义：该函数就是最直观的一个损失函数了，计算预测值和真实值之间的欧式距离。预测值和真实值越接近，两者的均方差就越小。

####交叉熵损失函数
基本定义：交叉熵（Cross Entropy）是Shannon信息论中一个重要概念，主要用于度量两个概率分布间的差异性信息。
####为什么不能使用均方差做为分类问题的损失函数？
1、回归问题通常用均方差损失函数，可以保证损失函数是个凸函数，即可以得到最优解。而分类问题如果用均方差的话，损失函数的表现不是凸函数，就很难得到最优解。而交叉熵函数可以保证区间内单调。

2、分类问题的最后一层网络，需要分类函数，Sigmoid或者Softmax，如果再接均方差函数的话，其求导结果复杂，运算量比较大。用交叉熵函数的话，可以得到比较简单的计算结果，一个简单的减法就可以得到反向误差。


####相对熵
相对熵又称KL散度，如果我们对于同一个随机变量 $x$ 有两个单独的概率分布 $P(x)$ 和 $Q(x)$，我们可以使用 KL 散度（Kullback-Leibler (KL) divergence）来衡量这两个分布的差异，这个相当于信息论范畴的均方差。

KL散度的计算公式：

$$D_{KL}(p||q)=\sum_{j=1}^n p(x_j) \ln{p(x_j) \over q(x_j)} \tag{4}$$

$n$ 为事件的所有可能性。$D$ 的值越小，表示 $q$ 分布和 $p$ 分布越接近。

####交叉熵
把上述公式变形：

$$
\begin{aligned}  
D_{KL}(p||q)&=\sum_{j=1}^n p(x_j) \ln{p(x_j)} - \sum_{j=1}^n p(x_j) \ln q(x_j) \\\\
&=- H(p(x)) + H(p,q) 
\end{aligned}
\tag{5}
$$

等式的前一部分恰巧就是 $p$ 的熵，等式的后一部分，就是交叉熵：

$$H(p,q) =- \sum_{j=1}^n p(x_j) \ln q(x_j) \tag{6}$$