step 1
#### 1.0.人工智能发展简史
起伏模式：
1. 研究（包括技术）取得进展。
2. 研究的进展让人们看到人工智能的潜力，产生非常乐观的期望，例如在1958年到1970年间科学家对人工智能各种突破的预计，当然他们的绝大多数预计都太乐观了。
3. 上述过高的期望让产业界开始热情地开发各种应用。
4. 但应用未能全部满足期望，于是人工智能行业进入低谷，直到下一波研究和技术取得突破性进展。在2007年之后，是大规模的数据和廉价的计算能力，让神经网络技术再度兴起，成为AI领域的明星技术。
#### 1.1.人工智能的定义
智能地把某件特定的事情做好，在某个领域增强人类的智慧，这种方式又叫做智能增强。
像人类一样能认知，思考，判断：模拟人类的智能。
#### 1.3神经网络的工作原理
1. 输入：
（x1,x2,x3）是外界输入信号。一般是一个训练数据样本的多个属性。
2. 权重：
（w1,w2,w3）是每个输入信号的权重值，权重值之和可以不为一。
3. 偏移：
在脑神经细胞中，一定是输入信号的电平/电流大于某个临界值时，神经元细胞才会处于兴奋状态，这个 b 实际就是那个临界值。
4. 求和计算：
Z=w1*x1+w2*x2+w3*x3+b 当我们把wi*xi变成矩阵运算 Z=w*x+b.
5. 激活函数：
求和之后，神经细胞已经处于兴奋状态了，已经决定要向下一个神经元传递信号了，但是要传递多强烈的信号，要由激活函数来确定。如果激活函数是一个阶跃信号的话，会像继电器开合一样咔咔的开启和闭合，在生物体中是不可能有这种装置的，而是一个渐渐变化的过程。所以一般激活函数都是有一个渐变的过程，也就是说是个曲线。

1. 一个神经元可以有多个输入。
2. 一个神经元只能有一个输出，这个输出可以同时输入给多个神经元。
3. 一个神经元的 ww 的数量和输入的数量一致。
4. 一个神经元只有一个 b。
5. w 和 b 有人为的初始值，在训练过程中被不断修改。
6. A 可以等于 Z，即激活函数不是必须有的。
7. 一层神经网络中的所有神经元的激活函数必须一致。

#### 1.4.神经网络的主要功能
回归或者叫做拟合
单层的神经网络能够模拟一条二维平面上的直线，从而可以完成线性分割任务。而理论证明，两层神经网络可以无限逼近任意连续函数。
神经网络中的三个基本概念：
<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/2/flow1.png"/>
#### 1. 线性反向传播：总的来说就是链式求导法则反复用。注意这里 x,y,z 不是变量，只是中间计算结果；w,b 才是变量。因为在后面要学习的神经网络中，要最终求解的目标是 w 和 b 的值。最终的 zz 值，受到了前面很多因素的影响：变量 w，变量 b，计算式 x，计算式 y。
#### 2. 非线性反向传播：解决一些线性解决不了的问题。
#### 3. 梯度下降三要素：

### 1.当前点；
### 2.方向；
### 3.步长。
含义：梯度：函数当前位置的最快上升点；
下降：与导数相反的方向，用数学语言描述就是那个减号。


#### 3.0 损失函数
### 2.3.5 学习率η的选择

在公式表达时，学习率被表示为$\eta$。在代码里，我们把学习率定义为`learning_rate`，或者`eta`。针对上面的例子，试验不同的学习率对迭代情况的影响，
<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/3/gd2d.png" />
图3-1 单变量的损失函数图

图3-1中，纵坐标是损失函数值，横坐标是变量。不断地改变变量的值，会造成损失函数值的上升或下降。而梯度下降算法会让我们沿着损失函数值下降的方向前进。

1. 假设我们的初始位置在 $A$ 点，$x=x_0$，损失函数值（纵坐标）较大，回传给网络做训练；
2. 经过一次迭代后，我们移动到了 $B$ 点，$x=x_1$，损失函数值也相应减小，再次回传重新训练；
3. 以此节奏不断向损失函数的最低点靠近，经历了 $x_2,x_3,x_4,x_5$；
4. 直到损失值达到可接受的程度，比如 $x_5$ 的位置，就停止训练.
### 3.0.4 神经网络中常用的损失函数

- 均方差函数，主要用于回归

- 交叉熵函数，主要用于分类

二者都是非负函数，极值在底部，用梯度下降法可以求解。
## 3.1 均方差函数
<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/3/mse1.png" ch="500" />

图3-3 平面上的样本数据

图3-4中，前三张显示了一个逐渐找到最佳拟合直线的过程。

- 第一张，用均方差函数计算得到 $Loss=0.53$；
- 第二张，直线向上平移一些，误差计算 $Loss=0.16$，比图一的误差小很多；
- 第三张，又向上平移了一些，误差计算 $Loss=0.048$，此后还可以继续尝试平移（改变 $b$ 值）或者变换角度（改变 $w$ 值），得到更小的损失函数值；
- 第四张，偏离了最佳位置，误差值 $Loss=0.18$，这种情况，算法会让尝试方向反向向下。
## 3.2 交叉熵损失函数
交叉熵（Cross Entropy）是Shannon信息论中一个重要概念，主要用于度量两个概率分布间的差异性信息。在信息论中，交叉熵是表示两个概率分布 p,q 的差异，其中 pp 表示真实分布，q 表示预测分布。
#### 相对熵(KL散度)

相对熵又称KL散度，如果我们对于同一个随机变量 x 有两个单独的概率分布 P(x) 和 Q(x)，我们可以使用 KL 散度来衡量这两个分布的差异，这个相当于信息论范畴的均方差。
$$D_{KL}(p||q)=\sum_{j=1}^n p(x_j) \ln{p(x_j) \over q(x_j)} \tag{4}$$
#### 交叉熵

把上述公式变形：

$$
\begin{aligned}  
D_{KL}(p||q)&=\sum_{j=1}^n p(x_j) \ln{p(x_j)} - \sum_{j=1}^n p(x_j) \ln q(x_j) \\\\
&=- H(p(x)) + H(p,q) 
\end{aligned}
\tag{5}
$$

等式的前一部分恰巧就是 $p$ 的熵，等式的后一部分，就是交叉熵：

$$H(p,q) =- \sum_{j=1}^n p(x_j) \ln q(x_j) \tag{6}$$

在机器学习中，我们需要评估标签值 $y$ 和预测值 $a$ 之间的差距，使用KL散度刚刚好，即 $D_{KL}(y||a)$，由于KL散度中的前一部分 $H(y)$ 不变，故在优化过程中，只需要关注交叉熵就可以了。所以一般在机器学习中直接用交叉熵做损失函数来评估模型。

$$loss =- \sum_{j=1}^n y_j \ln a_j \tag{7}$$
###  为什么不能使用均方差做为分类问题的损失函数？

1. 回归问题通常用均方差损失函数，可以保证损失函数是个凸函数，即可以得到最优解。而分类问题如果用均方差的话，损失函数的表现不是凸函数，就很难得到最优解。而交叉熵函数可以保证区间内单调。

2. 分类问题的最后一层网络，需要分类函数，Sigmoid或者Softmax，如果再接均方差函数的话，其求导结果复杂，运算量比较大。用交叉熵函数的话，可以得到比较简单的计算结果，一个简单的减法就可以得到反向误差。