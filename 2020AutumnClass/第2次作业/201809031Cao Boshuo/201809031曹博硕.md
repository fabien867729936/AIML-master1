# 第二次作业
## 预习
### 线性回归
#### 单变量线性回归问题

回归分析是一种数学模型。当因变量和自变量为线性关系时，它是一特殊的线性模型。

最简单的情形是一元线性回归，由大体上有线性关系的一个自变量和一个因变量组成，模型是：

Y = a+bX+varepsilon

X是自变量，Y是因变量，varepsilon是随机误差，a和b是参数，在线性回归模型中，a,b是我们通过算法学习出来的。

什么叫模型？第一次接触这个概念时，可能会有些不明觉厉。从常规概念上讲，是人们通过主观意识借助实体或者虚拟表现来构成对客观事物的描述，这种描述通常是有一定的逻辑或者数学含义的抽象表达方式。

对于线性回归模型，有如下一些概念需要了解：

- 通常假定随机误差 $\varepsilon$ 的均值为 $0$，方差为$σ^2$（$σ^2>0$，$σ^2$ 与 $X$ 的值无关）
- 若进一步假定随机误差遵从正态分布，就叫做正态线性模型
- 一般地，若有 $k$ 个自变量和 $1$ 个因变量（即公式1中的 $Y$），则因变量的值分为两部分：一部分由自变量影响，即表示为它的函数，函数形式已知且含有未知参数；另一部分由其他的未考虑因素和随机性影响，即随机误差
- 当函数为参数未知的线性函数时，称为线性回归分析模型
- 当函数为参数未知的非线性函数时，称为非线性回归分析模型
- 当自变量个数大于 $1$ 时称为多元回归
- 当因变量个数大于 $1$ 时称为多重回归

使用方法：

1. 最小二乘法
2. 梯度下降法
3. 简单的神经网络法
4. 更通用的神经网络算法

#### 最小二乘法

概念：通过最小化误差的平方和寻找数据的最佳函数匹配。利用最小二乘法可以简便地求得未知的数据，并使得这些求得的数据与实际数据之间的平方和为最小。最小二乘法还可以用于曲线拟合。

设拟合曲线的公式为Y=K*X+B
拟合直线的斜率为：
K=（(X*Y)的平均值-（X的平均值*Y的平均值））/（（X*X）的平均值-（X的平均值*X的平均值））
再根据求得的（X的平均值，Y的平均值）和已经确定的斜率K，利用待定系数法求出截距B。

#### 梯度下降法

数学原理：
- 预设函数：Z=x*w+b
- 损失函数：均方差函数
  
梯度下降的原理在于梯度下降以及后面的神经网络都是利用导数传递误差，再通过迭代方式一步一步地逼近真实解。

#### 神经网络法
神经网络法，相当于在用一个最简单的单层单点神经元。
对神经元的描述，分为输入层，输入层只接受一个输入特征，经过参数w,b计算后，直接输出结果。这样一个简单的“网络”，只能解决简单的一元线性回归问题，而且由于是现行的，不需要激活函数，大大简化了程序，而且便于循序渐进地理解各种知识点。权重w,b，在一元线性问题中，w,b都是一个标量。

- 假设输出层为1个神经元，线性预测公式为：Z=X*W+B
- 损失函数使用均方差函数
- 反向传播，由于单层无激活函数的神经网络只能处理线性回归问题，因而梯度下降与反向传播的相关计算与之前的完全相同。其实神经网络法和梯度下降法在本质上是一样的，只是神经网络法使用了一个崭新的编程模式，即以神经元为中心的代码结构设计，便于后面的功能扩充。

神经元的编程模型把梯度下降法包装了一下，这样就进入了神经网络的世界，从而可以有成熟的方法论可以解决更复杂的问题，比如多个神经元协同工作、多层神经网络的协同工作等等。

#### 多样本计算

单样本计算存在的一些缺点
- 前后两个相邻的样本，很可能会对反向传播产生相反的作用而互相抵消。
- 在样本数据量大时，逐个计算会花费很长时间。

梯度下降的三种形式：
单样本随机梯度下降
小批量样本梯度下降，实际工程中通常使用
全批量样本梯度下降，受单个样本影响最小，但是数据量较大，训练过程会变慢


##### 多样本的计算中会涉及矩阵运算，而所以的深度学习框架都对矩阵运算做了优化，会大幅提升运算速度

#### 多变量线性回归

当接收多个变量输入时，成为了多变量线性回归问题。此时要通过可视化方法理解就比较困难，通常需要用变量两两组对的方式来表现。

当变量多于一个时，两个变量的量纲和数值有可能差别很大，这种情况下，我们通常需要对样本特征数据做归一化，然后把数据喂给神经网络进行训练，否则会出现“消化不良”的情况。

#### 多元线性回归模型
- 多元线性回归模型
     Y=A0+A1*X1+A2*X2+...+AK*XK
- 对于一般的应用问题，建立多元线性回归模型时，为了保证回归模型具有优良的解释能力和预测效果，首先需要注意自变量的选择，其准则是：
  
  自变量对因变量必须有显著的影响，并呈密切的线性相关；
  自变量与因变量之间的线性相关必须是真实的，而不是形式上的；
  自变量之间应具有一定的互斥性，即自变量之间的相关程度不应高于自变量与因变量之因的相关程度；
  自变量应具有完整的统计数据，其预测值容易确定

  对于线性回归问题，求解方法可以用传统的数学方法解决这个问题，使用正规方程法，从而得到数学解析解；再使用神经网络方式来求得近似解，从而比较两者的精度，再进一步调试神经网络的参数，打到学习的目的。

- 数据标准化：深度学习的必要步骤之一，已经是大师们的必杀技能。理论层面上，神经网络是以样本在事件中的统计分布概率为基础进行训练和预测的，所以它对于样本数据的要求比较苛刻。
  
- 还原参数值：超参修改，还原真实的W,B值，唯一可以修改的地方就是样本数据特征值的标准化，并没有修改标签值。

- 标签值标准化：预测数据的标准化，只需要把训练数据的最小值和最大值记录下来，在预测时使用它们对预测数据做标准化，就相当于把预测数据“混入”训练数据。代码运行结果与正规方程解非常接近。我们只需要把预测数据看作是训练数据的一个记录，先标准化，再做预测，这样就不需要把权重矩阵还原了。
  
#### 线性二分类

分类问题在很多资料中都称为逻辑回归，其原因就是使用了线性模型加一个LOGISTIC二分类函数，共同构成了这样的分类器。神经网络的一个重要功能就是分类，现实世界中的分类任务复杂多样，但万变不离其宗，可以用同一种模式的神经网络来处理。

逻辑回归模型，逻辑回归是用来计算“事件=Success”和事件“Failure”的概率。当因变量的类型属于二元变量时，我们应该使用逻辑回归。

回忆线性回归，使用一条直线拟合样本数据，而逻辑回归是“拟合”0或1两个数值，而不是具体的连续数值，所以它叫广义线性模型。逻辑回归又称Logistic回归分析，常用于数据挖掘，疾病自动诊断，经济预测等领域。

逻辑回归的另一个名字叫做分类器，分为线性分类器和非线性分类器。

对率函数：既可以做激活函数使用，又可以当作二分类函数使用。在分类任务中，称其为Logistic函数，而作为激活函数时，称其为Sigmoid函数。它的定义域为全体实数，值域为（0，1）

#### 线性多分类

做多分类任务时，可以采用一对一，一对多，多对多的方式，那么神经网络使用的是哪一种方式，是需要思考的问题，SOFTMAX函数是多分类问题的分类函数，通过对它的分析，我们进一步学习多分类的原理，实现，以及可视化结果，从而理解神经网络的工作方式。

多分类的学习策略，线性多分类和非线性多分类的结果比较，直观的看，区别在于不同类别的样本点之间是否可以用一条直线来相互分割。线性多分类可以使用单层结构来解决，而非线性多分类需要使用双层结构。

多分类问题的三种解法
一对一方式:每次先只保留两个类别的数据，训练一个分类器。如果数据有N类，则需要训练C2n个分类器。
一对多方式：处理一个类别时，暂时把其他所有类别看作是一类。
多对多方式：多个类别数据组合，作二分类，预测时结合多个分类结果进行逻辑运算。

多标签学习
- 同时被标注多个标签，区别于多分类问题。

SOFTMAX函数
假设输入值时[3,1,-3],如果直接取MAX操作会变成[1,0,0],符合分类需要，有两个不足：
        分类结果缺少各元素间相差多少的信息，可以理解为“HARD-MAX”
        MAX函数本身不可导，无法应用于反向传播
    所以SOFTMAX函数加了个“SOFT”来模拟MAX的行为，同时保留了相对大小的信息

SOFT函数的特点
- 各个类别概率相加和为1
- 各个类别的概率均为非负

