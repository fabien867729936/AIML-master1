# 20200224第一次作业
## 学号：201702040   姓名：陈少辉

## 一、神经网络的基本工作原理
### 1、工作原理简介
#### ①神经元细胞的数学模型
神经网络由基本的神经元组成，下图就是一个神经元的数学/计算模型
![](./Images/NeuranCell.png)
由五部分组成：
（1）输入 input：(x1,x2,x3) 是外界输入信号，一般是一个训练数据样本的多个属性；
（2）权重 weights：(w1,w2,w3) 是每个输入信号的权重值。
（3）偏移 bias：从生物学上解释，在脑神经细胞中，一定是输入信号的电平/电流大于某个临界值时，神经元细胞才会处于兴奋状态，这个b实际就是那个临界值。
（4）求和计算 sum：$$Z = w1 \cdot x1 + w2 \cdot x2 + w3 \cdot x3 + b = \sum_{i=1}^m(w_i \cdot x_i) + b$$
（5）激活函数 activation：求和之后，神经细胞已经处于兴奋状态了，已经决定要向下一个神经元传递信号了，但是要传递多强烈的信号，要由激活函数来确定。
#### ②神经网络的训练过程
##### 单层神经网络模型
这是一个单层的神经网络，有m个输入 (这里m=3)，有n个输出 (这里n=2)。在单个神经元里，b是个值。但是在神经网络中，我们把b的值永远设置为1，而用b到每个神经元的权值来表示实际的偏移值，亦即(b1,b2)，这样便于矩阵运算。
- $(x1,x2,x3)$是一个样本数据的三个特征值
- $(w11,w12,w13)$是$(x1,x2,x3)$到$n1$的权重
- $(w21,w22,w23)$是$(x1,x2,x3)$到$n2$的权重
- $b1$是$n1$的偏移
- $b2$是$n2$的偏移
![](./Images/OneLayerNN.png)
##### 训练流程
![](./Images/TrainFlow.png)
##### 训练步骤
假设我们有以下训练数据样本：
|Id|x1|x2|x3|Y|
|---|---|---|---|---|
|1|0.5|1.4|2.7|3|
|2|0.4|1.3|2.5|5|
|3|0.1|1.5|2.3|9|
|4|0.5|1.7|2.9|1|
其中，x1，x2，x3是每一个样本数据的三个特征值，Y是样本的真实结果值：

1. 随机初始化权重矩阵，可以根据高斯分布或者正态分布等来初始化。这一步可以叫做"蒙"，但不是瞎蒙。
2. 拿一个或一批数据作为输入，带入权重矩阵中计算，再通过激活函数传入下一层，最终得到预测值。在本例中，我们先用Id-1的数据输入到矩阵中，得到一个A值，假设A=5
3. 拿到Id-1样本的真实值Y=3
4. 计算损失，假设用均方差函数 $Loss = (A-Y)^2=(5-3)^2=4$
5. 根据一些神奇的数学公式（反向微分），把Loss=4这个值用大喇叭喊话，告诉在前面计算的步骤中，影响A=5这个值的每一个权重矩阵，然后对这些权重矩阵中的值做一个微小的修改（当然是向着好的方向修改，这一点可以用数学家的名誉来保证）
6. 用Id-2样本作为输入再次训练（goto 2）
7. 这样不断地迭代下去，直到以下一个或几个条件满足就停止训练：损失函数值非常小；迭代了指定的次数；
 #### ③神经网络中的矩阵运算
下面这个图是一个两层的神经网络，包含隐藏层和输出层，输入层不算做一层：
![](./Images/TwoLayerNN.png)
其中，w1(m,n)，1表示第1层，表示第一层神经网络的权重矩阵，w2(m,n)表示第二层神经网络的权重矩阵。Visio中不容易写上下标，所以形式有所变动。
#### ④神经网络的主要功能
- **回归/拟合 Regression/fitting**
- **分类 Classification**
  单层的神经网络能够模拟一条二维平面上的直线，从而可以完成线性分割任务。而理论证明，两层神经网络可以无限逼近任意连续函数。

比如下面这张图，二维平面中有两类点，红色的和蓝色的，用一条直线肯定不能把两者分开了。

|拟合|分类|
|---|---|
|![](./Images/sgd_result.png)|![](./Images/Sample.png)|
#### ⑤激活函数作用
激活函数就相当于关节。看以下的例子：
$$Z1 = W1 \cdot X + B1$$
$$Z2 = W2 \cdot Z1 + B2$$
$$Z3 = W3 \cdot Z2 + B3$$
展开：
$$Z3=W3 \cdot(W2 \cdot (W1 \cdot X+B1)+B2)+B3$$
$$=(W3W2W1) \cdot X+ (W3W2B1+W3B2+B3)$$
$$=W \cdot X+B$$
$Z1,Z2,Z3$分别代表三层神经网络。最后可以看到，不管有多少层，总可以归结到WX+B的形式，这和单层神经网络没有区别。
如果我们不运用激活函数的话，则输出信号将仅仅是一个简单的线性函数。线性函数一个一级多项式。现如今，线性方程是很容易解决的，但是它们的复杂性有限，并且从数据中学习复杂函数映射的能力更小。一个没有激活函数的神经网络将只不过是一个线性回归模型（Linear regression Model）罢了，它功率有限，并且大多数情况下执行得并不好。

激活函数的另一个重要特征是：它应该是可导的。我们需要这样的特性，以便在网络中向后推进以计算相对于权重的误差（损失）梯度时执行反向优化策略，然后相应地使用梯度下降或任何其他优化技术改变权重以减少误差。
![](./Images/LinearvsActivation.png)
#### ⑥深度神经网络与深度学习介绍
1. 卷积神经网络 CNN (Convolutional Neural Networks)
对于图像类的机器学习问题，最有效的就是卷积神经网络。
![](./Images/conv_net.png)
2. 循环神经网络 RNN (Recurrent Neural Networks)
对于语言类的机器学习问题，最有效的就是循环神经网络。
![](./Images/rnn.png)
## 二、基本函数导数公式
### ① 基本函数及其导数

|公式序号|函数|导数|备注|
|---|---|---|---|
|1|$y=c$|$y'=0$|
|2|$y=x^a$|$y'=ax^{a-1}$|
|3|$y=log_ax$|$y'=\frac{1}{x}log_ae=\frac{1}{xlna}$|
|4|$y=lnx$|$y'=\frac{1}{x}$|
|5|$y=a^x$|$y'=a^xlna$|
|6|$y=e^x$|$y'=e^x$|
|7|$y=e^{-x}$|$y'=-e^{-x}$|
|8|$y=sin(x)$|$y'=cos(x)$|正弦函数|
|9|$y=cos(x)$|$y'=-sin(x)$|余弦函数|
|10|$y=tg(x)$|$y'=sec^2(x)=\frac{1}{cos^2x}$| 正切函数 |
|11|$y=ctg(x)$|$y'=-csc^2(x)$| 余切函数 |
|12|$y=arcsin(x)$|$y'=\frac{1}{\sqrt{1-x^2}}$| 反正弦函数 |
|13|$y=arccos(x)$|$y'=-\frac{1}{\sqrt{1-x^2}}$| 反余弦函数 |
|14|$y=arctan(x)$|$y'=\frac{1}{1+x^2}$| 反正切函数 |
|15|$y=arcctg(x)$|$y'=-\frac{1}{1+x^2}$| 反余切函数 |
|16|$y=sinh(x)=(e^x-e^{-x})/2$|$y'=cosh(x)$|双曲正弦函数 |
|17|$y=cosh(x)=(e^x+e^{-x})/2$|$y'=sinh(x)$|双曲余弦函数 |
|18|$y=tanh(x)=(e^x-e^{-x})/(e^x+e^{-x})$|$y'=sech^2(x)=1-tanh^2(x)$|双曲正切函数|
|19|$y=coth(x)=(e^x+e^{-x})/(e^x-e^{-x})$|$y'=-csch^2(x)$|双曲余切函数|
|20|$y=sech(x)=2/(e^x+e^{-x})$|$y'=-sech(x)*tanh(x)$|双曲正割函数|
|21|$y=csch(x)=2/(e^x-e^{-x})$|$y'=-csch(x)*coth(x)$| 双曲余割函数|
### ② 导数四则运算
$$[u(x) + v(x)]' = u'(x) + v'(x) \tag{30}$$
$$[u(x) - v(x)]' = u'(x) - v'(x) \tag{31}$$
$$[u(x)*v(x)]' = u'(x)*v(x) + v'(x)*u(x) \tag{32}$$
$$[\frac{u(x)}{v(x)}]'=\frac{u'(x)v(x)-v'(x)u(x)}{v^2(x)} \tag{33}$$
### ③偏导数
如$Z=f(x,y)$，则Z对x的偏导可以理解为当y是个常数时，Z单独对x求导：

$$Z'_x=f'_x(x,y)=\frac{\partial{Z}}{\partial{x}} \tag{40}$$

则Z对y的偏导可以理解为当x是个常数时，Z单独对y求导：

$$Z'_y=f'_y(x,y)=\frac{\partial{Z}}{\partial{y}} \tag{41}$$

在二元函数中，偏导的何意义，就是对任意的$y=y_0$的取值，在二元函数曲面上做一个$y=y_0$切片，得到$Z = f(x, y_0)$的曲线，这条曲线的一阶导数就是Z对x的偏导。对$x=x_0$同样，就是Z对y的偏导。
### ④复合函数求导（链式法则）

- 如果 $y=f(u), u=g(x)$ 则：

$$y'_x = f'(u) \cdot u'(x) = y'_u \cdot u'_x=\frac{dy}{du} \cdot \frac{du}{dx} \tag{50}$$

- 如果$y=f(u),u=g(v),v=h(x)$ 则：

$$
\frac{dy}{dx}=f'(u) \cdot g'(v) \cdot h'(x)=\frac{dy}{du} \cdot \frac{du}{dv} \cdot \frac{dv}{dx} \tag{51}
$$

- 如$Z=f(U,V)$，通过中间变量$U = g(x,y), V=h(x,y)$成为x,y的复合函数$Z=f[g(x,y),h(x,y)]$ 则：

$$
\frac{\partial{Z}}{\partial{x}}=\frac{\partial{Z}}{\partial{U}} \cdot \frac{\partial{U}}{\partial{x}} + \frac{\partial{Z}}{\partial{V}} \cdot \frac{\partial{V}}{\partial{x}} \tag{52}
$$

$$
\frac{\partial{Z}}{\partial{y}}=\frac{\partial{Z}}{\partial{U}} \cdot \frac{\partial{U}}{\partial{y}} + \frac{\partial{Z}}{\partial{V}} \cdot \frac{\partial{V}}{\partial{y}}
$$
### ⑤矩阵求导

如$A,B,X$都是矩阵，则：

$$
B\frac{\partial{(AX)}}{\partial{X}} = A^TB \tag{60}
$$

$$
B\frac{\partial{(XA)}}{\partial{X}} = BA^T \tag{61}
$$

$$
\frac{\partial{(X^TA)}}{\partial{X}} = \frac{\partial{(A^TX)}}{\partial{X}}=A \tag{62}
$$

$$
\frac{\partial{(A^TXB)}}{\partial{X}} = AB^T \tag{63}
$$

$$
\frac{\partial{(A^TX^TB)}}{\partial{X}} = BA^T, {dX^TAX \over dX} = (A+A^T)X \tag{64}
$$

$${dX^T \over dX} = I, {dX \over dX^T} = I, {dX^TX \over dX}=2X\tag{65}$$

$${du \over dX^T} = ({du^T \over dX})^T$$

$${du^Tv \over dx} = {du^T \over dx}v + {dv^T \over dx}u^T, {duv^T \over dx} = {du \over dx}v^T + u{dv^T \over dx} \tag{66}$$

$${dAB \over dX} = {dA \over dX}B + A{dB \over dX} \tag{67}$$

$${du^TXv \over dx}=uv^T, {du^TX^TXu \over dX}=2Xuu^T \tag{68}$$

$${d[(Xu-v)^T(Xu-v)] \over dX}=2(Xu-v)u^T \tag{69}$$
### ⑥标量对矩阵导数的定义
假定$y$是一个标量，$X$是一个$N \times M$大小的矩阵，有$y=f(X)$， $f$是一个函数。我们来看$df$应该如何计算。

首先给出定义：

$$
df = \sum_j^M\sum_i^N \frac{\partial{f}}{\partial{x_{ij}}}dx_{ij}
$$

下面我们引入矩阵迹的概念，所谓矩阵的迹，就是矩阵对角线元素之和。也就是说：

$$
tr(X) = \sum_i x_{ii}
$$
#### ⑦矩阵迹和导数的部分性质

这里将会给出部分矩阵的迹和导数的性质，作为后面推导过程的参考。性子急的同学可以姑且默认这是一些结论。

$$
d(X + Y) = dX + dY \tag{93}
$$
$$
d(XY) = (dX)Y + X(dY)\tag{94}
$$
$$
dX^T = {(dX)}^T \tag{95}
$$
$$
d(tr(X)) = tr(dX) \tag{96}
$$
$$
d(X \odot Y) = dX \odot Y + X \odot dY \tag{97}
$$
$$
d(f(X)) = f^{'}(X) \odot dX \tag{98}
$$
$$
tr(XY) = tr(YX) \tag{99}
$$
$$
tr(A^T (B \odot C)) = tr((A \odot B)^T C) \tag{100}
$$
## 三、 反向传播与梯度下降
反向传播与梯度下降的基本工作原理：

1. 初始化
2. 正向计算
3. 损失函数为我们提供了计算损失的方法
4. 梯度下降是在损失函数基础上向着损失最小的点靠近而指引了网络权重调整的方向
5. 反向传播把损失值反向传给神经网络的每一层，让每一层都根据损失值反向调整权重
6. goto 2，直到精度足够好（比如损失函数值小于0.001）

其实我们的祖先早就有了愚公移山、精卫填海的故事......一点儿一点儿，不断地向目标靠近！
## 四、线性反向传播
### ①正向计算

假设我们有一个函数：

$$z = x \cdot y \tag{1}$$

其中:

$$x = 2w + 3b \tag{2}$$

$$y = 2b + 1 \tag{3}$$

计算图如下：
![](./Images/flow1.png)
当w = 3, b = 4时，会得到如下结果：
![](./Images/flow2.png)
### ②反向传播求解w
#### 求w的偏导
我们从z开始一层一层向回看，图中各节点关于变量w的偏导计算结果如下：

$$因为z = x \cdot y，其中x = 2w + 3b，y = 2b + 1$$

所以：

$$\frac{\partial{z}}{\partial{w}}=\frac{\partial{z}}{\partial{x}} \cdot \frac{\partial{x}}{\partial{w}}=y \cdot 2=18 \tag{4}$$

其中：

$$\frac{\partial{z}}{\partial{x}}=\frac{\partial{}}{\partial{x}}(x \cdot y)=y=9$$

$$\frac{\partial{x}}{\partial{w}}=\frac{\partial{}}{\partial{w}}(2w+3b)=2$$

![](./Images/flow3.png)
### ③反向传播求解b
![](./Images/flow4.png)

### ④同时求解w和b的变化值
这次我们要同时改变w和b，到达最终结果为z=150的目的。
已知$\Delta z=12$，我们不妨把这个误差的一半算在w账上，另外一半算在b的账上：
$$\Delta b=\frac{\Delta z / 2}{63} = \frac{12/2}{63}=0.095$$
$$\Delta w=\frac{\Delta z / 2}{18} = \frac{12/2}{18}=0.333$$
$w = w-\Delta w=3-0.333=2.667$
$b = b - \Delta b=4-0.095=3.905$
$x=2w+3b=2 \times 2.667+3 \times 3.905=17.049$
$y=2b+1=2 \times 3.905+1=8.81$
$z=x \times y=17.049 \times 8.81=150.2$
## 五、 非线性反向传播
在上面的线性例子中，我们可以发现，误差一次性地传递给了初始值w和b，即，只经过一步，直接修改w和b的值，就能做到误差校正。因为从它的计算图看，无论中间计算过程有多么复杂，它都是线性的，所以可以一次传到底。缺点是这种线性的组合最多只能解决线性问题，不能解决更复杂的问题。这个我们在神经网络基本原理中已经阐述过了，需要有激活函数连接两个线性单元。
## 六、总结
通过这次课的学习，我学习到了神经网络的基本概念，掌握到了神经网络的基础知识。在这次课上我明白了神经网络的基本工作原理，并通过基本的数学导数公式，明白了反向传播与梯度下降的过程。在后面的学习中，我弄清楚了线性反向传播与非线性反向传播的区别，进一步加深了我对神经网络这门课的认识。
   神经网络这门课需要具备较强的数学与逻辑思维能力，前面的知识是一些基础的概念，我会认真复习巩固的，为后面更深入的学习打下坚实的基础。通过老师的详细讲解以及网上的教学视频的结合，我神经网络这个词有了深入的理解，我明白了"神经网络"就是模拟人的神经网络进行思维与学习，它具备深入学习的能力，是一种模拟人脑的神经网络以期能够实现类人工智能的机器学习技术，同时它是深度学习的基础。我会在老师的讲解中，认真学习这门课，并掌握这一门强大的机器学习方法，同时也更好地理解深度学习技术。

