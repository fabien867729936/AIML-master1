# 学习笔记总结
## step 5 （）非线性分类
用更复杂的数据样本来学习非线性多分类问题，并理解其工作原理。神经网络是通过一些线性变换加激活函数压缩，把线性不可分的问题转化为线性可分问题的。由于样本的复杂性，必须在隐层使用多个神经元才能完成分类任务。
### 10.0-10.6 多入单出双层——非线性二分类
了解双变量非线性二分类的两个提出问题：异或问题、双弧形问题
熟识二分类模型的评估标准。
* 非线性二分类实现
  定义可以完成非线性二分类的神经网络结构图
  根据网络结构，我们学习了前向计算过程以及反向传播过程
* 实现逻辑异或门（问题一）
  学习理解资料中的代码并运行实现逻辑异或门，解决问题一
* 逻辑异或门的工作原理
  之前证明了两层神经网络是可以解决异或问题的，接下来理解了神经网络在这个异或问题的上工作原理，此原理可以扩展到更复杂的问题空间。
* 实现双弧形二分类（问题二）
  学习理解资料中的代码并运行实现双弧形二分类，解决问题二
* 双弧形二分类的工作原理
  通过学习理解资料中的代码并运行得到以下结论：
    1.在第一层的线性变换中，原始样本被斜侧拉伸，角度渐渐左倾到40度，并且样本间距也逐渐拉大，原始样本归一化后在[0,1]之间，最后已经拉到了[-5,15]的范围。这种侧向拉伸实际上是为激活函数做准备。
    2.在激活函数计算中，由于激活函数的非线性，所以空间逐渐扭曲变形，使得红色样本点逐步向右下角移动，并变得稠密；而蓝色样本点逐步向左上方扩撒，相信它的极限一定是[0,1]空间的左边界和上边界；另外一个值得重点说明的就是，通过空间扭曲，红蓝两类之间可以用一条直线分割了！这是一件非常神奇的事情。
    3.最后的分类结果，从毫无头绪到慢慢向上拱起，然后是宽而模糊的分类边界，最后形成非常锋利的边界。
  小结：神经网络通过空间变换的方式，把线性不可分的样本变成了线性可分的样本，从而给最后的分类变得很容易。
### 11.0-11.3 多入多出双层——非线性多分类
了解双变量非线性多分类的提出问题：铜钱孔形问题
熟识多分类模型的评估标准。
* 非线性多分类
  设计出能完成非线性多分类的网络结构
  根据网络结构，我们学习了绘制前向计算图，
  再根据前向计算图，可以绘制出反向传播的路径
* 非线性多分类的工作原理
  了解隐层神经元数与分类结果的关系和神经元数与网络能力及分类结果的关系。
* 分类样本不平衡问题
  学习如何解决样本不平衡问题：
    平衡数据集
    尝试其它评价指标
    尝试产生人工数据样本
    尝试一个新的角度理解问题
    修改现有算法
    集成学习
### 12.0-12.3 多入多出三层——多变量非线性分类
了解双变量非线性多分类的提出问题（手写识别）
* 三层神经网络的实现
  定义神经网络：为了完成MNIST分类，需要设计一个三层神经网络结构。
  前向计算：用大写符号的矩阵形式的公式来描述，在每个矩阵符号的右上角是其形状。
  反向传播：和以前的两层网络没有多大区别，只不过多了一层，而且用了tanh激活函数，目的是想把更多的梯度值回传，因为tanh函数比sigmoid函数稍微好一些，比如原点对称，零点梯度值大。
* 梯度检查
  神经网络算法使用反向传播计算目标函数关于每个参数的梯度，可以看做解析梯度。由于计算过程中涉及到的参数很多，用代码实现的反向传播计算的梯度很容易出现误差，导致最后迭代得到效果很差的参数值。
  为了确认代码中反向传播计算的梯度是否正确，可以采用梯度检验（gradient check）的方法。通过计算数值梯度，得到梯度的近似值，然后和反向传播得到的梯度进行比较，若两者相差很小的话则证明反向传播的代码是正确无误的。
  这里用到所学的数学知识——数值微分
* 学习率与批大小
  对于一个固定的学习率，存在一个最优的batch size能够最大化测试精度，这个batch size和学习率以及训练集的大小正相关。对此实际上是有两个建议：

    1.如果增加了学习率，那么batch size最好也跟着增加，这样收敛更稳定。
    2.尽量使用大的学习率，因为很多研究都表明更大的学习率有利于提高泛化能力。如果真的要衰减，可以尝试其他办法，比如增加batch size，学习率对模型的收敛影响真的很大，慎重调整。
  输入值可以变化很大，但很大的输入值会得到接近于1的输出值。因此batch size和学习率的关系可以大致总结如下：

    1.增加batch size，需要增加学习率来适应，可以用线性缩放的规则，成比例放大
    2.到一定程度，学习率的增加会缩小，变成batch size的$\sqrt m$倍
    3.到了比较极端的程度，无论batch size再怎么增加，也不能增加学习率了