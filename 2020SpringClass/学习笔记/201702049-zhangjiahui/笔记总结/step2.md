# 学习笔记总结
## step 2线性回归（2.26）
用线性回归作为学习神经网络的起点，在它的基础上，逐步的增加一些新的知识点，会形成一条比较平缓的学习曲线，或者说是迈向神经网络的第一个小台阶。单层的神经网络，其实就是一个神经元，可以完成一些线性的工作，比如拟合一条直线，这用一个神经元就可以实现。

当这个神经元只接收一个输入时，就是单变量线性回归，可以在二维平面上用可视化方法理解。当接收多个变量输入时，叫做多变量线性回归，此时可视化方法理解就比较困难了，通常我们会用变量两两组对的方式来表现。当变量多于一个时，两个变量的量纲和数值有可能差别很大，这种情况下，我们通常需要对样本特征数据做归一化，然后把数据喂给神经网络进行训练，否则会出现“消化不良”的情况。
### 4.0-4.5单入单出的单层神经网络——单变量线性回归问题
回归分析是一种数学模型。当因变量和自变量为线性关系时，它是一种特殊的线性模型。最简单的情形是一元线性回归，由大体上有线性关系的一个自变量和一个因变量组成，模型是：

$$Y=a+bX+ε \tag{1}$$
X是自变量，Y是因变量，ε是随机误差，a和b是参数，在线性回归模型中，a和b是我们要通过算法学习出来的。

* 对于线性回归模型，有如下一些概念需要了解：
    通常假定随机误差的均值为0，方差为σ^2（σ^2﹥0，σ^2与X的值无关）；若进一步假定随机误差遵从正态分布，就叫做正态线性模型
    一般地，若有k个自变量和1个因变量（即公式1中的Y），则因变量的值分为两部分：一部分由自变量影响，即表示为它的函数，函数形式已知且含有未知参数；另一部分由其他的未考虑因素和随机性影响，即随机误差；
    当函数为参数未知的线性函数时，称为线性回归分析模型；当函数为参数未知的非线性函数时，称为非线性回归分析模型
    当自变量个数大于1时称为多元回归；当因变量个数大于1时称为多重回归。

如资料中图4-2所示，左侧为线性模型，可以看到直线穿过了一组三角形所形成的区域的中心线，并不要求这条直线穿过每一个三角形。右侧为非线性模型，一条曲线穿过了一组矩形所形成的区域的中心线。学习如何解决左侧的线性回归问题（求解w,b相关公式）：
  * 最小二乘法：也叫做最小平方法（Least Square），它通过最小化误差的平方和寻找数据的最佳函数匹配；
  * 梯度下降法：与最小二乘法比较可以看到，梯度下降法和最小二乘法的模型及损失函数是相同的，都是一个线性模型加均方差损失函数，模型用于拟合，损失函数用于评估效果。区别在于，最小二乘法从损失函数求导，直接求得数学解析解，而梯度下降以及后面的神经网络，都是利用导数传递误差，再通过迭代方式一步一步（用近似解）逼近真实解；
  * 简单的神经网络法；更通用的神经网络算法：在神经网络算法中，用一个最简单的线性回归的例子，来说明神经网络中最重要的反向传播和梯度下降的概念、过程以及代码

### 5.0-5.6多入单出单层——多变量线性回归问题
这个问题是个线性回归问题，而且是典型的多元线性回归，即包括两个或两个以上自变量的回归。多元线性回归的函数模型如下：

$$y=a_0+a_1x_1+a_2x_2+\dots+a_kx_k$$

* 对于一般的应用问题，建立多元线性回归模型时，为了保证回归模型具有优良的解释能力和预测效果，应首先注意自变量的选择，其准则是：

    1.自变量对因变量必须有显著的影响，并呈密切的线性相关；
    2.自变量与因变量之间的线性相关必须是真实的，而不是形式上的；
    3.自变量之间应具有一定的互斥性，即自变量之间的相关程度4.不应高于自变量与因变量之因的相关程度；
    4.自变量应具有完整的统计数据，其预测值容易确定。
解决方案：先使用正规方程，从而可以得到数学解析解，然后再使用神经网络方式来求得近似解，从而比较两者的精度，再进一步调试神经网络的参数，达到学习的目的。
  * 正规方程法 Normal Equations
  对于线性回归问题，除了前面提到的最小二乘法可以解决一元线性回归的问题外，也可以解决多元线性回归问题。
  对于多元线性回归，可以用正规方程来解决，也就是得到一个数学上的解析解。它可以解决下面这个公式描述的问题：

$$y=a_0+a_1x_1+a_2x_2+\dots+a_kx_k \tag{1}$$
  * 神经网络解法
  定义一个一层的神经网络，输入层为2或者更多。这个一层的神经网络的特点是：

    没有中间层，只有输入项和输出层（输入项不算做一层）；
    输出层只有一个神经元；
    神经元有一个线性输出，不经过激活函数处理，即在下图中，经过$\Sigma$求和得到$Z$值之后，直接把$Z$值输出。