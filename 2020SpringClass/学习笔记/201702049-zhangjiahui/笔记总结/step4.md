# 学习笔记总结
## step 4（）非线性回归
接下来开始两层神经网络的学习，从而解决非线性问题。在两层神经网络之间，必须有激活函数连接，从而加入非线性因素，提高神经网络的能力。所以，我们先从激活函数学起，一类是挤压型的激活函数，常用于简单网络的学习；另一类是半线性的激活函数，常用于深度网络的学习。
### 8.0-8.2 函数
* 激活函数
  激活函数的基本性质：
    非线性：线性的激活函数和没有激活函数一样；
    可导性：做误差反向传播和梯度下降，必须要保证激活函数的可导性；
    单调性：单一的输入会得到单一的输出，较大值的输入得到较大值的输出。
    小结：
    神经网络最后一层不需要激活函数；激活函数只用于连接前后两层神经网络
* 挤压型激活函数
  这一类函数的特点是，当输入值域的绝对值较大的时候，其输出在两端是饱和的，都具有S形的函数曲线以及压缩输入值域的作用，所以叫挤压型激活函数，又可以叫饱和型激活函数。

  在英文中，通常用Sigmoid来表示，原意是S型的曲线，在数学中是指一类具有压缩作用的S型的函数，在神经网络中，学习了解了两个常用的Sigmoid函数，一个是Logistic函数，另一个是Tanh
  函数。
  小结：
    Sigmoid，指的是对数几率函数用于激活函数时的称呼；
    Logistic，指的是对数几率函数用于二分类函数时的称呼；
    Tanh，指的是双曲正切函数用于激活函数时的称呼。
* 半线性激活函数（非饱和型激活函数）
  ReLU函数：Rectified Linear Unit，修正线性单元，线性整流函数，斜坡函数。
  Leaky ReLU函数：LReLU，带泄露的线性整流函数。
  Softplus函数、ELU函数
### 9.0-9.7 单入单出双层——非线性回归
* 用多项式回归法拟合正弦曲线
  了解多项式回归的概念（多种形式）
  分别学习了用二次多项式拟合、用三次多项式拟合和用四次多项式拟合，并将不同项数的多项式拟合结果比较。
* 用多项式回归法拟合复合函数曲线
  从正弦曲线的拟合经验来看，三次多项式以下肯定无法解决，所以在这一节中学习从四次多项式开始试验。
  分别学习了用四次多项式拟合、用六次多项式拟合和用八次多项式拟合
* 双层神经网络实现非线性回归
  万能近似定理(universal approximation theorem) $^{[1]}$，是深度学习最根本的理论依据。它证明了在给定网络具有足够多的隐藏单元的条件下，配备一个线性输出层和一个带有任何“挤压”性质的激活函数（如Sigmoid激活函数）的隐藏层的前馈神经网络，能够以任何想要的误差量近似任何从一个有限维度的空间映射到另一个有限维度空间的Borel可测的函数。
  定义神经网络结构：定义一个两层的神经网络，输入层不算，一个隐藏层，含3个神经元，一个输出层。
* 曲线拟合
  试验正弦曲线的拟合
  验复合函数的曲线拟合
* 非线性回归的工作原理
  工作步骤：
  第一步 把X拆成两个线性序列z1和z2
  第二步 计算z1的激活函数值a1
  第三步 计算z2的激活函数值a2
  第四步 计算Z值