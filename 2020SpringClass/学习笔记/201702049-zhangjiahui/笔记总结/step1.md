# 学习笔记总结
## step 1（2.25）
首先了解了人工智能的三种不同层面的定义，范式的演化的四个阶段（经验、理论、计算仿真、数据探索）以及各阶段的应用（经验归纳、理论推导、数据模拟、数据探索）和神经网络的基本工作原理（包括其模型、训练过程、矩阵运算、主要功能）。
  * 小结
    一个神经元可以有多个输入；一个神经元只能有一个输出，这个输出可以同时输入给多个神经元；一个神经元的w的数量和输入的数量一致；一个神经元至于偶一个b；w和b有人为的初始值，在训练过程中被不断修改；A可以等于Z，即激活函数不是必须有的；一层神经网络中所有的神经元的激活函数必须一致。

其次学习了神经网络中的三个基本概念：
    反向传播（线性/非线性）、
    梯度下降（三要素：当前点、方向、步长）、
    损失函数（均方差函数、交叉熵函数）
    通过一些实例直观地了解了这三个概念以及基本工作原理。
  * 小结
    总结反向传播与梯度下降的工作基本原理：
    1.初始化；2.正向计算；
    3.损失函数为我们提供了计算损失的方法；
    4.梯度下降是在损失函数基础上向着损失最小的点靠近而指引了网络权重调整的方向；
    5.反向传播把损失值反向传给神经网络的每一层，让每一层都根据损失值反向调整权重；
    6.goto 2，直到精度足够好（比如损失函数值小于0.001）。

    损失函数的作用，就是计算神经网络每次迭代的前向计算结果与真实值的差距，从而指导下一步的训练向正确的方向进行。
    损失函数的使用步骤：
    1.用随机值初始化前向计算公式的参数；
    2.代入样本，计算输出的预测值；
    3.用损失函数计算预测值和标签值（真实值）的误差；
    4.根据损失函数的导数，沿梯度最小方向将误差回传，修正5.前向计算公式中的各个权重值；
    6.goto 2, 直到损失函数值达到一个满意的值就停止迭代。
    注：均方差函数主要用于回归、交叉熵函数主要用于分类；二者都是非负函数，极值在底部，用梯度下降法可以求解
    回归问题通常用均方差损失函数，可以保证损失函数是个凸函数，即可以得到最优解。而分类问题如果用均方差的话，损失函数的表现不是凸函数，就很难得到最优解。而交叉熵函数可以保证区间内单调。
    分类问题的最后一层网络，需要分类函数，Sigmoid或者Softmax，如果再接均方差函数的话，其求导结果复杂，运算量比较大。用交叉熵函数的话，可以得到比较简单的计算结果，一个简单的减法就可以得到反向误差。