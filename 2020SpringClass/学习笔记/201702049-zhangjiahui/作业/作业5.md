## 作业5 阅读相关资料，总结循环神经网络处理语音识别应用的过程
全连接网络和卷积网络在运行时每次接收的都是独立的输入数据，没有记忆能力。在有些应用中需要神经网络具有记忆功能，典型的是时间序列预测问题，时间序列可以抽象的表示为一个向量序列。这里的下标表示时刻，神经网络每个时刻接收一个向量输入。不同时刻的向量之间存在关系，每个时刻的向量与更早时刻的向量相关。例如，在说话时当前要说的词和之前所说的词之间相关，依赖于上下文语境。我们需要根据输入序列来产生输出向量。这类问题称为序列预测问题，输入序列的长度可能不固定。语音识别与自然语言处理的问题是这类序列预测问题的典型代表。前者的输入是一个时间序列的语音信号；后者是文字序列。

* 循环层的工作原理

  循环神经网络（简称RNN）会记住网络在上一个时刻的输出值，并将该值用于当前时刻输出值的生成，这由循环层实现。RNN的输入为前面介绍的向量序列，每个时刻接收一个输入，网络会产生一个输出，而这个输出是由之前的序列共同作用决定的。假设 t 时刻 循环层的状态值为ht，它由上一时刻的状态值以及当前时刻的输入值共同决定。现在的问题是确定这个表达式的具体形式，即将上一时刻的状态值与当前时刻的输入值整合到一起。在全连接神经网络中，神经元的输出值是对输入值进行加权，然后用激活函数进行作用，得到输出。在这里，我们可以对上一时刻的状态值，当前时刻的输入值进行类似的处理，即将它们分别都乘以权重矩阵，然后整合起来。整合可以采用加法，也可以采用乘法或者更复杂的运算，最简单的是加法，乘法在数值上不稳定，多次乘积之后数为变得非常大或者非常小。显然，这里需要两个权重矩阵，分别作用于上一时刻状态值，当前时刻的输入值，由此得到递推关系式。
* 网络结构
  最简单的循环神经网络由一个输入层，一个循环层，一个输出层组成。输出层接收循环层的输出值作为输入并产生输出，它不具有记忆功能。输出层实现的变换，函数g的类型根据任务而定，对于分类任务一般选用softmax函数，输出各个类的概率。结合循环层和输出层，循环神经网络完成的变换，在这里只使用了一个循环层和一个输出层，实际使用时可以有多个循环层，即深度循环神经网络。
* 深层网络
  上面介绍的循环神经网络只有一个输入层，一个循环层和一个输出层，这是一个浅层网络。和全连接网络以及卷积网络一样，我们可以把它推广到任意多个隐含层的情况，得到深度循环神经网络。
  这里有3种方案，第一种方案为Deep Input-to-Hidden Function，在循环层之前加入多个普通的前馈层，将输入向量进行多层映射之后再送入循环层进行处理。第二种方案是Deep Hidden -to-Hidden Transition，它使用多个循环层，这和前馈型神经网络类似，唯一不同的是计算隐含层输出的时候需要利用本隐含层在上一个时刻的输出值。第三种方案是Deep Hidden-to-Output Function，它在循环层到输出层之间加入多前馈层，这和第一种情况类似。由于循环层一般用tanh作为激活函数，层次过多之后会导致梯度消失问题，和残差网络类似，可以采用跨层连接的方案。在语音识别、自然语言处理问题上，我们会看到深层循环神经网络的应用，实验结果证明深层网络比浅层网络有更好的精度。
* 训练算法
  前面我们介绍了循环神经网络的结构，接下来要解决的问题是网络的参数如何通过训练确定。由于循环神经网络的输入是时间序列，因此每个训练样本是一个时间序列，包含多个相同维度的向量。解决循环神经网络训练问题的算法是Back Propagation Through Time算法，简称BPTT，原理和标准的反向传播算法类似，都是建立误差项的递推公式，根据误差项计算出损失函数对权重矩阵、偏置向量的梯度值。不同的是，全连接神经网络中递推是在层之间建立的，而这里是沿着时间轴建立的。