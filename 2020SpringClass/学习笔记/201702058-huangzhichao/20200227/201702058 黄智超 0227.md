## 0227的每日总结
### 线性分类
#### 线性二分类
##### 二分类函数
+ 对率函数Logistic Function，即可以做为激活函数使用，又可以当作二分类函数使用。
    + 公式:$$a(z) = \frac{1}{1 + e^{-z}}$$
    + 导数:$$a^{'}(z) = a(z)(1 - a(z))$$
    + 输入值域：$$(-\infty, \infty)$$
    + 输出值域：$$(0,1)$$ 
+ 正向传播：
    + 矩阵运算: $$ z=x \cdot w + b \tag{1} $$
    + 分类计算：$$ a = Logistic(z)={1 \over 1 + e^{-z}} \tag{2} $$
    + 损失函数计算（二分类交叉熵损失函数）：$$ loss(w,b) = -[y \ln a+(1-y)\ln(1-a)] \tag{3} $$
+  反向传播：
    + 损失函数loss对a的偏导：$$ \frac{\partial loss}{\partial a}=-[{y \over a}+{-(1-y) \over 1-a}]=\frac{a-y}{a(1-a)} \tag{4} $$
    + 损失函数a对z的偏导：$$ \frac{\partial a}{\partial z}= a(1-a) \tag{5} $$
    + 损失函数loss对z的偏导：$$ \frac{\partial loss}{\partial z}=a-y \tag{6} $$
##### 线性二分类的实现
+ 运行结果：
![](02271.jpg)
##### 线性二分类原理
###### 二分类过程：
    + 正向计算：$$ z = x_1 w_1+ x_2 w_2 + b \tag{1} $$
    + 分类计算：$$ a={1 \over 1 + e^{-z}} \tag{2} $$
    + 损失函数计算：$$ loss = -[y \ln (a)+(1-y) \ln (1-a)] \tag{3} $$
##### 二分类结果可视化
+ 运行结果：
![](02272.jpg)
##### 实现逻辑与或非门
+ 运行结果：
![](02273.jpg)![](02274.jpg)![](02275.jpg)
##### 用双曲正切函数做二分类函数
+ 修改前向计算和反向传播函数：
    + 增加双曲正切分类函数
    + 修改前向计算方法
    + 修改反向传播方法
+ 修改损失函数：
    + 交叉熵原公式：
        + $$loss=-[y_i \ln a_i + (1-y_i) \ln (1-a_i)] \tag{2}$$
    + 改成：
        + $$ {\partial loss \over \partial a_i} = {2(a_i-y_i) \over (1+a_i)(1-a_i)} \tag{8} $$
    + 增加新的损失函数
    + 修改反向传播方法
+ 再次修改损失函数代码
+ 修改样本数据标签
+ 运行结果：
![](02276.jpg)![](02277.jpg)![](02278.jpg)
#### 线性多分类
##### 多分类函数
+ 如图所示：![](02279.jpg)
+ 分类器：
    + 分类是数据挖掘的一种非常重要的方法。分类的概念是在已有数据的基础上学会一个分类函数或构造出一个分类模型（即我们通常所说的分类器(Classifier)）。该函数或模型能够把数据库中的数据纪录映射到给定类别中的某一个，从而可以应用于数据预测。 
    + 分类器是数据挖掘中对样本进行分类的方法的统称，包含决策树、逻辑回归、朴素贝叶斯、神经网络等算法。
+ 交叉熵：
    + 是Shannon信息论中一个重要概念，主要用于度量两个概率分布间的差异性信息。
    + 语言模型的性能通常用交叉熵和复杂度（perplexity）来衡量。交叉熵的意义是用该模型对文本识别的难度，或者从压缩的角度来看，每个词平均要用几个位来编码。
    + 复杂度的意义是用该模型表示这一文本平均的分支数，其倒数可视为每个词的平均概率。
    + 平滑是指对没观察到的N元组合赋予一个概率值，以保证词序列总能通过语言模型得到一个概率值。
    + 通常使用的平滑技术有图灵估计、删除插值平滑、Katz平滑和Kneser-Ney平滑。
    + 交叉熵可在神经网络(机器学习)中作为损失函数，p表示真实标记的分布，q则为训练后的模型的预测标记分布，交叉熵损失函数可以衡量p与q的相似性。
    + 交叉熵作为损失函数还有一个好处是使用sigmoid函数在梯度下降时能避免均方误差损失函数学习速率降低的问题，因为学习速率可以被输出的误差所控制。
##### 线性多分类实现
+ 运行结果：
![](022710.jpg)
##### 线性多分类的工作原理
+ 线性计算：$$z_1 = x_1 w_{11} + x_2 w_{21} + b_1 \tag{1}$$ $$z_2 = x_1 w_{12} + x_2 w_{22} + b_2 \tag{2}$$ $$z_3 = x_1 w_{13} + x_2 w_{23} + b_3 \tag{3}$$
+ 分类计算：$$ a_1=\frac{e^{z_1}}{\sum_i e^{z_i}}=\frac{e^{z_1}}{e^{z_1}+e^{z_2}+e^{z_3}} \tag{4} $$ $$ a_2=\frac{e^{z_2}}{\sum_i e^{z_i}}=\frac{e^{z_2}}{e^{z_1}+e^{z_2}+e^{z_3}} \tag{5} $$ $$ a_3=\frac{e^{z_3}}{\sum_i e^{z_i}}=\frac{e^{z_3}}{e^{z_1}+e^{z_2}+e^{z_3}} \tag{6} $$
+ 损失函数计算：
    + 单样本时（$n$表示类别数，$j$表示类别序号）:$$ loss(w,b)=-(y_1 \ln a_1 + y_2 \ln a_2 + y_3 \ln a_3) $$ $$ =-\sum_{j=1}^{n} y_j \ln a_j \tag{7} $$
    + 批量样本时（$m$表示样本数，$i$表示样本序号）：$$J(w,b) =- \sum_{i=1}^m (y_{i1} \ln a_{i1} + y_{i2} \ln a_{i2} + y_{i3} \ln a_{i3})$$ $$ =- \sum_{i=1}^m \sum_{j=1}^n y_{ij} \ln a_{ij} \tag{8}$$
##### 线性多分类结果可视化
+ 运行结果：
![](022711.jpg)![](022712.jpg)
### 心得体会
今天学习的是线性分类，线性分类有两种，一种是线性二分类，另一种是线性多分类。分别对两种工作原理有了大概的了解，然后通过代码实现，更加清楚了交叉熵和损失函数的作用。
