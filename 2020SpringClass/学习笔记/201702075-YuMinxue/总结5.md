# 总结
## 模型的推理与部署
### 神经网络模型
以一个tensorflow生成的模型作为例子。

首先，使用代码生成一个非常简单的深度学习模型，由一个卷积层，一个relu激活函数组成，

```python
import tensorflow as tf
SEED = 46
def main():
    # first generate weights and bias used in conv layers
    conv1_weights = tf.Variable(
      tf.truncated_normal([5, 5, 3, 8],  # 5x5 filter, depth 8.
                          stddev=0.1,
                          seed=SEED))
    conv1_biases = tf.Variable(tf.zeros([8]))

    # data and out is placeholder used for input and output data in practice
    data = tf.placeholder(dtype=tf.float32, name="data", shape=[8, 32, 32, 3])
    out = tf.placeholder(dtype=tf.float32, name="out", shape=[8, 32, 32, 8])

    # as the structure of the simple model
    def model():
        conv = tf.nn.conv2d(data,
                        conv1_weights,
                        strides=[1, 1, 1, 1],
                        padding='SAME', name="conv")
        out = tf.nn.relu(tf.nn.bias_add(conv, conv1_biases), name="relu")

    # saver is used for saving model
    saver = tf.train.Saver()
    with tf.Session() as sess:
        model()
        # initialize all variables
        tf.global_variables_initializer().run()
        # save the model in the file path
        saver.save(sess, './model')
```

在上面这份代码中，我们生成了一个输入大小是8\*32\*32\*3，输出尺寸是8\*32\*32\*8，卷积核大小是5\*5，输入通道数是3，输出通道数是8的只有一个卷积层的神经网络。我们甚至没有给这个神经网络进行训练，就直接进行了保存。这样一个神经网络模型文件里是一个什么样的结构呢？是像一般的程序一样，编译完了程度就被转化成了一堆逻辑指令呢？还是继续维持着这样一个模型文件的结构呢？

在运行完上面的代码后，在当前路径下应该多了四个文件，checkpoint，model.index， model.meta， model.data-00000-of-00001，如果没有的话请重新运行文件生成这样的文件。这样四个文件里面保存了什么呢？按照官方文档的解释，meta文件保存了序列化后的计算图，index文件则保存了数据文件的索引，包括变量名等等，data文件是保存了类似于变量值这样的数据的文件，checkpoint记录了最新的模型文件的名称或者序号。
### Windows模型
模型的部署

下面我们以实际生成的mnist模型为例，来看一下如何在应用中集成模型进行推理。

这里以Windows平台为例，应用程序使用C#语言。我们有两种方案：

- 使用[Windows Machine Learning](https://docs.microsoft.com/zh-cn/windows/ai/)加载模型并推理，这种方式要求系统必须是Windows 10，版本号大于17763
- 使用由微软开源的[OnnxRuntime](https://github.com/Microsoft/onnxruntime)库加载模型并推理，系统可以是Windows 7 或 Windows 10，但目前仅支持x64平台

---
### Android模型
- 可以部署在Android/IOS端的模型
- 将模型转换到Android端
-  将模型部署到Android

## 模型转换

在这里呢，遵循教程承前启后的脉络，我们选择将之前训练的模型(onnx格式的模型)转换成caffe2的模型格式。当然，这里如果不想要进行这一步操作，我们也可以直接使用caffe2进行模型的训练操作。

因为具体onnx模型的内容和大家的训练代码有关，我们提供了一个onnx模型供大家[下载](https://github.com/Microsoft/ai-edu/tree/master/B-%E6%95%99%E5%AD%A6%E6%A1%88%E4%BE%8B%E4%B8%8E%E5%AE%9E%E8%B7%B5/B6-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86%E7%AE%80%E6%98%8E%E6%95%99%E7%A8%8B/%E5%BE%AE%E8%BD%AF-%E6%96%B9%E6%A1%881/11.3/trained_model)。

如何将一个onnx模型转化成caffe2的模型呢？这一步的操作其实比较简单，只需要按照onnx-caffe2的官方例程一步一步操作即可。这里我们也给出一个示例代码

```python
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
from __future__ import unicode_literals

from caffe2.proto import caffe2_pb2
from caffe2.python import core
from onnx_caffe2.backend import Caffe2Backend
from onnx_caffe2.helper import c2_native_run_net, save_caffe2_net, load_caffe2_net, \
    benchmark_caffe2_model, benchmark_pytorch_model
import logging
import numpy as np
import onnx

log = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)

# load onnx model
onnx_model = onnx.load("model.onnx")

# Check whether the onnx_model is valid or not.
log.info("Check the ONNX model.")
onnx.checker.check_model(onnx_model)

# Convert the ONNX model to a Caffe2 model.
log.info("Convert the model to a Caffe2 model.")
init_net, predict_net = Caffe2Backend.onnx_graph_to_caffe2_net(onnx_model.graph, device="CPU")

# Save the converted Caffe2 model in the protobuf files.
log.info("Save the Caffe2 models as pb files.")
init_file = "./model_init.pb"
predict_file = "./model_predict.pb"
save_caffe2_net(init_net, init_file, output_txt=False)
save_caffe2_net(predict_net, predict_file, output_txt=True)
```

通过运行上述脚本，我们会发现在目录结构下多出了`model_predict.pb`和`model_init.pb`这样两个文件，也就是我们通过转换操作得到的caffe2的model了。

通过caffe2的官方文档，我们可以知道这样两个文件分别存储了网络的结构和对应的参数。也就是说，这两个文件保存了我们所需要使用的网络结构。
