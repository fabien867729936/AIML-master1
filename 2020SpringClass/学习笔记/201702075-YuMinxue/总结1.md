# 总结
深度学习的入门知识可以大概归纳成9个步骤，简称为9步学习法：
1.  基本概念
2.  线性回归
3.  线性分类
4.  非线性回归
5.  非线性分类
6.  模型的推理与应用部署
7.  深度神经网络
8.  卷积神经网络
9.  循环神经网络
## 神经网络基本概念
首先会讲解一下神经网络基本的训练和工作原理，然后是导数公式和反向传播公式，包括矩阵求导，再然后是反向传播和梯度下降，先从简单的线性方式说起（只有加法和乘法），而且用代入数值的方式来消除对公式的恐惧心理。然后分层的复杂（非线性）函数的反向传播，同样用数值代入方式手推反向过程。梯度下降是神经网络的基本学习方法。
## 神经网络的基本工作原理
### 神经元细胞的数学模型
输入input
权重weights
偏移bias
求和sum
激活函数
#### 小结
一个神经元可以有多个输入一个神经元只能有一个输出，这个输出可以同时输入给多个神经元**一个神经元的w的数量和输入的数量一致一个神经元只有一个bw和b有人为的初始值，在训练过程中被不断修改激活函数不是必须有的，亦即A可以等于Z一层神经网络中的所有神经元的激活函数必须一致用单变量和双变量两种方式说明，配以可视化的图解。再多的变量就无法用可视化方式来解释了，所以我们力求用简单的方式理解复杂的事物。  
### 神经网络的训练过程
单层的神经网络，有m个输入 (这里m=3)，有n个输出 (这里n=2)。在单个神经元里，b是个值。但是在神经网络中，我们把b的值永远设置为1，而用b到每个神经元的权值来表示实际的偏移值，亦即(b1,b2)，这样便于矩阵运算。也有些作者把b写成x0，其实是同一个意思，只不过x0用于等于1。

-   (x1,x2,x3)(x1,x2,x3)是一个样本数据的三个特征值
-   (w11,w12,w13)(w11,w12,w13)是(x1,x2,x3)(x1,x2,x3)到n1n1的权重
-   (w21,w22,w23)(w21,w22,w23)是(x1,x2,x3)(x1,x2,x3)到n2n2的权重
-   b1b1是n1n1的偏移
-   b2b2是n2n2的偏移
同一个特征x1，对于n1、n2来说，权重是不相同的，因为n1、n2是两个神经元，它们完成不同的任务（特征识别）。我们假设x1,x2,x3分别代表红绿蓝三种颜色，而n1,n2分别用于识别暖色和冷色，那么x1到n1的权重，肯定要大于x1到n2的权重，因为x1代表红色，是暖色。
而对于n1来说，x1，x2，x3输入的权重也是不相同的，因为它要对不同特征有选择地接纳。如同上面的例子，n1对于代表红色的x1，肯定是特别重视，权重值较高；而对于代表蓝色的x3，尽量把权重值降低，才能有正确的输出。
### 神经网络中的矩阵运算
一个两层的神经网络，包含隐藏层和输出层，输入层不算做一层。
其中，w1(m,n)，1表示第1层，表示第一层神经网络的权重矩阵，w2(m,n)表示第二层神经网络的权重矩阵。Visio中不容易写上下标，所以形式有所变动。
$$Z^1_1 = w^1_{1,1}x_1+w^1_{1,2}x_2+b^1_1 \tag{上标1表示第1层}$$
$$Z^1_2 = w^1_{2,1}x_1+w^1_{2,2}x_2+b^1_2$$
$$Z^1_{3} = w^1_{3,1}x_1+w^1_{3,2}x_2+b^1_{3}$$
变成矩阵运算：
$$Z^1_1=\begin{pmatrix}w^1_{1,1}&w^1_{1,2}\end{pmatrix}
\begin{pmatrix}x_1 \\ x_2\end{pmatrix}+b^1_1$$
$$Z^1_2=\begin{pmatrix}w^1_{2,1}&w^1_{2,2}\end{pmatrix}
\begin{pmatrix}x_1 \\ x_2\end{pmatrix}+b^1_2$$
$$Z^1_{3}=\begin{pmatrix}w^1_{3,1}&w^1_{3,2}\end{pmatrix}
\begin{pmatrix}x_1 \\ x_2\end{pmatrix}+b^1_3$$
再变成大矩阵：
$$Z1 =
\begin{pmatrix}
w^1_{1,1}&w^1_{1,2} \\
w^1_{2,1}&w^1_{2,2}\\
w^1_{3,1}&w^1_{3,2}
\end{pmatrix}
\begin{pmatrix}x_1 \\
x_2 \end{pmatrix}
+\begin{pmatrix}b^1_1 \\
b^1_2 \\
b^1_3
\end{pmatrix}$$
最后变成矩阵符号：
$$Z1 = W1 \cdot X + B1$$
然后是激活函数运算：
$$A1=a{(Z1)} (有激活函数)|or|A = Z(无激活函数)$$
同理可得：
$$Z2 = W2 \cdot A1 + B2$$
$$A2=a{(Z2)}$$
### 神经网络的主要功能
- **回归/拟合 Regression/fitting**
- **分类 Classification**
单层的神经网络能够模拟一条二维平面上的直线，从而可以完成线性分割任务。而理论证明，两层神经网络可以无限逼近任意连续函数。
使用一个两层的神经网络可以得到一个非常近似的结果，使得分类误差在满意的范围之内。而这个真实的连续函数的原型是：
$$y=0.4x^2 + 0.3xsin(15x) + 0.01cos(50x)-0.3$$
从输入层到隐藏层的矩阵计算，就是对输入数据进行了空间变换，使其可以被线性可分，然后输出层画出了一个分界线。而训练的过程，就是确定那个空间变换矩阵的过程。因此，多层神经网络的本质就是对复杂函数的拟合。
神经网络的训练结果，是一大堆的权重组成的数组（近似解），并不能得到上面那种精确的数学表达式（数学解析解）。
##  反向传播与梯度下降
反向传播与梯度下降的基本工作原理：
1.  初始化
2.  正向计算
3.  损失函数为我们提供了计算损失的方法
4.  梯度下降是在损失函数基础上向着损失最小的点靠近而指引了网络权重调整的方向
5.  反向传播把损失值反向传给神经网络的每一层，让每一层都根据损失值反向调整权重
6.  goto 2，直到精度足够好（比如损失函数值小于0.001）
**反向传播可以分为线性反向传播和非线性反向传**
###  梯度下降
梯度下降的三要素
1.  当前点
2.  方向
3.  步长
“梯度下降”包含了两层含义：
梯度：函数当前位置的最快上升点
下降：与导数相反的方向，用数学语言描述就是那个减号亦即与上升相反的方向运动就是下降。
