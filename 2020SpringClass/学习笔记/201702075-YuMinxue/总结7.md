# 总结
## 卷积的反向传播原理
###  计算反向传播的梯度矩阵

正向公式：

$$Z = W*A+b \tag{0}$$

其中，W是卷积核，*表示卷积（互相关）计算，A为当前层的输入项，b是偏移（未在图中画出），Z为当前层的输出项，但尚未经过激活函数处理。

我们举一个具体的例子便于分析。下面是正向计算过程：
分解到每一项就是下列公式：

$$z_{11} = w_{11} \cdot a_{11} + w_{12} \cdot a_{12} + w_{21} \cdot a_{21} + w_{22} \cdot a_{22} + b \tag{1}$$
$$z_{12} = w_{11} \cdot a_{12} + w_{12} \cdot a_{13} + w_{21} \cdot a_{22} + w_{22} \cdot a_{23} + b \tag{2}$$
$$z_{21} = w_{11} \cdot a_{21} + w_{12} \cdot a_{22} + w_{21} \cdot a_{31} + w_{22} \cdot a_{32} + b \tag{3}$$
$$z_{22} = w_{11} \cdot a_{22} + w_{12} \cdot a_{23} + w_{21} \cdot a_{32} + w_{22} \cdot a_{33} + b \tag{4}$$

求损失函数J对a11的梯度：

$$
\frac{\partial J}{\partial a_{11}}=\frac{\partial J}{\partial z_{11}} \frac{\partial z_{11}}{\partial a_{11}}=\delta_{z11}\cdot w_{11} \tag{5}
$$

上式中，$\delta_{z11}$是从网络后端回传到本层的z11单元的梯度。

求J对a12的梯度时，先看正向公式，发现a12对z11和z12都有贡献，因此需要二者的偏导数相加：

$$
\frac{\partial J}{\partial a_{12}}=\frac{\partial J}{\partial z_{11}} \frac{\partial z_{11}}{\partial a_{12}}+\frac{\partial J}{\partial z_{12}} \frac{\partial z_{12}}{\partial a_{12}}=\delta_{z11} \cdot w_{12}+\delta_{z12} \cdot w_{11} \tag{6}
$$

最复杂的是求a22的梯度，因为从正向公式看，所有的输出都有a22的贡献，所以：

$$
\frac{\partial J}{\partial a_{22}}=\frac{\partial J}{\partial z_{11}} \frac{\partial z_{11}}{\partial a_{22}}+\frac{\partial J}{\partial z_{12}} \frac{\partial z_{12}}{\partial a_{22}}+\frac{\partial J}{\partial z_{21}} \frac{\partial z_{21}}{\partial a_{22}}+\frac{\partial J}{\partial z_{22}} \frac{\partial z_{22}}{\partial a_{22}} 
$$
$$
=\delta_{z11} \cdot w_{22} + \delta_{z12} \cdot w_{21} + \delta_{z21} \cdot w_{12} + \delta_{z22} \cdot w_{11} \tag{7}
$$

同理可得所有a的梯度。

观察公式7中的w的顺序，貌似是把原始的卷积核旋转了180度，再与传入误差项做卷积操作，即可得到所有元素的误差项。而公式5和公式6并不完备，是因为二者处于角落，这和卷积正向计算中的padding是相同的现象。因此，我们把传入的误差矩阵Delta-In做一个zero padding，再乘以旋转180度的卷积核，就是要传出的误差矩阵Delta-Out：

最后可以统一成为一个简洁的公式：

$$\delta_{out} = \delta_{in} * W^{rot180} \tag{8}$$

这个误差矩阵可以继续回传到下一层。

- 当Weights是3x3时，$\delta_{in}$需要padding=2，即加2圈0，才能和Weights卷积后，得到正确尺寸的$\delta_{out}$
- 当Weights是5x5时，$\delta_{in}$需要padding=4，即加4圈0，才能和Weights卷积后，得到正确尺寸的$\delta_{out}$
- 以此类推：当Weights是NxN时，$\delta_{in}$需要padding=N-1，即加N-1圈0

举例：

正向时stride=1：$A^{(10 \times 8)}*W^{(5 \times 5)}=Z^{(6 \times 4)}$

反向时，$\delta_z^{(6 \times 4)} + 4 padding = \delta_z^{(14 \times 12)}$

然后：$\delta_z^{(14 \times 12)} * W^{rot180(5 \times 5)}= \delta_a^{(10 \times 8)}$
##  卷积的反向传播代码实现

### 方法1

完全按照17.3中的讲解来实现反向传播，但是由于有17.2中关于numba帮助，我们在实现代码时，可以考虑把一些模块化的计算放到独立的函数中，用numba在运行时编译加速。

```Python
    def backward_numba(self, delta_in, flag):
        # 如果正向计算中的stride不是1，转换成是1的等价误差数组
        dz_stride_1 = expand_delta_map(delta_in, ...)
        # 计算本层的权重矩阵的梯度
        self._calculate_weightsbias_grad(dz_stride_1)
        # 由于输出误差矩阵的尺寸必须与本层的输入数据的尺寸一致，所以必须根据卷积核的尺寸，调整本层的输入误差矩阵的尺寸
        (pad_h, pad_w) = calculate_padding_size(...)
        dz_padded = np.pad(dz_stride_1, ...)
        # 计算本层输出到下一层的误差矩阵
        delta_out = self._calculate_delta_out(dz_padded, flag)
        #return delta_out
        return delta_out, self.WB.dW, self.WB.dB

    # 用输入数据乘以回传入的误差矩阵,得到卷积核的梯度矩阵
    def _calculate_weightsbias_grad(self, dz):
        self.WB.ClearGrads()
        # 先把输入矩阵扩大，周边加0
        (pad_h, pad_w) = calculate_padding_size(...)
        input_padded = np.pad(self.x, ...)
        # 输入矩阵与误差矩阵卷积得到权重梯度矩阵
        (self.WB.dW, self.WB.dB) = calcalate_weights_grad(...)
        self.WB.MeanGrads(self.batch_size)

    # 用输入误差矩阵乘以（旋转180度后的）卷积核
    def _calculate_delta_out(self, dz, layer_idx):
        if layer_idx == 0:
            return None
        # 旋转卷积核180度
        rot_weights = self.WB.Rotate180()
        # 定义输出矩阵形状
        delta_out = np.zeros(self.x.shape)
        # 输入梯度矩阵卷积旋转后的卷积核，得到输出梯度矩阵
        delta_out = calculate_delta_out(dz, ..., delta_out)

        return delta_out
```
为了节省篇幅，上面的代码中做了一些省略，只保留了基本的实现思路，并给出了详尽的注释，相信读者在充分理解17.3的内容的基础上，可以看懂。

其中，两个计算量大的函数，一个是计算权重矩阵的基础函数calcalate_weights_grad，另一个是计算输出误差矩阵的基础函数calculate_delta_out，都使用了numba的方式实现，以加快反向传播代码的运行速度。

### 方法2

在前向计算中，我们试验了img2col的方法，取得了不错的效果。在反向传播中，也有对应的逆向方法，叫做col2img。下面我们基于它来实现另外一种反向传播算法，其基本思想是：把反向传播也看作是全连接层的方式，直接用矩阵运算代替卷积操作，然后把结果矩阵再转换成卷积操作的反向传播所需要的形状。

#### 代码实现

```Python
    def backward_col2img(self, delta_in, layer_idx):
        OutC, InC, FH, FW = self.WB.W.shape
        # 误差矩阵变换
        delta_in_2d = np.transpose(delta_in, axes=(0,2,3,1)).reshape(-1, OutC)
        # 计算Bias的梯度
        self.WB.dB = np.sum(delta_in_2d, axis=0, keepdims=True).T / self.batch_size
        # 计算Weights的梯度
        dW = np.dot(self.col_x.T, delta_in_2d) / self.batch_size
        # 转换成卷积核的原始形状
        self.WB.dW = np.transpose(dW, axes=(1, 0)).reshape(OutC, InC, FH, FW)# 计算反向传播误差矩阵
        dcol = np.dot(delta_in_2d, self.col_w.T)
        # 转换成与输入数据x相同的形状
        delta_out = col2img(dcol, self.x.shape, FH, FW, self.stride, self.padding)
        return delta_out, self.WB.dW, self.WB.dB
```

#### 单样本单通道的实例讲解

假设有1个样本1个通道且图片为3x3的矩阵：
```
x=
 [[[[0 1 2]
    [3 4 5]
    [6 7 8]]]]

col_x=
 [[0. 1. 3. 4.]
  [1. 2. 4. 5.]
  [3. 4. 6. 7.]
  [4. 5. 7. 8.]]
```
卷积核也只有1个形状为1x1x2x2的矩阵：
```
w=
 [[[[0 1]
    [2 3]]]]
```
卷积核展开后：
```
col_w=
 [[0]
  [1]
  [2]
  [3]]
```
卷积的结果会是一个样本在一个通道上的2x2的输出。

再假设从后端反向传播回来的输入误差矩阵：
```
delta_in=
 [[[[0 1]
    [2 3]]]]
```
误差矩阵经过下式变换：
```Python
delta_in_2d = np.transpose(delta_in, axes=(0,2,3,1)).reshape(-1, OutC)
```
得到：
```
delta_in_2d=
 [[0]
 [1]
 [2]
 [3]]
```
计算dB（这一步和全连接层完全相同）：
```Python
self.WB.dB = np.sum(delta_in_2d, axis=0, keepdims=True).T / self.batch_size
```
得到：
```
dB=
 [[6.]]
```
计算dW（这一步和全连接层完全相同）：
```Python
dW = np.dot(self.col_x.T, delta_in_2d) / self.batch_size
```
得到：
```
dW=
 [[19.]
  [25.]
  [37.]
  [43.]]
```
还原dW到1x1x2x2的卷积核形状：
```Python
self.WB.dW = np.transpose(dW, axes=(1, 0)).reshape(OutC, InC, FH, FW) 
```
得到：
```
dW=
 [[[[19. 25.]
    [37. 43.]]]]
```
至此，dB和dW都已经得到，本层的梯度计算完毕，需要把梯度回传给前一层，所以要计算输出误差矩阵（这一步和全连接层完全相同）：
```Python
dcol = np.dot(delta_in_2d, self.col_w.T)
```
得到：
```
dcol=
 [[0 0 0 0]
 [0 1 2 3]
 [0 2 4 6]
 [0 3 6 9]]
```
转换成正确的矩阵形状：
```Python
delta_out = col2img(dcol, self.x.shape, FH, FW, self.stride, self.padding)
```
得到：
```
delta_out=
 [[[[ 0.  0.  1.]
    [ 0.  4.  6.]
    [ 4. 12.  9.]]]]
```

得到上述4x4矩阵后，我们要把它逆变换到一个3x3的矩阵中，步骤如下：
1. 左侧第一行红色椭圆内的四个元素移到右侧红色圆形内；
2. 在1的基础上，左侧第二行黄色椭圆内的四个元素移到右侧黄色圆形内，其中与原有元素重叠的地方则两个值相加。比如中间那个元素就是0+2=2；
3. 在2的基础上，左侧第三行蓝色椭圆内的四个元素移到右侧蓝色圆形内，其中与原有元素重叠的地方则两个值相加。比如中间那个元素再次加2；
4. 在3的基础上，左侧第四行绿色椭圆内的四个元素移到右侧绿色圆形内，其中与原有元素重叠的地方则两个值相加，中间的元素再次加0，还是4；中间靠下的元素原值是6，加6后为12。

这个结果和最后一步delta_out的结果完全一致。
#### 误差输入矩阵

delta_in是本层的误差输入矩阵，它的形状应该和本层的前向计算结果一样。在本例中，误差输入矩阵的形状应该是：(batch_size * output_channel * output_height * output_width) = (2 x 2 x 2 x 2)：
```
delta_in=
(样本1)
    (通道1)
 [[[[ 0  1]
   [ 2  3]]
    (通道2)
  [[ 4  5]
   [ 6  7]]]
(样本2)
    (通道1)
 [[[ 8  9]
   [10 11]]
    (通道2)
  [[12 13]
   [14 15]]]]
```
为了做img2col的逆运算col2img，我们把它转换成17.2中的结果数据的形状8x2：
```Python
delta_in_2d = np.transpose(delta_in, axes=(0,2,3,1)).reshape(-1, OutC)
```
```
delta_in_2d=
 [[ 0  4]
  [ 1  5]
  [ 2  6]
  [ 3  7]
  [ 8 12]
  [ 9 13]
  [10 14]
  [11 15]]
```
计算权重矩阵的梯度：
```Python
dW = np.dot(self.col_x.T, delta_in_2d) / self.batch_size
```
```
dW=
 [[ 564.  812.]
 [ 586.  850.]
 [ 630.  926.]
 [ 652.  964.]
 [ 762. 1154.]
 [ 784. 1192.]
 [ 828. 1268.]
 [ 850. 1306.]
 [ 960. 1496.]
 [ 982. 1534.]
 [1026. 1610.]
 [1048. 1648.]]
```
但是这个12x2的结果是对应的权重矩阵的二维数组展开形式的，所以要还原成原始的卷积核形式2x3x2x2：
```Python
self.WB.dW = np.transpose(dW, axes=(1, 0)).reshape(OutC, InC, FH, FW)
```
```
dW=
(过滤器1)
    (卷积核1)
 [[[[ 564.  586.]
   [ 630.  652.]]
    (卷积核2)
  [[ 762.  784.]
   [ 828.  850.]]
    (卷积核3)
  [[ 960.  982.]
   [1026. 1048.]]]

(过滤器2)
    (卷积核1)
 [[[ 812.  850.]
   [ 926.  964.]]
    (卷积核2)
  [[1154. 1192.]
   [1268. 1306.]]
    (卷积核3)
  [[1496. 1534.]
   [1610. 1648.]]]]
```

计算误差输出矩阵：

```Python
dcol = np.dot(delta_in_2d, self.col_w.T)
```
得到：
```
dcol=
 [[ 48  52  56  60  64  68  72  76  80  84  88  92]
 [ 60  66  72  78  84  90  96 102 108 114 120 126]
 [ 72  80  88  96 104 112 120 128 136 144 152 160]
 [ 84  94 104 114 124 134 144 154 164 174 184 194]
 [144 164 184 204 224 244 264 284 304 324 344 364]
 [156 178 200 222 244 266 288 310 332 354 376 398]
 [168 192 216 240 264 288 312 336 360 384 408 432]
 [180 206 232 258 284 310 336 362 388 414 440 466]]
```
但是dcol对应的是输入数据的二维展开形式4x12，应该把它还原成2x3x3x3的形式：
```Python
delta_out = col2img(dcol, self.x.shape, FH, FW, self.stride, self.padding)
```
得到：
```
delta_out=
(样本1)
    (通道1)
 [[[[  48.  112.   66.]
   [ 128.  296.  172.]
   [  88.  200.  114.]]
    (通道2)
  [[  64.  152.   90.]
   [ 176.  408.  236.]
   [ 120.  272.  154.]]
    (通道3)
  [[  80.  192.  114.]
   [ 224.  520.  300.]
   [ 152.  344.  194.]]]

(样本2)
    (通道1)
 [[[ 144.  320.  178.]
   [ 352.  776.  428.]
   [ 216.  472.  258.]]
    (通道2)
  [[ 224.  488.  266.]
   [ 528. 1144.  620.]
   [ 312.  672.  362.]]
    (通道3)
  [[ 304.  656.  354.]
   [ 704. 1512.  812.]
   [ 408.  872.  466.]]]]
```

###  正确性与性能测试

在正向计算中，numba稍胜一筹，下面我们来测试一下二者的反向计算性能，然后比较梯度输出矩阵的结果来验证正确性。

```Python
def test_performance():
    ...
    # dry run
    f1 = c1.forward_numba(x)
    b1, dw1, db1 = c1.backward_numba(delta_in, 1)
    # run
    s1 = time.time()
    for i in range(100):
        f1 = c1.forward_numba(x)
        b1, dw1, db1 = c1.backward_numba(delta_in, 1)
    e1 = time.time()
    print("method numba:", e1-s1)

    # run
    s2 = time.time()
    for i in range(100):
        f2 = c1.forward_img2col(x)
        b2, dw2, db2 = c1.backward_col2img(delta_in, 1)
    e2 = time.time()
    print("method img2col:", e2-s2)

    print("compare correctness of method 1 and method 2:")
    print("forward:", np.allclose(f1, f2, atol=1e-7))
    print("backward:", np.allclose(b1, b2, atol=1e-7))
    print("dW:", np.allclose(dw1, dw2, atol=1e-7))
    print("dB:", np.allclose(db1, db2, atol=1e-7))
```

先用numba方法测试1000次的正向+反向，然后再测试1000次img2col的正向+反向，同时我们会比较反向传播的三个输出值：误差矩阵b、权重矩阵梯度dw、偏移矩阵梯度db。

输出结果：

```
method numba: 11.830008506774902
method img2col: 3.543151378631592
compare correctness of method 1 and method 2:
forward: True
backward: True
dW: True
dB: True
```
## 池化的前向计算和反向传播

### 常用池化方法

池化 pooling，又称为下采样，downstream sampling or sub-sampling。

池化方法分为两种，一种是最大值池化 Max Pooling，一种是平均值池化 Mean/Average Pooling。
- 最大值池化，是取当前池化视野中所有元素的最大值，输出到下一层特征图中。
- 平均值池化，是取当前池化视野中所有元素的平均值，输出到下一层特征图中。

其目的是：

- 扩大视野：就如同先从近处看一张图片，然后离远一些再看同一张图片，有些细节就会被忽略
- 降维：在保留图片局部特征的前提下，使得图片更小，更易于计算
- 平移不变性，轻微扰动不会影响输出：比如上如中最大值池化的4，即使向右偏一个像素，其输出值仍为4
- 维持同尺寸图片，便于后端处理：假设输入的图片不是一样大小的，就需要用池化来转换成同尺寸图片

一般我们都使用最大值池化。

### 池化的其它方式

在上面的例子中，我们使用了size=2x2，stride=2的模式，这是常用的模式，即步长与池化尺寸相同。

stride=1, size=2x2的情况，可以看到，右侧的结果中，有一大堆的3和4，基本分不开了，所以其池化效果并不好。

假设输入图片的形状是 $W_1 \times H_1 \times D_1$，其中W是图片宽度，H是图片高度，D是图片深度（多个图层），F是池化的视野（正方形），S是池化的步长，则输出图片的形状是：

- $W_2 = (W_1 - F)/S + 1$
- $H_2 = (H_1 - F)/S + 1$
- $D_2 = D_1$

池化层不会改变图片的深度，即D值前后相同。

### 池化层的训练

我们假设2x2的图片中，$[[1,2],[3,4]]$是上一层网络回传的残差，那么：

- 对于最大值池化，残差值会回传到当初最大值的位置上（请对照图14.3.1），而其它三个位置的残差都是0。
- 对于平均值池化，残差值会平均到原始的4个位置上。
#### Max Pooling

正向公式：

$$w = max(a,b,e,f)$$

反向公式（假设Input Layer中的最大值是b）：

$${\partial w \over \partial a} = 0$$
$${\partial w \over \partial b} = 1$$
$${\partial w \over \partial e} = 0$$
$${\partial w \over \partial f} = 0$$

因为a,e,f对w都没有贡献，所以偏导数为0，只有b有贡献，偏导数为1。

$$\delta_a = {\partial J \over \partial a} = {\partial J \over \partial w} {\partial w \over \partial a} = 0$$

$$\delta_b = {\partial J \over \partial b} = {\partial J \over \partial w} {\partial w \over \partial b} = \delta_w \cdot 1 = \delta_w$$

$$\delta_e = {\partial J \over \partial e} = {\partial J \over \partial w} {\partial w \over \partial e} = 0$$

$$\delta_f = {\partial J \over \partial f} = {\partial J \over \partial w} {\partial w \over \partial f} = 0$$

#### Mean Pooling

正向公式：

$$w = \frac{1}{4}(a+b+e+f)$$

反向公式（假设Layer-1中的最大值是b）：

$${\partial w \over \partial a} = \frac{1}{4}$$
$${\partial w \over \partial b} = \frac{1}{4}$$
$${\partial w \over \partial e} = \frac{1}{4}$$
$${\partial w \over \partial f} = \frac{1}{4}$$

因为a,b,e,f对w都有贡献，所以偏导数都为1：

$$\delta_a = {\partial J \over \partial a} = {\partial J \over \partial w} {\partial w \over \partial a} = \frac{1}{4}\delta_w$$

$$\delta_b = {\partial J \over \partial b} = {\partial J \over \partial w} {\partial w \over \partial b} = \frac{1}{4}\delta_w$$

$$\delta_e = {\partial J \over \partial e} = {\partial J \over \partial w} {\partial w \over \partial e} = \frac{1}{4}\delta_w$$

$$\delta_f = {\partial J \over \partial f} = {\partial J \over \partial w} {\partial w \over \partial f} = \frac{1}{4}\delta_w$$

无论是max pooling还是mean pooling，都没有要学习的参数，所以，在卷积网络的训练中，池化层需要做的只是把误差项向后传递，不需要计算任何梯度。

###  实现方法1

按照标准公式来实现池化的正向和反向代码。

```Python
class PoolingLayer(CLayer):
    def forward_numba(self, x, train=True):
        self.x = x
        self.batch_size = self.x.shape[0]
        self.z = jit_maxpool_forward(...)
        return self.z

    def backward_numba(self, delta_in, layer_idx):
        delta_out = jit_maxpool_backward(...)
        return delta_out
```

有了前面的经验，这次我们直接把前向和反向函数用numba方式来实现，并在前面加上@nb.jit修饰符：
```Python
@nb.jit(nopython=True)
def jit_maxpool_forward(...):
    ...
    return z

@nb.jit(nopython=True)
def jit_maxpool_backward(...):
    ...
    return delta_out
```

### 实现方法2

池化也有类似与卷积优化的方法来计算，比如有以下数据：

假设大写字母为池子中的最大元素，并且用max_pool方式。

原始数据先做img2col变换，然后做一次np.max(axis=1)的max计算，会大大增加速度，然后把结果reshape成正确的矩阵即可。做一次大矩阵的max计算，比做4次小矩阵计算要快很多。

```Python
class PoolingLayer(CLayer):
    def forward_img2col(self, x, train=True):
        self.x = x
        N, C, H, W = x.shape
        col = img2col(x, self.pool_height, self.pool_width, self.stride, 0)
        col_x = col.reshape(-1, self.pool_height * self.pool_width)
        self.arg_max = np.argmax(col_x, axis=1)
        out1 = np.max(col_x, axis=1)
        out2 = out1.reshape(N, self.output_height, self.output_width, C)
        self.z = np.transpose(out2, axes=(0,3,1,2))
        return self.z

    def backward_col2img(self, delta_in, layer_idx):
        dout = np.transpose(delta_in, (0,2,3,1))
        dmax = np.zeros((dout.size, self.pool_size))
        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()
        dmax = dmax.reshape(dout.shape + (self.pool_size,))
        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)
        dx = col2img(dcol, self.x.shape, self.pool_height, self.pool_width, self.stride, 0)
        return dx
```

### 性能测试

比较一下以上两种实现方式的性能，来最终决定使用哪一种。

```Python
if __name__ == '__main__':
    batch_size = 64
    input_channel = 3
    iw = 28
    ih = 28
    x = np.random.randn(batch_size, input_channel, iw, ih)
    p = PoolingLayer((input_channel,iw,ih),(2,2),2, "MAX")
    # dry run
    f1 = p.forward_numba(x, True)
    delta_in = np.random.random(f1.shape)
    # run
    s1 = time.time()
    for i in range(5000):
        f1 = p.forward_numba(x, True)
        b1 = p.backward_numba(delta_in, 0)
    e1 = time.time()
    print(e1-s1)    

    # run
    s2 = time.time()
    for i in range(5000):
        f2 = p.forward_img2col(x, True)
        b2 = p.backward_col2img(delta_in, 1)
    e2 = time.time()
    print(e2-s2)

    print(np.allclose(f1, f2, atol=1e-7))
    print(np.allclose(b1, b2, atol=1e-7))
```

对同样的一批64个样本，分别用两种方法做5000次的前向和反向计算，得到的结果：
```
Elapsed of numba: 17.537396907806396
Elapsed of img2col: 22.51519775390625
forward: True
backward: True
```
numba方法用了17秒，img2col方法用了22秒。并且两种方法的返回矩阵值是一样的。
