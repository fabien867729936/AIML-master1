# 第一章 神经网络的基本工作原理
## 人工智能概念
人工智能是一门基于计算机科学，生物学，心理学，神经科学，数学和哲学等学科的科学和技术。人工智能的一个主要推动力要开发与人类智能相关的计算机功能，例如推理，学习和解决问题的能力。
人工智能之父 John McCarthy说：人工智能就是制造智能的机器，更特指制作人工智能的程序。人工智能模仿人类的思考方式使计算机能智能的思考问题，人工智能通过研究人类大脑的思考、学习和工作方式，然后将研究结果作为开发智能软件和系统的基础。
## 人工智能历史

1940-1950：

一帮来自数学，心理学，工程学，经济学和政治学领域的科学家在一起讨论人工智能的可能性，当时已经研究出了人脑的工作原理是神经元电脉冲工作。

1950-1956：

伦·图灵（Alan Turing）发表了一篇具有里程碑意义的论文，其中他预见了创造思考机器的可能性。

重要事件： 曼彻斯特大学的Christopher Strachey使用Ferranti Mark 1 机器写了一个跳棋程序， Dietrich Prinz写了一个国际象棋程序。

1956：

达特茅斯会议，人工智能诞生。约翰麦卡锡创造了人工智能一词并且演示了卡内基梅隆大学首个人工智能程序。

1956-1974：

推理研究，主要使用推理算法，应用在棋类等游戏中。自然语言研究，目的是让计算机能够理解人的语言。日本，早稻田大学于1967年启动了WABOT项目，并于1972年完成了世界上第一个全尺寸智能人形机器人 WABOT-1 。

1974-1980：

由于当时的计算机技术限制，很多研究迟迟不能得到预期的成就，这时候AI处于研究低潮。

1980-1987：

在20世纪80年代，世界各地的企业采用了一种称为“ 专家系统 ” 的人工智能程序，知识表达系统成为主流人工智能研究的焦点。在同一年，日本政府通过其第五代计算机项目积极资助人工智能。1982年，物理学家John Hopfield发明了一种神经网络可以以全新的方式学习和处理信息。

1987-1993：

第二次AI研究低潮。

1993-2011 ：

出现了智能代理，它是感知周围环境，并采取最大限度提高成功的机会的系统。这个时期自然语言理解和翻译，数据挖掘，Web爬虫出现了较大的发展。

里程碑的事件：1997年深蓝击败了当时的世界象棋冠军Garry Kasparov。2005年，斯坦福大学的机器人在一条没有走过的沙漠小路上自动驾驶131英里。

2011年至今：

在深度学习，大数据和强人工智能的发展迅速。
## 神经网络的基本工作原理
## 1.0 神经网络的基本工作原理简介

### 1.0.1 神经元细胞的数学模型
神经网络由基本的神经元组成。
#### 输入 input

#### 权重 weights

#### 偏移 bias

#### 求和计算 sum

#### 激活函数 activation


#### 小结

- 一个神经元可以有多个输入
- 一个神经元只能有一个输出，这个输出可以同时输入给多个神经元**
- 一个神经元的w的数量和输入的数量一致
- 一个神经元只有一个b
- w和b有人为的初始值，在训练过程中被不断修改
- 激活函数不是必须有的，亦即A可以等于Z
- 一层神经网络中的所有神经元的激活函数必须一致

### 1.0.2 神经网络的训练过程
#### 单层神经网络模型
#### 训练流程
#### 前提条件
#### 步骤

### 1.0.3 神经网络中的矩阵运算

### 1.0.4 神经网络的主要功能

- **回归/拟合 Regression/fitting**
- **分类 Classification**
单层的神经网络能够模拟一条二维平面上的直线，从而可以完成线性分割任务。而理论证明，两层神经网络可以无限逼近任意连续函数。

神经网络的训练结果，是一大堆的权重组成的数组（近似解），并不能得到上面那种精确的数学表达式（数学解析解）。

### 1.0.5 为什么需要激活函数
#### 生理学上的例子
#### 激活函数的作用
如果我们不运用激活函数的话，则输出信号将仅仅是一个简单的线性函数。线性函数一个一级多项式。现如今，线性方程是很容易解决的，但是它们的复杂性有限，并且从数据中学习复杂函数映射的能力更小。一个没有激活函数的神经网络将只不过是一个线性回归模型（Linear regression Model）罢了，它功率有限，并且大多数情况下执行得并不好。

没有激活函数，我们的神经网络将无法学习和模拟其他复杂类型的数据，例如图像、视频、音频、语音等。这就是为什么我们要使用人工神经网络技术，诸如深度学习（Deep learning），来理解一些复杂的事情，一些相互之间具有很多隐藏层的非线性问题，而这也可以帮助我们了解复杂的数据。


### 1.0.6 为什么需要深度神经网络与深度学习

通常我们把三层以上的网络称为深度神经网络。两层的神经网络虽然强大，但可能只能完成二维空间上的一些拟合与分类的事情。如果对于图片、语音、文字序列这些复杂的事情，就需要更复杂的网络来理解和处理。第一个方式是增加每一层中神经元的数量，但这是线性的，不够有效。另外一个方式是增加层的数量，每一层都处理不同的事情。

1. 卷积神经网络 CNN (Convolutional Neural Networks)

对于图像类的机器学习问题，最有效的就是卷积神经网络。

2. 循环神经网络 RNN (Recurrent Neural Networks)

对于语言类的机器学习问题，最有效的就是循环神经网络。


### 1.0.7 Deep Learning的训练过程简介

1. 使用自下上升非监督学习（就是从底层开始，一层一层的往顶层训练）

2. 自顶向下的监督学习（就是通过带标签的数据去训练，误差自顶向下传输，对网络进行微调）

   