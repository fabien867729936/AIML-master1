# 第7章 多入单出的单层神经网路

## 7.0 线性多分类问题

### 7.0.1 提出问题
### 7.0.2 多分类学习策略

#### 线性多分类和非线性多分类的区别
它们的区别在于不同类别的样本点之间是否可以用一条直线来互相分割。对神经网络来说，线性多分类可以使用单层结构来解决，而分线性多分类需要使用双层结构。

#### 二分类与多分类的关系
多分类问题一共有三种解法：
1. 一对一
2. 一对多
3. 多对多
#### 多分类与多标签
多分类学习中，虽然有多个类别，但是每个样本只属于一个类别。

## 7.1 多分类函数
此函数对线性多分类和非线性多分类都适用。

### 7.1.1 多分类函数定义 - Softmax
#### 为什么叫做Softmax？

假设输入值是：[3,1,-3]，如果取max操作会变成：[1,0,0]，这符合我们的分类需要。但是有两个不足：
1. 分类结果是[1，0，0]，只保留的非0即1的信息，没有各元素之间相差多少的信息，可以理解是“Hard-Max”
2. max操作本身不可导，无法用在反向传播中。

所以Softmax加了个"soft"来模拟max的行为，但同时又保留了相对大小的信息。

#### Softmax的工作原理
### 7.1.2 正向传播

#### 矩阵运算

$$
z=x \cdot w + b \tag{1}
$$

#### 分类计算

$$
a_j = \frac{e^{z_j}}{\sum\limits_{i=1}^m e^{z_i}}=\frac{e^{z_j}}{e^{z_1}+e^{z_2}+\dots+e^{z_m}} \tag{2}
$$

#### 损失函数计算

计算单样本时，m是分类数：
$$
loss(w,b)=-\sum_{i=1}^m y_i \ln a_i \tag{3}
$$

计算多样本时，m是分类树，n是样本数：
$$J(w,b) =- \sum_{j=1}^n \sum_{i=1}^m y_{ij} \log a_{ij} \tag{4}$$

### 7.1.3 反向传播
#### 实例化推导
#### 一般性推导
## Softmax函数的Python实现

## 7.2 线性多分类的神经网络实现  
### 7.2.1 定义神经网络结构
如果有三个以上的分类同时存在，我们需要对每一类别分配一个神经元，这个神经元的作用是根据前端输入的各种数据，先做线性处理（Y=WX+B)，然后做一次非线性处理，计算每个样本在每个类别中的预测概率，再和标签中的类别比较，看看预测是否准确，如果准确，则奖励这个预测，给与正反馈；如果不准确，则惩罚这个预测，给与负反馈。两类反馈都反向传播到神经网络系统中去调整参数。

这个网络只有输入层和输出层，由于输入层不算在内，所以是一层网络。

#### 输入层
#### 权重矩阵w/b
#### 输出层

### 7.2.2 样本数据
#### 样本标签数据

### 7.2.3 代码实现
#### 添加分类函数
#### 前向计算
#### 反向传播
#### 计算损失函数值
#### 推理函数
#### 主程序

### 7.2.4 运行结果
#### 损失函数历史记录


## 7.3 线性多分类原理
此原理对线性多分类和非线性多分类都适用。

### 7.3.1 多分类过程
1. 线性计算
2. 分类计算
3. 损失函数计算
* a.单样本时
* b.批量样本时
### 7.3.2 数值计算举例
#### 如果标签值表明是此样本为第一类
#### 如果标签值表明是此样本为第二类

### 7.3.3 多分类的几何原理
#### 当样本属于第一类时
#### 当样本属于第二类时
#### 当样本属于第三类时

  
## 7.4 多分类结果可视化
### 7.4.1 显示原始数据图
### 7.4.2 显示分类结果分割线图
### 7.4.3 理解神经网络的分类方式

