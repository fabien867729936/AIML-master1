# 第四章 单入单出的单层神经网络

## 4.0 单变量线性回归问题
### 4.0.1 提出问题

线性回归的解题思路：利用已有值，预测未知值。也就是说，这些读者不经意间使用了线性回归模型。而实际上，这个例子非常简单，只有一个自变量和一个因变量，因此可以用简单直接的方法来解决问题。但是，当有多个自变量时，这种直接的办法可能就会失效了。假设有三个自变量，很有可能不能够在样本中找到和这三个自变量的组合非常接近的数据，此时我们就应该借助更系统的方法了。

### 4.0.2 一元线性回归模型

回归分析是一种数学模型。当因变量和自变量为线性关系时，它是一种特殊的线性模型。

最简单的情形是一元线性回归，由大体上有线性关系的一个自变量和一个因变量组成，模型是：

$$Y=a+bX+ε \tag{1}$$

X是自变量，Y是因变量，ε是随机误差，a和b是参数，在线性回归模型中，a和b是我们要通过算法学习出来的。


### 4.0.3 解决方案

我们可以用几种方法来解决这个问题：

1. 最小二乘法
2. 梯度下降法
3. 简单的神经网络法
4. 更通用的神经网络算法

### 4.0.4 公式形态

这里要解释一下线性公式中w和x的顺序问题。在很多教科书中，我们可以看到下面的公式：

$$y = w^Tx+b \tag{1}$$

或者：

$$y = w \cdot x + b \tag{2}$$

而我们在本书中使用：

$$y = x \cdot w + b \tag{3}$$

这三者的主要区别是样本数据x的形状定义，相应地会影响到w的形状定义。举例来说，如果x有三个特征值，那么w必须有三个权重值与每一个特征值对应，则：

- 公式1的矩阵形式是：

x是列向量：

$$
x=
\begin{pmatrix}
x_{1} \\
x_{2} \\
x_{3}
\end{pmatrix}
$$

w也是列向量：

$$
w=
\begin{pmatrix}
w_{1} \\ w_{2} \\ x_{3}
\end{pmatrix}
$$
$$
y=w^Tx+b=
\begin{pmatrix}
w1 & w2 & w3
\end{pmatrix}
\begin{pmatrix}
x_{1} \\
x_{2} \\
x_{3}
\end{pmatrix}
+b
$$
$$
=w1 \cdot x1 + w2 \cdot x2 + w3 \cdot x3 + b \tag{4}
$$

w和x都是列向量，所以需要先把w转置后，再与x做矩阵乘法。

- 公式2的矩阵形式

公式2与公式1的区别是w的形状，在公式2中，w直接就是个行向量：

$$
w=
\begin{pmatrix}
w_{1} & w_{2} & w_{3}
\end{pmatrix}
$$

而x的形状仍然是列向量：

$$
x=
\begin{pmatrix}
x_{1} \\
x_{2} \\
x_{3}
\end{pmatrix}
$$


这样相乘之前不需要做矩阵转置了：

$$
y=wx+b=
\begin{pmatrix}
w1 & w2 & w3
\end{pmatrix}
\begin{pmatrix}
x_{1} \\
x_{2} \\
x_{3}
\end{pmatrix}
+b
$$
$$
=w1 \cdot x1 + w2 \cdot x2 + w3 \cdot x3 + b \tag{5}
$$

- 公式3的矩阵形式

x是个行向量：

$$
x=
\begin{pmatrix}
x_{1} & x_{2} & x_{3}
\end{pmatrix}
$$

w是列向量：

$$
w=
\begin{pmatrix}
w_{1} \\ w_{2} \\ x_{3}
\end{pmatrix}
$$

所以x在前，w在后：

$$
y=x \cdot w+b=
\begin{pmatrix}
x_1 & x_2 & x_3
\end{pmatrix}
\begin{pmatrix}
w_{1} \\
w_{2} \\
w_{3}
\end{pmatrix}
+b
$$
$$
=x1 \cdot w1 + x2 \cdot w2 + x3 \cdot w3 + b \tag{6}
$$

比较公式4，5，6，其实最后的运算结果是相同的。

我们再分析一下前两种形式的x矩阵，由于x是个列向量，意味着特征由行表示，当有2个样本同时参与计算时，x需要增加一列，变成了如下形式：

$$
x=
\begin{pmatrix}
x_{11} & x_{21} \\
x_{12} & x_{22} \\
x_{13} & x_{23} 
\end{pmatrix}
$$

x的第一个下标表示样本序号，第二个下标表示样本特征，所以$x_{21}$是第2个样本的第1个特征。看$x_{21}$这个序号很别扭，一般我们都是认为行在前、列在后，但是$x_{21}$却是处于第1行第2列，和习惯正好相反。

如果采用第三种形式，则两个样本的x的矩阵是：

$$
x=
\begin{pmatrix}
x_{11} & x_{12} & x_{13} \\
x_{21} & x_{22} & x_{23}
\end{pmatrix}
$$

## 4.1 最小二乘法

### 4.1.1 历史

最小二乘法，也叫做最小平方法（Least Square），它通过最小化误差的平方和寻找数据的最佳函数匹配。利用最小二乘法可以简便地求得未知的数据，并使得这些求得的数据与实际数据之间误差的平方和为最小。最小二乘法还可用于曲线拟合。其他一些优化问题也可通过最小化能量或最大化熵用最小二乘法来表达。

1801年，意大利天文学家朱赛普·皮亚齐发现了第一颗小行星谷神星。经过40天的跟踪观测后，由于谷神星运行至太阳背后，使得皮亚齐失去了谷神星的位置。随后全世界的科学家利用皮亚齐的观测数据开始寻找谷神星，但是根据大多数人计算的结果来寻找谷神星都没有结果。时年24岁的高斯也计算了谷神星的轨道。奥地利天文学家海因里希·奥尔伯斯根据高斯计算出来的轨道重新发现了谷神星。

高斯使用的最小二乘法的方法发表于1809年他的著作《天体运动论》中。法国科学家勒让德于1806年独立发明“最小二乘法”，但因不为世人所知而默默无闻。勒让德曾与高斯为谁最早创立最小二乘法原理发生争执。

1829年，高斯提供了最小二乘法的优化效果强于其他方法的证明，因此被称为高斯-马尔可夫定理。

### 4.1.2 数学原理

线性回归试图学得：

$$z(x_i)=w \cdot x_i+b \tag{1}$$

使得：

$$z(x_i) \simeq y_i \tag{2}$$

### 4.1.3 代码实现
### 4.1.4 运算结果

## 4.2 梯度下降法

### 4.2.1 数学原理

在下面的公式中，我们规定x是样本特征值（单特征），y是样本标签值，z是预测值，下标 $i$ 表示其中一个样本。

#### 预设函数（Hypothesis Function）

为一个线性函数：

$$z_i = x_i \cdot w + b \tag{1}$$

#### 损失函数（Loss Function）

为均方差函数：

$$loss(w,b) = \frac{1}{2} (z_i-y_i)^2 \tag{2}$$


与最小二乘法比较可以看到，梯度下降法和最小二乘法的模型及损失函数是相同的，都是一个线性模型加均方差损失函数，模型用于拟合，损失函数用于评估效果。

区别在于，最小二乘法从损失函数求导，直接求得数学解析解，而梯度下降以及后面的神经网络，都是利用导数传递误差，再通过迭代方式一步一步逼近近似解。

### 4.2.2 梯度计算

#### 计算z的梯度

根据公式2：
$$
{\partial loss \over \partial z_i}=z_i - y_i \tag{3}
$$

#### 计算w的梯度

我们用loss的值作为误差衡量标准，通过求w对它的影响，也就是loss对w的偏导数，来得到w的梯度。由于loss是通过公式2->公式1间接地联系到w的，所以我们使用链式求导法则，通过单个样本来求导。

根据公式1和公式3：

$$
{\partial{loss} \over \partial{w}} = \frac{\partial{loss}}{\partial{z_i}}\frac{\partial{z_i}}{\partial{w}}=(z_i-y_i)x_i \tag{4}
$$

#### 计算b的梯度

$$
\frac{\partial{loss}}{\partial{b}} = \frac{\partial{loss}}{\partial{z_i}}\frac{\partial{z_i}}{\partial{b}}=z_i-y_i \tag{5}
$$

### 4.2.3 代码实现
### 4.2.4 运行结果

## 4.3 神经网络法
在梯度下降法中，我们简单讲述了一下神经网络做线性拟合的原理，即：

1. 初始化权重值
2. 根据权重值放出一个解
3. 根据均方差函数求误差
4. 误差反向传播给线性计算部分以调整权重值
5. 是否满足终止条件？不满足的话跳回2

### 4.3.1 定义神经网络结构

#### 输入层

此神经元在输入层只接受一个输入特征，经过参数w,b的计算后，直接输出结果。这样一个简单的“网络”，只能解决简单的一元线性回归问题，而且由于是线性的，我们不需要定义激活函数，这就大大简化了程序，而且便于大家循序渐进地理解各种知识点。

严格来说输入层在神经网络中并不能称为一个层。

#### 权重w/b

因为是一元线性问题，所以w/b都是一个标量。

#### 输出层

输出层1个神经元，线性预测公式是：

$$z_i = x_i \cdot w + b$$

z是模型的预测输出，y是实际的样本标签值，下标 $i$ 为样本。

#### 损失函数

因为是线性回归问题，所以损失函数使用均方差函数。

$$loss(w,b) = \frac{1}{2} (z_i-y_i)^2$$

### 4.3.2 反向传播

#### 计算w的梯度

$$
{\partial{loss} \over \partial{w}} = \frac{\partial{loss}}{\partial{z_i}}\frac{\partial{z_i}}{\partial{w}}=(z_i-y_i)x_i
$$

#### 计算b的梯度

$$
\frac{\partial{loss}}{\partial{b}} = \frac{\partial{loss}}{\partial{z_i}}\frac{\partial{z_i}}{\partial{b}}=z_i-y_i
$$


### 4.3.3 代码实现
#### 定义类
#### 前向计算
#### 反向传播
#### 梯度更新
#### 训练过程

只训练一轮的算法是：

***
for 循环，直到所有样本数据使用完毕：
1. 读取一个样本数据
2. 前向计算
3. 反向传播
4. 更新梯度
***

#### 推理预测

#### 主程序

### 4.3.4 运行结果


## 4.4 多样本单特征值计算

单样本计算有一些缺点：
1. 很有可能前后两个相邻的样本，会对反向传播产生相反的作用而互相抵消。假设样本1造成了误差为0.5，w的梯度计算结果是0.1；紧接着样本2造成的误差为-0.5，w的梯度计算结果是-0.1，那么前后两次更新w就会产生互相抵消的作用。
2. 在样本数据量大时，逐个计算会花费很长的时间。由于我们在本例中样本量不大（200个样本），所以计算速度很快，觉察不到这一点。在实际的工程实践中，动辄10万甚至100万的数据量，轮询一次要花费很长的时间。

### 4.4.1 前向计算

由于有多个样本同时计算，所以我们使用$x_i$表示第 $i$ 个样本，X是样本组成的矩阵，Z是计算结果矩阵，w和b都是标量：

$$
Z = X \cdot w + b \tag{1}
$$

把它展开成3个样本（3行，每行代表一个样本）的形式：

$$
X=\begin{pmatrix}
    x_1 \\ 
    x_2 \\ 
    x_3
\end{pmatrix}
$$

$$
Z= 
\begin{pmatrix}
    x_1 \\ 
    x_2 \\ 
    x_3
\end{pmatrix} \cdot w + b
$$
$$
=\begin{pmatrix}
    x_1 \cdot w + b \\ 
    x_2 \cdot w + b \\ 
    x_3 \cdot w + b
\end{pmatrix}
=\begin{pmatrix}
    z_1 \\ 
    z_2 \\ 
    z_3
\end{pmatrix} \tag{2}
$$

### 4.4.2 损失函数

用传统的均方差函数，其中，z是每一次迭代的预测输出，y是样本标签数据。我们使用m个样本参与计算，因此损失函数为：

$$J(w,b) = \frac{1}{2m}\sum_{i=1}^{m}(z_i - y_i)^2$$

我们假设每次有3个样本参与计算，即m=3，则损失函数实例化后的情形是：

$$
J(w,b) = \frac{1}{2\times3}[(z_1-y_1)^2+(z_2-y_2)^2+(z_3-y_3)^2]
$$
$$
=sum[(Z-Y)^2]/3/2 \tag{3}
$$

### 4.4.3 求w的梯度
### 4.4.4 求b的梯度

## 4.5实现逻辑非门

### 4.5.1 原理

单层神经网络，又叫做感知机，它可以轻松实现逻辑与、或、非门。由于逻辑与、或门，需要有两个变量输入，目前我们只学习了单变量输入，所以，我们可以先实现非门。

### 4.5.2 代码实现
### 4.5.3 运行结果