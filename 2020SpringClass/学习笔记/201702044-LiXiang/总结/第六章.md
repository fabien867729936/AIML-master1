# 第6章 多入单出的单层神经网路

## 6.0 线性二分类

### 6.0.1 提出问题
### 6.0.2 逻辑回归模型

回归问题可以分为两类：线性回归和逻辑回归。在第二步中，我们学习了线性回归模型，在第三步中，我们将一起学习逻辑回归模型。

逻辑回归的英文是Logistic Regression，逻辑回归是用来计算“事件=Success”和“事件=Failure”的概率。当因变量的类型属于二元（1 / 0，真/假，是/否）变量时，我们就应该使用逻辑回归。

回忆线性回归，使用一条直线拟合样本数据，而逻辑回归是“拟合”0或1两个数值，而不是具体的连续数值，所以它叫广义线性模型。逻辑回归又称logistic回归分析，常用于数据挖掘，疾病自动诊断，经济预测等领域。

自变量既可以是连续的，也可以是分类的。然后通过logistic回归分析，可以得到自变量的权重，从而可以大致了解到底哪些因素是胃癌的危险因素。同时根据该权值可以根据危险因素预测一个人患癌症的可能性。

逻辑回归的另外一个名字叫做分类器，分为线性分类器和非线性分类器，本章中我们学习线性分类器。而无论是线性还是非线性分类器，又分为两种：二分类问题和多分类问题，在本章中我们学习二分类问题。线性多分类问题将会在下一章讲述，非线性分类问题在后续的步骤中讲述。

综上所述，我们本章要学习的路径是：回归问题->逻辑回归问题->线性逻辑回归即分类问题->线性二分类问题。


## 6.1 二分类函数

此函数对线性和非线性二分类都适用。

### 6.1.1 二分类函数

对率函数Logistic Function，即可以做为激活函数使用，又可以当作二分类函数使用。在二分类任务中，叫做Logistic函数，而在作为激活函数时，叫做Sigmoid函数。

- 公式

$$a(z) = \frac{1}{1 + e^{-z}}$$

- 导数

$$a^{'}(z) = a(z)(1 - a(z))$$

具体求导过程可以参考8.1节。

- 输入值域

$$(-\infty, \infty)$$

- 输出值域

$$(0,1)$$


### 6.1.2 正向传播

#### 矩阵运算

$$
z=x \cdot w + b \tag{1}
$$

#### 分类计算

$$
a = Logistic(z)={1 \over 1 + e^{-z}} \tag{2}
$$

#### 损失函数计算

二分类交叉熵损失函数：

$$
loss(w,b) = -[y \ln a+(1-y)\ln(1-a)] \tag{3}
$$

### 6.1.3 反向传播

#### 求损失函数loss对a的偏导

$$
\frac{\partial loss}{\partial a}=-[{y \over a}+{-(1-y) \over 1-a}]=\frac{a-y}{a(1-a)} \tag{4}
$$

#### 求损失函数a对z的偏导

$$
\frac{\partial a}{\partial z}= a(1-a) \tag{5}
$$

#### 求损失函数loss对z的偏导

使用链式法则链接公式4和公式5：

$$
\frac{\partial loss}{\partial z}=\frac{\partial loss}{\partial a}\frac{\partial a}{\partial z}
$$
$$
=\frac{a-y}{a(1-a)} \cdot a(1-a)=a-y \tag{6}
$$
使用交叉熵函数求导得到的分母，与Logistic分类函数求导后的结果，正好可以抵消，最后只剩下了$a-y$这一项。以满足以下条件：
1. 损失函数满足二分类的要求，无论是正例还是反例，都是单调的；
2. 损失函数可导，以便于使用反向传播算法；
3. 让计算过程非常简单，一个减法就可以搞定。

#### 多样本情况

我们用三个样本做实例化推导：

$$Z=
\begin{pmatrix}
  z_1 \\ z_2 \\ z_3
\end{pmatrix},
A=logistic\begin{pmatrix}
  z_1 \\ z_2 \\ z_3
\end{pmatrix}=
\begin{pmatrix}
  a_1 \\ a_2 \\ a_3
\end{pmatrix}
$$
$$
J(w,b)= -[y_1 \ln a_1+(1-y_1)\ln(1-a_1)] 
$$
$$
-[y_2 \ln a_2+(1-y_2)\ln(1-a_2)] 
$$
$$
-[y_3 \ln a_3+(1-y_3)\ln(1-a_3)] 
$$

$$
{\partial J(w,b) \over \partial Z}=
\begin{pmatrix}
  {\partial J(w,b) / \partial z_1} \\
  {\partial J(w,b) / \partial z_2} \\
  {\partial J(w,b) / \partial z_3}
\end{pmatrix} \tag{代入公式6结果}
$$
$$
=\begin{pmatrix}
  a_1-y_1 \\
  a_2-y_2 \\
  a_3-y_3 
\end{pmatrix}=A-Y
$$

所以，用矩阵运算时可以简化为矩阵相减的形式：$A-Y$。

### 6.1.4 对数几率的来历

经过数学推导后可以知道，神经网络实际也是在做这样一件事：经过调整w和b的值，把所有正例的样本都归纳到大于0.5的范围内，所有负例都小于0.5。但是如果只说大于或者小于，无法做准确的量化计算，所以用一个对率函数来模拟。

## 6.2 线性二分类的神经网络实现
如何用神经网络在两组不同标签的样本之间画一条明显的分界线。这条分界线可以是直线，也可以是曲线。这就是二分类问题。如果只画一条分界线的话，无论是直线还是曲线，我们可以用一支假想的笔（即一个神经元），就可以达到目的，也就是说笔的走向，完全依赖于这一个神经元根据输入信号的判断。
1. 从视觉上判断是线性可分的，所以我们使用单层神经网络即可；
2. 输入特征是经度和纬度，所以我们在输入层设置两个输入X1=经度，X2=维度；
3. 最后输出的是一个二分类，分别是楚汉地盘，可以看成非0即1的二分类问题，所以我们只用一个输出单元就可以了。

### 6.2.1 定义神经网络结构
根据前面的猜测，看来我们只需要一个二入一出的神经元就可以搞定。这个网络只有输入层和输出层，由于输入层不算在内，所以是一层网络。
#### 输入层
输入经度(x1)和纬度(x2)两个特征：

$$
x=\begin{pmatrix}
x_{1} & x_{2}
\end{pmatrix}
$$

#### 权重矩阵
输入是2个特征，输出一个数，则W的尺寸就是2x1：

$$
w=\begin{pmatrix}
w_{1} \\ w_{2}
\end{pmatrix}
$$

B的尺寸是1x1，行数永远是1，列数永远和W一样。

$$
b=\begin{pmatrix}
b_{1}
\end{pmatrix}
$$

#### 输出层

$$
z = x \cdot w + b
=\begin{pmatrix}
    x_1 & x_2
\end{pmatrix}
\begin{pmatrix}
    w_1 \\ w_2
\end{pmatrix}
$$
$$
=x_1 \cdot w_1 + x_2 \cdot w_2 + b \tag{1}
$$
$$a = Logistic(z) \tag{2}$$

#### 损失函数
二分类交叉熵函损失数：

$$
loss(w,b) = -[yln a+(1-y)ln(1-a)] \tag{3}
$$

### 6.2.2 反向传播
### 6.2.3 代码实现
### 6.2.4 运行结果

# 6.3 线性二分类原理

### 6.3.1 线性分类和线性回归的异同
此原理对线性和非线性二分类都适用。

线性分类，试图在含有两种样本的空间中划出一条分界线，让双方截然分开，就好像是中国象棋的棋盘中的楚河汉界一样。与线性回归相似的地方是，两者都需要划出那条“直线”来，但是不同的地方也很明显：

||线性回归|线性分类|
|---|---|---|
|相同点|需要在样本群中找到一条直线|需要在样本群中找到一条直线|
|不同点|用直线来拟合所有样本，使得各个样本到这条直线的距离尽可能最短|用直线来分割所有样本，使得正例样本和负例样本尽可能分布在直线两侧|

### 6.3.2 二分类过程

以单样本双特征值为例来说明神经网络的二分类过程，这是用代数方式来解释其工作原理。

1. 正向计算

$$
z = x_1 w_1+ x_2 w_2 + b  \tag{1}
$$

2. 分类计算

$$
a={1 \over 1 + e^{-z}} \tag{2}
$$

3. 损失函数计算

$$
loss = -[y \ln (a)+(1-y) \ln (1-a)] \tag{3}
$$

### 6.3.3 数值计算举例
### 6.3.4 二分类的几何原理

## 6.3 二分类结果可视化

### 6.3.1 可视化的重要性
神经网络的可视化，说简单也很简单，说难也很难，关键是对框架系统的理解，对运行机制和工作原理的理解，掌握了这些，可视化就会使一件轻而易举且令人愉快的事情。

### 6.3.2 权重值的含义
### 6.3.3 代码实现
### 6.3.4 运行结果

## 6.4 实现逻辑与门和或门
用分类的思想来实现下列4个逻辑门：
- 与门 AND
- 与非门 NAND
- 或门 OR
- 或非门 NOR
### 6.4.1 网络模型
### 6.4.2 训练样本

每个类型的逻辑门都只有4个训练样本。

- 逻辑与门的样本
- 逻辑与非门的样本
- 逻辑或门的样本
- 逻辑或非门的样本
### 6.4.4 代码实现

#### 读取数据
#### 测试函数
#### 训练函数
### 6.4.5 运行结果

## 6.5 用双曲正切函数做二分类函数
### 6.5.1 提出问题
**提出问题：能不能用双曲正切函数作为分类函数呢？**

对于对率函数来说，一般使用0.5作为正负类的分界线，那我们会自然地想到，对于双曲正切函数来说，可以使用0作为正负类的分界线。

所谓分界线，其实只是人们理解神经网络做二分类的一种方式，对于神经网络来说，其实并没有分界线这个概念，它要做的是通过线性变换，尽量把正例向上方推，把负例向下方推。

### 6.5.2 修改前向计算和反向传播函数
#### 增加双曲正切分类函数
#### 修改前向计算方法
#### 修改反向传播方法
### 6.5.3 修改损失函数
#### 增加新的损失函数
#### 修改反向传播方法
### 6.5.4 再次修改损失函数代码
### 6.5.5 修改样本数据标签值
