# 第三次上课总结
## 线性回归
### 摘要
用线性回归作为学习神经网络的起点，是一个非常好的选择，因为线性回归问题本身比较容易理解，在它的基础上，逐步的增加一些新的知识点，会形成一条比较平缓的学习曲线，或者说是迈向神经网络的第一个小台阶。

单层的神经网络，其实就是一个神经元，可以完成一些线性的工作，比如拟合一条直线，这用一个神经元就可以实现。当这个神经元只接收一个输入时，就是单变量线性回归，可以在二维平面上用可视化方法理解。当接收多个变量输入时，叫做多变量线性回归，此时可视化方法理解就比较困难了，通常我们会用变量两两组对的方式来表现。

当变量多于一个时，两个变量的量纲和数值有可能差别很大，这种情况下，我们通常需要对样本特征数据做归一化，然后把数据喂给神经网络进行训练，否则会出现“消化不良”的情况。
### 单变量线性回归问题
利用已有值，预测未知值。
### 一元线性回归模型
$$Y=a+bX+ε \tag{1}$$

X是自变量，Y是因变量，ε是随机误差，a和b是参数，在线性回归模型中，a和b是我们要通过算法学习出来的。
### 单样本随机梯度下降

SDG(Stochastic Grident Descent)
#### 特点
  
  - 训练样本：每次使用一个样本数据进行一次训练，更新一次梯度，重复以上过程。
  - 优点：训练开始时损失值下降很快，随机性大，找到最优解的可能性大。
  - 缺点：受单个样本的影响最大，损失函数值波动大，到后期徘徊不前，在最优解附近震荡。不能并行计算。
### 小批量样本梯度下降

Mini-Batch Gradient Descent
#### 特点

  - 训练样本：选择一小部分样本进行训练，更新一次梯度，然后再选取另外一小部分样本进行训练，再更新一次梯度。
  - 优点：不受单样本噪声影响，训练速度较快。
  - 缺点：batch size的数值选择很关键，会影响训练结果。
  - ### 全批量样本梯度下降 

Full Batch Gradient Descent
#### 特点

  - 训练样本：每次使用全部数据集进行一次训练，更新一次梯度，重复以上过程。
  - 优点：受单个样本的影响最小，一次计算全体样本速度快，损失函数值没有波动，到达最优点平稳。方便并行计算。
  - 缺点：数据量较大时不能实现（内存限制），训练过程变慢。初始值不同，可能导致获得局部最优解，并非全局最优解。
  ### 实现逻辑非门
  #### 原理
  单层神经网络，又叫做感知机，它可以轻松实现逻辑与、或、非门。由于逻辑与、或门，需要有两个变量输入，目前我们只学习了单变量输入，所以，我们可以先实现非门。

很多阅读材料上会这样介绍：有公式
$z=wx+b$，令$w=-1,b=1$，则：

- 当x=0时，$z = -1 \times 0 + 1 = 1$
- 当x=1时，$z = -1 \times 1 + 1 = 0$
  

这是人工找到的精确解，如何使用单层神经网络训练出这个结果呢？我们先分析一下输入数据：

||x|y|
|---|---|---|
|样本1|0|1|
|样本2|1|0|

我们可以看作是用一条直线来拟合这两个样本：当x=0的时候，y=1；当x=1的时候，y=0。
### 神经网络解法
与单特征值的线性回归问题类似，多变量（多特征值）的线性回归可以被看做是一种高维空间的线性拟合。以具有两个特征的情况为例，这种线性拟合不再是用直线去拟合点，而是用平面去拟合点。
### 样本特征数据归一化
#### 为什么要做归一化
理论层面上，神经网络是以样本在事件中的统计分布概率为基础进行训练和预测的，所以它对样本数据的要求比较苛刻。
#### 归一化

    把数据线性地变成[0,1]或[-1,1]之间的小数，把带单位的数据（比如米，公斤）变成无量纲的数据，区间缩放。

    归一化有三种方法:

1. Min-Max归一化：
$$x_{new}={x-x_{min} \over x_{max} - x_{min}} \tag{1}$$

2. 平均值归一化
   
$$x_{new} = {x - \bar{x} \over x_{max} - x_{min}} \tag{2}$$

3. 非线性归一化

对数转换：
$$y=log(x) \tag{3}$$

反余切转换：
$$y=atan(x) \cdot 2/π  \tag{4}$$

#### 标准化

把每个特征值中的所有数据，变成平均值为0，标准差为1的数据，最后为正态分布。Z-score规范化（标准差标准化 / 零均值标准化，其中std是标准差）：

$$x_{new} = (x - \bar{x})／std \tag{5}$$

#### 中心化

平均值为0，无标准差要求：
$$x_{new} = x - \bar{x} \tag{6}$$
#### 从经验上说，归一化是让不同维度之间的特征在数值上有一定比较性，可以大大提高分类器的准确性。
![](\03.png)
### 预测数据的归一化
我们在针对训练数据做归一化时，得到的最重要的数据是训练数据的最小值和最大值，我们只需要把这两个值记录下来，在预测时使用它们对预测数据做归一化，这就相当于把预测数据“混入”训练数据。前提是预测数据的特征值不能超出训练数据的特征值范围，否则有可能影响准确程度。
#### 代码实现
min-max标准化（Min-Max Normalization）

min-max标准化python代码如下：
![](\04.png)
下面将数据缩至0-1之间，采用MinMaxScaler函数
![](\05.png)
最后输出
![](\06.png)
## 总结与心得体会
通过本节课的学习，我学习到了线性回归及数据归一化等一系列的神经网络算法而我们 为什么用梯度下降？
由浅入深，我们最容易想到的调整参数（权重和偏置）是穷举。即取遍参数的所有可能取值，比较在不同取值情况下得到的损失函数的值，即可得到使损失函数取值最小时的参数值。然而这种方法显然是不可取的。因为在深度神经网络中，参数的数量是一个可怕的数字，动辄上万，十几万。并且，其取值有时是十分灵活的，甚至精确到小数点后若干位。若使用穷举法，将会造成一个几乎不可能实现的计算量。第二个想到的方法就是微分求导。通过将损失函数进行全微分，取全微分方程为零或较小的点，即可得到理想参数。（补充：损失函数取下凸函数，才能使得此方法可行。现实中选取的各种损失函数大多也正是如此。）可面对神经网络中庞大的参数总量，纯数学方法几乎是不可能直接得到微分零点的。因此我们使用了梯度下降法。既然无法直接获得该点，那么我们就想要一步一步逼近该点。一个常见的形象理解是，爬山时一步一步朝着坡度最陡的山坡往下，即可到达山谷最底部。（至于为何不能闪现到谷底，原因是参数数量庞大，表达式复杂，无法直接计算）我们都知道，向量场的梯度指向的方向是其函数值上升最快的方向，也即其反方向是下降最快的方向。计算梯度的方式就是求偏导。这里需要引入一个步长的概念。个人理解是：此梯度对参数当前一轮学习的影响程度。步长越大，此梯度影响越大。若以平面直角坐标系中的函数举例，若初始参数x=10，步长为1 。那么参数需要调整十次才能到达谷底。若步长为5，则只需2次。若为步长为11，则永远无法到达真正的谷底。本次课的学习又加深了我对神经网络的理解，为之后更加深层次的学习打下了基础。