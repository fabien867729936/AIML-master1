# 第一单元
## 请举例分析BP神经网络的应用，思路尽量具体
### 具体应用实例。根据表，预测序号15的跳高成绩。
#### 数据整理
我们将前14组国内男子跳高运动员各项素质指标作为输入，即（30m行进跑，立定三级跳远，助跑摸高，助跑4-6步跳高，负重深蹲杠铃，杠铃半蹲系数，100m，抓举），将对应的跳高成绩作为输出。并用matlab自带的premnmx()函数将这些数据归一化处理。

数据集：（注意：每一列是一组输入训练集，行数代表输入层神经元个数，列数输入训练集组数）

P=[3.2 3.2 3 3.2 3.2 3.4 3.2 3 3.2 3.2 3.2 3.9 3.1 3.2;

9.6 10.3 9 10.3 10.1 10 9.6 9 9.6 9.2 9.5 9 9.5 9.7;

3.45 3.75 3.5 3.65 3.5 3.4 3.55 3.5 3.55 3.5 3.4 3.1 3.6 3.45;

2.15 2.2 2.2 2.2 2 2.15 2.14 2.1 2.1 2.1 2.15 2 2.1 2.15;

140 120 140 150 80 130 130 100 130 140 115 80 90 130;

2.8 3.4 3.5 2.8 1.5 3.2 3.5 1.8 3.5 2.5 2.8 2.2 2.7 4.6;

11 10.9 11.4 10.8 11.3 11.5 11.8 11.3 11.8 11 11.9 13 11.1 10.85;

50 70 50 80 50 60 65 40 65 50 50 50 70 70];

T=[2.24 2.33 2.24 2.32 2.2 2.27 2.2 2.26 2.2 2.24 2.24 2.2 2.2 2.35];

#### 模型建立
BP网络模型

    BP网络（Back-ProPagation Network）又称反向传播神经网络， 通过样本数据的训练，不断修正网络权值和阈值使误差函数沿负梯度方向下降，逼近期望输出。它是一种应用较为广泛的神经网络模型，多用于函数逼近、模型识别分类、数据压缩和时间序列预测等。
![](\81.png)
模型求解 

#### 网络结构设计

1) 输入输出层的设计

该模型由每组数据的各项素质指标作为输入，以跳高成绩作为输出，所以输入层的节点数为8，输出层的节点数为1。

2) 隐层设计

有关研究表明, 有一个隐层的神经网络, 只要隐节点足够多, 就可以以任意精度逼近一个非线性函数。因此, 本文采用含有一个隐层的三层多输入单输出的BP网络建立预测模型。在网络设计过程中, 隐层神经元数的确定十分重要。隐层神经元个数过多, 会加大网络计算量并容易产生过度拟合问题; 神经元个数过少, 则会影响网络性能, 达不到预期效果。网络中隐层神经元的数目与实际问题的复杂程度、输入和输出层的神经元数以及对期望误差的设定有着直接的联系。目前, 对于隐层中神经元数目的确定并没有明确的公式, 只有一些经验公式, 神经元个数的最终确定还是需要根据经验和多次实验来确定。本文在选取隐层神经元个数的问题上参照了以下的经验公式:
![](\82.png)

其中, n为输入层神经元个数, m 为输出层神经元个数, a 为[ 1, 10]之间的常数。 

根据上式可以计算出神经元个数为4-13个之间，在本次实验中选择隐层神经元个数为6.

网络结构示意图如下：
![](\83.png)
#### 激励函数的选取

BP神经网络通常采用Sigmoid可微函数和线性函数作为网络的激励函数。本文选择S型正切函数tansig作为隐层神经元的激励函数。而由于网络的输出归一到[ -1, 1]范围内, 因此预测模型选取S 型对数函数tansig作为输出层神经元的激励函数。

#### 模型的实现

此次预测选用MATLAB中的神经网络工具箱进行网络的训练, 预测模型的具体实现步骤如下:

将训练样本数据归一化后输入网络, 设定网络隐层和输出层激励函数分别为tansig和logsig函数, 网络训练函数为traingdx, 网络性能函数为mse,隐层神经元数初设为6。设定网络参数。网络迭代次数epochs为5000次, 期望误差goal为0.00000001, 学习速率lr为0. 01。设定完参数后, 开始训练网络。
![](\84.png)
该网络通过24次重复学习达到期望误差后则完成学习。

网络训练完成后，只需要将各项素质指标输入网络即可得到预测数据。

    预测结果为：2.20

# 第二单元
## 举例说明深度学习的应用
medical
* 在医疗领域，可以用来识别癌细胞，发现新药物等。CNN还可以用来识别异常的肿瘤或者癌细胞。

# 第三单元
## 举例说明卷积神经网络的应用
### 目标检测
图像识别中，目标检测的任务，是对输入图像样本准确进行分类的基础上，检测其中包含的某些目标，并对它们准确定位并标识。

### 目标定位
图像分类问题一般都采用Softmax回归来解决，最后输出的结果是一个多维列向量，且向量的维数与假定的分类类别数一致。在此基础上希望检测其中的包含的各种目标并对它们进行定位，这里对这个监督学习任务的标签表示形式作出定义。
![](\85.png)
### R-CNN
R-CNN（Region CNN)是Girshick等人2013年在论文[Rich feature hierarchies for accurate object detection and semantic segmentation]中提出的一种目标检测算法，其中提出的候选区域（Region Proposal）概念在计算机视觉领域有很大的影响力，它可以说是利用深度学习进行目标检测的开山之作。
R-CNN意为带区域的卷积网络，类似之前所述的滑窗检测算法，先用卷积网络训练一个能够准确识别目标的分类器，但这个算法试图选出一些区域为候选区域，只在这些区域也就是只在少数的窗口上运行分类器。候选区域的选取采用的是一种称为图像分割的算法。
后续有一系列的研究工作，试图改进这个算法，而出现了Fast R-CNN、Faster R-CNN算法，不过这些算法在运行速度方面还是不如YOLO算法。

# 第四单元
## 阅读算法攻破人脸识别“口罩”难题，两天落地千人小区准确率达97%，讨论卷积神经网络在实际过程中可能碰到的问题以及解决方案
### 智能机器人
在2006年前，大多数机器学习方法是使用浅结构模型来处理数据，且结构模型至多只有一层或两层的非线性特征的层。最有代表性的若干浅层结构：高斯混合模型（Gaussian Mixture Model,GMM），K均值聚类，支持向量机（Support Vector Machine,SVM）、Logistic回归等。

目前，浅层结构模型已经被应用于解决一些在简单的实际问题，但是当有复杂的真实世界问题时，浅层模型将不能很好的表达，比如自然图像、自然语言（NLP）、自然声音和人类语音等领域时，这些模型效果和表达能力将会不起作用。

深度学习（Deep Learning, DL），从狭义上理解，就是一种具有一定的结构和训练方法且含有多个隐含层的神经网络；从广义上理解，可以把具有任何层次结构的机器学习方法称为深度学习。在深度学习过程中，从输入图像，经过无监督的逐层训练和学习图像特征，通过有监督的训练更新整个网络参数，最小化损失函数，在输出层实现正确的分类。最近的几年里，深度学习在机器学习领域得到了飞速的发展，相关的理论成果和实践成果也层出不穷，其主要的贡献是在自然语言处理、图像分类任务、声音识别等领域上。自1974年Paul Werbos等人提出反向传播算法（BP算法），解决了由浅层的神经网络模型应用到深层的神经网络模型中线性不可分的问题。深层神经网络一般使用反向传播算法的方法来训练，但是由于层数较多并且随机初始化的方法较粗糙，随着深度加深，会造成的训练结果不稳定或产生“梯度消失”的问题，使得深层神经网络无法work。

### 深度网络

直到2006年多伦多大学的Geoff Hinton大牛在《Science》杂志上发表的一篇关于深度置信网（Deep Belief Networks，DBN）的文章，他的方法是层叠很多个受限玻尔兹曼机（Restricted Boltzmann Machine,RBMs）组成的一个深度置信网络，经过无监督的逐层贪心算法来训练，解决了局部最优、梯度消失等问题，并在MN1ST手写数据库上取得很高的识别率，结果为98.8%。

受DBN方法的启发，2007年，另一位深度学习大牛Benigo将DBN中的每层RBM替换为自动编码机（Auto-encoder），提出了层叠自动编码机（Stacked Auto-encoder，SAE）的深度结构，经过对MN1ST手写数字数据库的无监督预训练，取得了98.6%识别率。此时，深度学习的三种常用的基本架构就是DBN结构、SAE结构与1998年LeCun提出的卷积神经网络结构（Convolutional Neural Network, CNN）。在学术界和工业界引起了广大的关注，另外在人脸识别上、自然图像Imagenet任务识别上取得了非常大的的成果。

2012年6月，《纽约时报》报道了由著名机器学习专家Andrew Ng和Google软件架构师Jeff Dean主持的Google Brains项目，该项目使用包含16000颗CPU的并行计算平台，在没有标注信息的情况下，对深度神经网络进行无监督训练，网络学会了自动从数百万张Youtube视频截图中识别猫脸。

### Google Brains

2013年，Wan提出使用dropconnect的网络模型，该模型取得了在MNIST数据库上0.28%的分类错误率，在NROB数据库上3.23%的识别错误率以及在CIFAR-10数据库上9.32%的分类错误率。2014年Sun Yi提出一种深度隐藏身份特征的方法(deep hidden identity feature，Deep ID)来对人脸识别，在人脸识别库LFW中获得了3.55%的错误率。其方法是通过裁剪等手段把人脸分为多个块，然后把每个区域块输入到卷积神经网络中，使用卷积池化对图像特征进行提取，不断地训练网络参数，优化最小损失函数，并在最后一层隐藏层的神经元激活值中生成Deep ID特征。

2015年1月百度开发的计算机视觉系统Deep Image，这一系统是针对超级计算机对深度学习算法优化而设计，并在Image Net对象识别中，这一系统的top-5错误率仅为5.98%。2015年5月LeCun在《nature》上发表了一篇关于深度学习的综述，详细了描述了近年来深度学习在语音识别、图像分类、物体检测等领域取得的重大成果。

现在深度学习网络模型已能够识别较为常见的自然图像。深度学习的模型随着不断的更新改进以及计算机硬件特别是 GPU 的发展，不止提高了图像分类的准确度，而且也避免了大量的人工特征提取的工作。在不久的未来，随着图像数据增多，基于深度学习对图像处理的方法很有可能成为主流图像分类技术。

虽然目前基于卷积神经网络图像分类系统虽然很多，而且在识别效果上非常不错。但其中一些基本问题仍然没有得到很好的解决，主要表现在两个方面：第一、尚未形成一套完整的通用理论。现在许多识别系统都是根据特定的数据库进行特别的设计网络的深度和层次，通过不断的摸索发现最佳的参数和优化算法，人为因素比较大，也没有较系统的理论阐述影响卷积神经网络识别效果的因素。第二、现有的方法尚存在一些缺陷。特别是对自然图像进行分类识别时，对卷积神经网络的初始状态参数以及寻优算法的选取，会对网络训练造成很大影响，选择不好会造成网络的不工作，或者有可能陷入局部极小、欠拟合、过拟合等诸多问题。

# 第五单元
## 阅读相关资料，总结循环神经网络处理语音识别应用的过程
音识别

深度学习最早应用于语音识别问题时的作用是替代GMM-HMM框架中的高斯混合模型，负责声学模型的建模，即DNN-HMM结构。在这种结构里，深层神经网络负责计算音频帧属于某一声学状态的概率或者是提取出声音的特征，其余的部分和GMM-HMM结构相同。

语音识别的困难之处在于输入语音信号序列中每个发音单元的起始位置和终止位置是未知的，即不知道输出序列和输入序列之间的对齐关系，这属于前面介绍的时序分类问题。

深度学习技术在语音识别里一个有影响力的成果是循环神经网络和CTC的结合，和卷积神经网络、自动编码器等相比，循环神经网络具有可以接受不固定长度的序列数据作为输入的优势，而且具有记忆功能。文献[14]将CTC技术用于语音识别问题。语音识别中，识别出的字符序列或者音素序列长度一定不大于输入的特征帧序列。CTC在标注符号集中加上空白符号blank，然后利用循环神经网络进行标注，再把blank符号和预测出的重复符号消除。下图是CTC的原理：
![](\86.png)

假设x为语音输入序列，l为识别出来的文字序列，  为循环神经网络的输出。可能有多个连续帧对应一个文字，有些帧可能没有任何输出，按照CTC的原理，用多对一的函数B把输出序列中重复的字符进行合并，形成一个唯一的序列：
![](\87.png)

其中l为文字序列，  是带有冗余的循环神经网络输出。映射函数B将神经网络的输出序列映射成文字序列l。分类器的输出为对输入序列最可能的标签值：

![](\88.png)
解码时采用的是前缀搜索技术。CTC在这里起到了对齐的作用，最显著的优势是实现了端到端的学习，无需人工对语音序列进行分割，这样做还带来了精度上的提升。

在实现时循环神经网络采用了双向LSTM网络，简称BLSTM。训练样本集的音频数据被切分成10毫秒的帧，其中相邻帧之间有5毫秒的重叠，使用MFCC特征作为循环神经网络的输入向量。原始音频信号被转换成一个MFCC向量序列。特征向量为26维，包括了对数能量和一阶导数值。向量的每一个分量都进行了归一化。在解码时，使用最优路径和前缀搜索解码，解码的结果就是语音识别要得到的标记序列。

文献[14]中的循环神经网络是一个浅层的网络，文献[17]提出了一种用深度双向LSTM网络和CTC框架进行语音识别的方法，这种方法主要的改进是使用了多个双向LSTM层，称为深度LSTM网络。。对于多层RNN网络，计算公式为：
![](\89.png)

对于深度双向LSTM网络原理类似，只是把隐含层的变换换成LSTM结构的公式.
![](\90.png)

假设输入的声学序列数据为x，输出音素序列为y。第一步是给定输入序列和所有可能的输出序列，用循环神经网络计算出条件概率值p(y|x)。在训练时的样本为输入序列以及对应的输出序列。训练时的损失函数为对数似然函数：
![](\91.png)

这里使用CTC来对序列z进行分类，对于一段输入的语音数据，分类的结果是一个音素序列。假设有k个音素，再加上一个空白符，是一个k+1类的分类问题。循环神经网络的最后一层为softmax层，输出k+1个概率值，在时刻t输出值为p(y|t)。

神经网络在每一个时刻确定是输出一个音素，还是不输出即输出空白符。将所有时刻的输出值合并在一起，得到了一个输入和输出序列的对齐方案。CTC对所有的对齐方式进行概率求和得到p(z|x)。

在使用CTC时，循环神经网络被设计成双向的，这样每个时刻的概率输出值为：
![](\92.png)

其中N是隐含层的数量，y是神经网络的输出向量。上式用softmax映射根据神经网络的输出向量得到每一个音素的概率值。

前面介绍的CTC框架输入是声学数据，输出是音素数据，只是一个声学模型。接下来还需要将音素序列转化成最终的文字序列作为识别结果，需要一个语言模型。在这里采用RNN transducer，一种集成了声学建模CTC和语言模型RNN的方法，后者负责将音素转化成文字，二者联合起来训练得到模型，我们称第一个网络为CTC网络，第二个网络为预测网络。

假设和为CTC网络最后一个CTC最后一个隐含层的前向和后向输出序列，p为预测网络的隐含层输出序列。在每个时刻t，u为输出网络，它包含一个线性层，接受输入和，产生输出向量lt，另外还包含一个tanh隐含层，接受输入值lt和pu，产生输出值htu，最后将htu送入类的softmax层得到概率值p(k|t,u)。整个过程的计算公式为：
![](\93.png)

RNN transducer只是给出了任何一个输出序列相对于输入序列的条件概率值，还需要解码算法得到概率最大的输出序列。在这里使用了集束搜索算法，算法给出n个最优的候选结果，选择的依据是概率值P(k|t)。

整个系统的输入数据是对音频数据进行分帧后的编码向量，具体做法是对分帧后的音频数据进行傅里叶编码，然后40个傅里叶系数，加上能量，以及它们的一阶和二阶导数构成的向量，因此特征向量为123维。整个向量进行了归一化。在这里使用了61个音素，它们被映射为39个类。实验结果证明，更深的网络具有更高的准确率，双向LSTM比单向网络也有更高的精度。


整个系统的输入为音频数据，使用20毫秒的窗口对原始音频数据分帧，然后计算对数谱，对功率进行归一化形成序列数据，送入神经网络中处理。首先是1D或者2D卷积层，然后是双向RNN，接下来全力连接的lookahead卷积层，最后是CTC分类器。整个模型也实现了端到端的训练。


在每个时刻t神经网络的输出值为  。其中  为字母表中的符号或者是空格。对于英文为：

{a,b,c,...,z,space,apotrohpe,blank}

其中space为词之间的边界。对于中文输出值为简化的汉字字符。识别时CTC模型和语言模型结合起来使用。解码时使用集束搜索算法寻找输出序列y，最大化如下函数：
![](\93.png)

第一部分为RNN的损失函数，第二部分为语言模型的损失函数，第三部分对英文为单词数，对汉语为字数,和为人工设定的权重参数。

网络的最前端是卷积层，对输入的频谱向量执行1D或者2D卷积。实验结果证明2D卷积有更好的效果。

整个网络包含多个循环层，循环层还使用了批量归一化技术，它可以作用于前一层和本层上一时刻状态值的线性加权和，也可以只作用于前一层的输入值。

在所有循环层之前，加上了lookahead卷积层，计算公式为：
![](\94.png)

其中d为前一层的神经元个数，h是前一层的输出值，W是  的权重矩阵，  为时间步长。

# 第六单元
## 以是否戴口罩检测情景应用为例，讨论目标检测应用的过程以及关键问题
无两。

不料，疫情来袭，当大家纷纷戴上口罩，摄像头背后的它便罢工待毙，再不识其人了。有专家表示，人脸戴上口罩后隐藏了大部分的面部特征，从而将准确率降低至 30％。

在疫情面前，人脸识别显得鸡肋和尴尬，最后还得人工上场。但高传染性的病毒肆意，非接触式的检测识别方案才是正解。

但 AI 算法之所以强大，在于其迭代与自我进化的空间无限。9 天，一家 AI 公司连夜奋战，终于攻破人脸识别的「口罩」难题，至少在有限的小区范围内，口罩人脸识别已经不再是问题。


### 另辟蹊径测模型

遮挡下的人脸识别是业内公认的一个难题。

一方面，戴上口罩后，首先因鼻子、嘴巴等五官信息被遮挡，人脸面部可用于辨别的信息会大幅减少；
其次脸部轮廓等可辨别信息也在物理分布上发生较大变化，因此按照传统思路训练出的人脸识别模型，精度都会出现大幅下降；
还有数据，戴口罩下的人脸数据本身就非常缺乏，疫情之下也很难短时间内快速采集数据，或者找采集公司定制用以训练。
考虑到挑战难度，小视科技 AI 研究院院长胡建国决定，首先从算法模型上突围。

此前，团队已经积累的近千个基础模型，但是否有与实际需求匹配尚是个未知数。目前，业界仍然没有一个稳定且高效的针对口罩的人脸识别算法。

进行模型评测要有戴口罩下的人脸测试数据，管理层紧急发动公司全员及亲属，用了两天时间采集一个小规模数据集。

紧接着远程调用计算集群进行模型评测，他们找到了一种有效的模型思路——采用眼部特征与整体人脸特征的融合，并结合注意力机制增强眼部特征，通过自研的轻量级网络，单独训练眼部关键点的模型，来提升模型在口罩遮挡下的人脸识别率。

胡建国解释，计算机视觉中的注意力机制与人类视觉的选择性类似，核心目标也是从众多信息中获取最相关的信息。佩戴口罩的人脸中眼睛成为了人脸识别的关键信息，基于口罩的人脸识别采用眼部关键点和注意力机制相结合的方法来增强眼部特征，眼部特征图与整体人脸特征图的多级融合，充分挖掘人脸的有效信息，提升模型在口罩遮挡情况下的表现。

![](\95.png)
人脸识别模型注意力机制效果，第一列表示原图，第二列表示热力图，第三列表示导反向传播图，第四列表示导向反向传播灰度图，图中观察到人眼的区域得到了更多的关注。

在人脸识别模型的训练过程中，胡建国团队同时加载预先训练的眼部关键点网络用于特征图的提取，并与人脸识别网络提取的特征图相结合，结合注意力机制突出眼部特征，提高识别的准确率。

在内部测试集中，该模型结果仅比普通算法模型指标低 5%。而在此之前，业界其他方案的公开口罩人脸识别仍在 80%-90% 区间。

# 第七单元
## 举例说明生成对抗网络的应用
让机器拥有想象力

神经网络对于物体的本质是怎么“想”的？

为了搞懂这个问题，谷歌大脑的研究人员使用GAN，让神经网络的“想法”呈现在你的眼前。当然，这些“想法”看起来十分迷幻。

其实这原本是一个图像分类器，而生成的迷幻图片，是故意对图像进行过度处理的副产品。现在这套系统有个单独的名字：“深梦（Deep Dream）”。
![](\96.png)

△Deep Dream模型利用普通照片生成的一张奇幻照片

![](\97.png)

△Deep Dream模型将塔楼、房屋和小鸟等对象融入图像中的效果示例

想要运行Deep Dream模型，你只需要输入一张图像，然后这个模型就开始穷尽所能，寻找被训练识别的目标。在一张完全不相关的图像中，神经网络可能会发现一些与狗、房子、水母等物体的相似之处。

就好像你看到云朵会产生联想一样，Deep Dream模型会放大它找到的相似之处。

举个例子，当你运行这个辨识网络时，它指出某张图是狗的可能性为40%，然后开始修改输入图像，使得这张图为狗的可能性增加到60%。并不断重复这个过程，直到输入图像明显地转化成一张看起来像狗的图像。

按照这种方式，通过将图像逐渐转化为的另一种物体，这个神经网络就把自己的“想象”，展现在你的面前。

谷歌提出的Deep Dream模型将传统的思路（即给定相同输入只产生一种输出的想法），改变为不断修改输入来获取最佳输出。

# 第八单元
## 简要说明采用Seq2seq模型实现机器翻译的原理数据准备和预处理

### 数据处理步骤
将文件中的双语平行语料分开成train.en和train.zh，并把语料分割成一句一句的
裁剪语料，将分割的每两个语言的句子变成一个pair
英文语料预处理：

    a. unicode -> ascii
    b. .!? 前后加上空格
    c. 删除处字母和.!?以外的其他所有字符
    d. 删除连续的空格，仅保留一个
    e. 前后加上&lt;start>,&lt;end> &lt;start&gt;,&lt;end&gt;<start>,<end>

中文语料预处理:

    a. 。！？前后加上空格
    b. 删除。！？以外的标点符号
    c. 删除连续的空格，仅保留一个
    d. 使用结巴分词
    e. 前后加上&lt;start>,&lt;end> &lt;start&gt;,&lt;end&gt;<start>,<end>

### Seq2Seq
简单的说，seq2seq就是根据一个输入序列x，来生成另一个输出序列y。它有很多应用，比如机器翻译，文档摘取，问答系统等等。在翻译中，输入序列是待翻译的文本，输出序列是翻译后的文本；在问答系统中，输入序列是提出的问题，而输出序列是答案。

为了解决seq2seq问题，有人提出了encoder-decoder模型，也就是编码-解码模型。所谓编码，就是将输入序列转化成一个固定长度的向量；解码，就是将之前生成的固定向量再转化成输出序列。具体实现的时候，编码器和解码器都不是固定的,可选的有CNN/RNN/BiRNN/GRU/LSTM等等，可以自由组合。比如说，在编码时使用GRU,解码时使用LSTM等等。

### 编码The encoder

这边选取编码和解码都是RNN的组合来说明。在RNN中，当前时间的隐藏状态
是由上一时间的状态和当前时间输入决定的。

# 第九单元
## 总结深度学习（目标检测或图像分类）项目的过程
深度学习目标检测的大致流程
![](\98.png)