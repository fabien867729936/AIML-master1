# 第五次上课总结
## 第七章 多入单出的单层神经网络
### 线性多分类的神经网络实现
如果有三个以上的分类同时存在，我们需要对每一类别分配一个神经元，这个神经元的作用是根据前端输入的各种数据，先做线性处理（Y=WX+B)，然后做一次非线性处理，计算每个样本在每个类别中的预测概率，再和标签中的类别比较，看看预测是否准确，如果准确，则奖励这个预测，给与正反馈；如果不准确，则惩罚这个预测，给与负反馈。两类反馈都反向传播到神经网络系统中去调整参数。

这个网络只有输入层和输出层，由于输入层不算在内，所以是一层网络。
#### 定义神经网络
从图示来看，似乎在三个颜色区间之间有两个比较明显的分界线，而且是直线，即线性可分的。

* 从视觉上判断是线性可分的，所以我们使用单层神经网络即可

* 输入特征是两个，X1=经度，X2=纬度

* 最后输出的是三个分类，分别是魏蜀吴，所以输出层有三个神经元

如果有三个以上的分类同时存在，我们需要对每一类别分配一个神经元，这个神经元的作用是根据前端输入的各种数据，先做线性处理（Y=WX+B)，然后做一次非线性处理，计算每个样本在每个类别中的预测概率，再和标签中的类别比较，看看预测是否准确，如果准确，则奖励这个预测，给与正反馈；如果不准确，则惩罚这个预测，给与负反馈。两类反馈都反向传播到神经网络系统中去调整参数。

这个网络只有输入层和输出层，由于输入层不算在内，所以是一层网络。
![](\19.png)
与前面的单层网络不同的是，本图最右侧的输出层还多出来一个Softmax分类函数，这是多分类任务中的标准配置，可以看作是输出层的激活函数，并不单独成为一层，与二分类中的Logistic函数一样。
![](\30.png)
#### 样本数据
使用SimpleDataReader类读取数据后，观察一下数据的基本属性：
![](\20.png)
训练数据X，140个记录，两个特征，最小值0.058，最大值9.925
标签数据Y，140个记录，一个分类值，取值范围是[1,2,3]
#### 样本标签数据
一般来说，在标记样本时，我们会用1，2，3这样的标记，来指明是哪一类。所以样本数据中是这个样子的：

Y=(y1 y2 … y140)=(32…1)

在有Softmax的多分类计算时，我们用下面这种等价的方式，俗称One-Hot，就是在一个向量中只有一个数据是1，其它都是0。

Y=(y1 y2 … y140)=(001 010 … 100)

OneHot的意思，在这一列数据中，只有一个1，其它都是0。1所在的列数就是这个样本的分类类别。

标签数据对应到每个样本数据上，列对齐，只有(1,0,0)，(0,1,0)，(0,0,1)三种组合，分别表示第一类、第二类和第三类。

在SimpleDataReader中实现ToOneHot()方法，把原始标签转变成One-Hot编码：
![](\21.png)
#### 代码实现
##### 添加分类函数
![](\22.png)
#### 前向计算
前向函数需要增加分类函数调用：
![](\23.png)
#### 反向传播
![](\24.png)
#### 计算损失函数值
损失函数不再是均方差和二分类交叉熵了，而是交叉熵函数对于多分类的形式，并且添加条件分支来判断只在网络类型为多分类时调用此损失函数。
![](\25.png)
#### 推理函数
![](\26.png)
注意在推理之前，先做了归一化，因为原始数据是在[0,10]范围的。

函数np.argmax的作用是比较output里面的几个数据的值，返回最大的那个数据的行数或者列数，0-based。比如ouput=(1.02,-3,2.2)时，会返回2，因为2.2最大，所以我们再加1，把返回值变成[1，2，3]的其中一个。

np.argmax函数的参数axis=1，是因为有4个样本参与预测，所以需要在第二维上区分开来，分别计算每个样本的argmax值。
![](\27.png)
#### 运行结果
![](\28.png)
![](\29.png)
* 经纬度相对值为(5,1)时，概率0.734最大，属于2，蜀国
* 经纬度相对值为(7,6)时，概率0.598最大，属于3，吴国
* 经纬度相对值为(5,6)时，概率0.383最大，属于1，魏国
* 经纬度相对值为(2,7)时，概率0.513最大，属于1，魏国
### 总结与心得体会
通过本节课的学习，我了解了多入多出的神经网络，学习了什么是多分类函数，以及二分类和多分类之间的区别，并实现了线性多分类的神经网络，本节课的学习使我对神经网络以及深度学习产生了深厚的兴趣，为之后算法的学习打下了深厚的基础