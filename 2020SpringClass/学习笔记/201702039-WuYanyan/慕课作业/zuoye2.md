# 作业二
举例说明深度学习的应用
## 自然语言处理
自然语言处理(natural language processing，NLP) 是让计算机能够使用人类语言，例如英语或法语。为了让简单的程序能够高效明确地解析，计算机程序通常读取和发出特殊化的语言。而自然语言通常是模糊的，并且可能不遵循形式的描述。自然语言处理中的应用如机器翻译，学习者需要读取一种人类语言的句子，并用另一种人类语言发出等同的句子。许多NLP 应用程序基于语言模型，语言模型定义了关于自然语言中的字、字符或字节序列的概率分布。与本章讨论的其他应用一样，非常通用的神经网络技术可以成功地应用于自然语言处理。然而，为了实现卓越的性能并扩展到大型应用程序，一些领域特定的策略也很重要。为了构建自然语言的有效模型，通常必须使用专门处理序列数据的技术。在很多情况下，我们将自然语言视为一系列词，而不是单个字符或字节序列。因为可能的词总数非常大，基于词的语言模型必须在极高维度和稀疏的离散空间上操作。为了使这种空间上的模型在计算和统计意义上都高效，研究者已经开发了几种策略。
### n-gram
语言模型(language model)定义了自然语言中标记序列的概率分布。根据模型的设计，标记可以是词、字符甚至是字节。标记总是离散的实体。最早成功的语言模型基于固定长度序列的标记模型，称为n-gram。一个n-gram 是一个包含n 个标记的序列。
### 神经语言模型
神经语言模型(neural language model, NLM) 是一类用来克服维数灾难的语言模型，它使用词的分布式表示对自然语言序列建模(Bengio et al., 2001b)。不同于基于类的n-gram 模型，神经语言模型在能够识别两个相似的词，并且不丧失将每个词编码为彼此不同的能力。神经语言模型共享一个词(及其上下文) 和其他类似词(和上下文之间) 的统计强度。模型为每个词学习的分布式表示，允许模型处理具有类似共同特征的词来实现这种共享。例如，如果词dog和词cat映射到具有许多属性的表示，则包含词cat的句子可以告知模型对包含词dog的句子做出预测，反之亦然。因为这样的属性很多，所以存在许多泛化的方式，可以将信息从每个训练语句传递到指数数量的语义相关语句。维数灾难需要模型泛化到指数多的句子(指数相对句子长度而言)。该模型通过将每个训练句子与指数数量的类似句子相关联克服这个问题。
### 高维输出
在许多自然语言应用中，通常希望我们的模型产生词(而不是字符) 作为输出的基本单位。对于大词汇表，由于词汇量很大，在词的选择上表示输出分布的计算成本可能非常高。在许多应用中，V 包含数十万词。表示这种分布的朴素方法是应用一个仿射变换，将隐藏表示转换到输出空间，然后应用softmax 函数。假设我们的词汇表V 大小为|V|。因为其输出维数为|V|，描述该仿射变换线性分量的权重矩阵非常大。这造成了表示该矩阵的高存储成本，以及与之相乘的高计算成本。因为softmax 要在所有|V| 输出之间归一化，所以在训练时以及测试时执行全矩阵乘法是必要的。 我们不能仅计算与正确输出的权重向量的点积。因此，输出层的高计算成本在训练期间(计算似然性及其梯度) 和测试期间(计算所有或所选词的概率) 都有出现。对于专门的损失函数，可以有效地计算梯度(Vincent et al., 2015)，但是应用于传统softmax 输出层的标准交叉熵损失时会出现许多困难。
### 结合n-gram 和神经语言模型
n-gram 模型相对神经网络的主要优点是n-gram 模型具有更高的模型容量(通过存储非常多的元组的频率)，并且处理样本只需非常少的计算量(通过查找只匹配当前上下文的几个元组)。如果我们使用哈希表或树来访问计数，那么用于n-gram 的计算量几乎与容量无关。相比之下，将神经网络的参数数目加倍通常也大致加倍计算时间。当然，避免每次计算时使用所有参数的模型是一个例外。嵌入层每次只索引单个嵌入，所以我们可以增加词汇量，而不会增加每个样本的计算时间。一些其他模型，例如平铺卷积网络，可以在减少参数共享程度的同时添加参数以保持相同的计算量。然而，基于矩阵乘法的典型神经网络层需要与参数数量成比例的计算量。因此，增加容量的一种简单方法是将两种方法结合，由神经语言模型和n-gram 语言模型组成集成(Bengio et al., 2001b, 2003)。对于任何集成，如果集成成员产生独立的错误，这种技术可以减少测试误差。集成学习领域提供了许多方法来组合集成成员的预测，包括统一加权和在验证集上选择权重。Mikolovet al. (2011a) 扩展了集成，不是仅包括两个模型，而是包括大量模型。我们也可以将神经网络与最大熵模型配对并联合训练(Mikolov et al., 2011b)。该方法可以被视为训练具有一组额外输入的神经网络，额外输入直接连接到输出并且不连接到模型的任何其他部分。额外输入是输入上下文中特定n-gram 是否存在的指示器，因此这些变量是非常高维且非常稀疏的。模型容量的增加是巨大的(架构的新部分包含高达|sV |n 个参数)，但是处理输入所需的额外计算量是很小的(因为额外输入非常稀疏)。
### 神经机器翻译
机器翻译以一种自然语言读取句子并产生等同含义的另一种语言的句子。机器翻译系统通常涉及许多组件。在高层次，一个组件通常会提出许多候选翻译。由于语言之间的差异，这些翻译中的许多翻译是不符合语法的。例如，许多语言在名词后放置形容词，因此直接翻译成英语时，它们会产生诸如“apple red”的短语。提议机制提出建议翻译的许多变体，理想情况下应包括“red apple”。翻译系统的第二个组成部分(语言模型) 评估提议的翻译，并可以评估“red apple”比“apple red”更好。最早的机器翻译神经网络探索中已经纳入了编码器和解码器的想法(Allen 1987; Chris-man 1991; Forcada and ~Neco 1997)，而翻译中神经网络的第一个大规模有竞争力的用途是通过神经语言模型升级翻译系统的语言模型(Schwenk et al., 2006; Schwenk, 2010)。之前，大多数机器翻译系统在该组件使用n-gram 模型。机器翻译中基于n-gram 的模型不仅包括传统的回退n-gram 模型(Jelinek and Mercer, 1980; Katz, 1987; Chen and Goodman, 1999)，而且包括最大熵语言模型(maximum entropy language models)(Berger et al., 1996)，其中给定上下文中常见的词，a±ne-softmax 层预测下一个词。
—