# 第九次作业
本次接着上节课所讲的卷积神经网络继续讲解卷积的反向传播原理和池化的前向计算和反向计算等等知识。
## 反向传播原理
### 计算反向传播的梯度矩阵

正向公式：

$$Z = W*A+b \tag{0}$$

其中，W是卷积核，*表示卷积（互相关）计算，A为当前层的输入项，b是偏移（未在图中画出），Z为当前层的输出项，但尚未经过激活函数处理。
### 有多个卷积核时的梯度计算

有多个卷积核也就意味着有多个输出通道。

正向公式：

$$z111 = w111 \cdot a11 + w112 \cdot a12 + w121 \cdot a21 + w122 \cdot a22$$
$$z112 = w111 \cdot a12 + w112 \cdot a13 + w121 \cdot a22 + w122 \cdot a23$$
$$z121 = w111 \cdot a21 + w112 \cdot a22 + w121 \cdot a31 + w122 \cdot a32$$
$$z122 = w111 \cdot a22 + w112 \cdot a23 + w121 \cdot a32 + w122 \cdot a33$$

$$z211 = w211 \cdot a11 + w212 \cdot a12 + w221 \cdot a21 + w222 \cdot a22$$
$$z212 = w211 \cdot a12 + w212 \cdot a13 + w221 \cdot a22 + w222 \cdot a23$$
$$z221 = w211 \cdot a21 + w212 \cdot a22 + w221 \cdot a31 + w222 \cdot a32$$
$$z222 = w211 \cdot a22 + w212 \cdot a23 + w221 \cdot a32 + w222 \cdot a33$$
### 代码实现
运行结果
## 池化的前向计算和反向传播
### 常用池化方法
池化 pooling，又称为下采样，downstream sampling or sub-sampling。

池化方法分为两种，一种是最大值池化 Max Pooling，一种是平均值池化 Mean/Average Pooling。
- 最大值池化，是取当前池化视野中所有元素的最大值，输出到下一层特征图中。
- 平均值池化，是取当前池化视野中所有元素的平均值，输出到下一层特征图中。
### Max Pooling

正向公式：

$$w = max(a,b,e,f)$$

反向公式（假设Input Layer中的最大值是b）：

$${\partial w \over \partial a} = 0$$
$${\partial w \over \partial b} = 1$$
$${\partial w \over \partial e} = 0$$
$${\partial w \over \partial f} = 0$$

###  Mean Pooling

正向公式：

$$w = \frac{1}{4}(a+b+e+f)$$

反向公式（假设Layer-1中的最大值是b）：

$${\partial w \over \partial a} = \frac{1}{4}$$
$${\partial w \over \partial b} = \frac{1}{4}$$
$${\partial w \over \partial e} = \frac{1}{4}$$
$${\partial w \over \partial f} = \frac{1}{4}$$
## 经典卷积神经网络模型
CNN从90年代的LeNet开始，沉寂了10年，也孵化了10年，直到2012年AlexNet开始再次崛起，后续的ZF Net、VGG、GoogLeNet、ResNet、DenseNet，网络越来越深，架构越来越复杂，解决反向传播时梯度消失的方法也越来越巧妙。
### LeNet-5模型
#### 概念
在CNN的应用中，文字识别系统所用的LeNet-5模型是非常经典的模型。LeNet-5模型是1998年，Yann LeCun教授提出的，它是第一个成功大规模应用在手写数字识别问题的卷积神经网络，在MNIST数据集中的正确率可以高达99.2%。
#### 工作原理
LeNet-5模型一共有7层，每层包含众多参数，也就是卷积神经网络中的参数。包含了卷积层，池化层，全连接层。为了方便，我们把卷积层称为C层，下采样层叫做下采样层。

首先，输入层输入原始图像，原始图像被处理成32×32个像素点的值。然后，后面的隐层计在卷积和子抽样之间交替进行。C1层是卷积层，包含了六个特征图。每个映射也就是28x28个神经元。卷积核可以是5x5的十字形，这28×28个神经元共享卷积核权值参数，通过卷积运算，原始信号特征增强，同时也降低了噪声，当卷积核不同时，提取到图像中的特征不同；C2层是一个池化层，池化层的功能在上文已经介绍过了，它将局部像素值平均化来实现子抽样。

池化层包含了六个特征映射，每个映射的像素值为14x14，这样的池化层非常重要，可以在一定程度上保证网络的特征被提取，同时运算量也大大降低，减少了网络结构过拟合的风险。因为卷积层与池化层是交替出现的，所以隐藏层的第三层又是一个卷积层，第二个卷积层由16个特征映射构成，每个特征映射用于加权和计算的卷积核为10x10的。第四个隐藏层，也就是第二个池化层同样包含16个特征映射，每个特征映射中所用的卷积核是5x5的。第五个隐藏层是用5x5的卷积核进行运算，包含了120个神经元，也是这个网络中卷积运算的最后一层。

之后的第六层便是全连接层，包含了84个特征图。全连接层中对输入进行点积之后加入偏置，然后经过一个激活函数传输给输出层的神经元。最后一层，也就是第七层，为了得到输出向量，设置了十个神经元来进行分类，相当于输出一个包含十个元素的一维数组，向量中的十个元素即0到9。
### AlexNet 
AlexNet网络结构在整体上类似于LeNet，都是先卷积然后在全连接。但在细节上有很大不同。AlexNet更为复杂。AlexNet有60 million个参数和65000个 神经元，五层卷积，三层全连接网络，最终的输出层是1000通道的softmax。
### 图像分类
#### 颜色分类
运行结果：
#### 几何分类
运行结果：
#### 几何和颜色分类
## 总结
本次课学习了卷积层的反向传播还有池化层的前向计算和反向传播相关知识和概念，还有他们正向计算和反向计算的公式等等。然后还学习了经典卷积神经网络模型，并在资料和老师讲解基础上逐渐了解了LeNet-5模型、AlexNet模型、Dropout等等相关概念和工作原理，让我越来越了解了卷积神经网络的各个方面的知识。在最后学习到卷积神经网络在图像分类方面的应用，逐渐实现颜色分类、几何图形分类和两个之间的分类、MNIST分类等等。本次的知识让我收获颇多，让我的知识量又上升了一层，希望以后的学习能够更上一层。