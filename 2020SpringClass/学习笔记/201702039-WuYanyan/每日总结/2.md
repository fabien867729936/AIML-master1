# 第二次作业
本次课学习了神经网络的梯度下降，损失函数。损失函数又分为均方差损失函数和交叉熵损失函数。
## 一.梯度下降
### 梯度
1. 微分
   看待看待微分的意义，可以有不同的角度，最常用的两种是：
   - 函数图像中，某点的切线的斜率
   - 函数的变化率
2. 梯度：梯度实际上就是多变量微分的一般化
![](media\4.png)

  我们可以看到，梯度就是分别对每个变量进行微分，然后用逗号分割开，梯度是用<>包括起来，说明梯度其实一个向量。梯度是微积分中一个很重要的概念，之前提到过梯度的意义。在单变量的函数中，梯度其实就是函数的微分，代表着函数在某个给定点的切线的斜率。
  在多变量函数中，梯度是一个向量，向量有方向，梯度的方向就指出了函数在给定点的上升最快的方向。梯度的方向是函数在给定点上升最快的方向，那么梯度的反方向就是函数在给定点下降最快的方向，这正是我们所需要的。所以我们只要沿着梯度的方向一直走，就能走到局部的最低点！
### 梯度下降算法
#### 1.概念
梯度下降法的计算过程就是沿梯度下降的方向求解极小值。当梯度向量为零，说明到达一个极值点，这也是梯度下降算法迭代计算的终止条件。在大多数神经网络深度学习算法中都会涉及到某种形式的优化，而目前深度学习优化中运用最多的就是随机梯度下降法。因此，梯度下降法作为一个最优化算法，常用于机器学习和人工智能当中用来递归性地逼近最小偏差模型。
#### 2.直观理解
![](media\5.png)

在山峰附件的某处，要一步一步走向山底，一个好的办法是求解当前位置的梯度，然后沿着梯度的负方向向下走一步，然后继续求解当前位置的梯度，继续沿着梯度的负方向走下去，这样一步一步直到山底，这其中用到的方向就是梯度下降法。梯度下降法也有一个问题就是如果初始点的位置选择的不合适，就容易导致找到的一个局部最优解，而不是全局最优解。
#### 3.单变量函数的梯度下降
测试代码：
![](media\6.png)
#### 4.双变量函数梯度下降
![](media\7.png)
## 二.损失函数
#### 概念
在各种材料中经常看到的中英文词汇有：误差，偏差，Error，Cost，Loss，损失，代价......意思都差不多，在本书中，使用“损失函数”和“Loss Function”这两个词汇，具体的损失函数符号用J来表示，误差值用loss表示。“损失”就是所有样本的“误差”的总和，亦即（m为样本数）：
![](media\8.png)
#### 作用
损失函数的作用，就是计算神经网络每次迭代的前向计算结果与真实值的差距，从而指导下一步的训练向正确的方向进行。

如何使用损失函数呢？具体步骤：
- 用随机值初始化前向计算公式的参数；
- 代入样本，计算输出的预测值；
- 用损失函数计算预测值和标签值（真实值）的误差；
- 根据损失函数的导数，沿梯度最小方向将误差回传，修正前向计算公式中的各个权重值；
- goto 2, 直到损失函数值达到一个满意的值就停止迭代。

### 1.均方差损失函数
#### 概念
该函数就是最直观的一个损失函数了，计算预测值和真实值之间的欧式距离。预测值和真实值越接近，两者的均方差就越小。均方差函数常用于线性回归(linear regression)，即函数拟合(function fitting)。公式如下：
![](media\9.png)
#### 工作原理
要想得到预测值a与真实值y的差距，最朴素的想法就是用Error=ai-yi
对于单个样本来说，这样做没问题，但是多个样本累计时，ai-yi有可能有正有负，误差求和时就会导致相互抵消，从而失去价值。所以有了绝对值差的想法：Error=|ai-yi|
### 2.交叉熵损失函数
#### 概念
交叉熵（Cross Entropy）是Shannon信息论中一个重要概念，主要用于度量两个概率分布间的差异性信息。在信息论中，交叉熵是表示两个概率分布 p,q的差异，其中 p表示真实分布，q表示非真实分布，那么H(p,q)
就称为交叉熵：
![](media\10.png)

交叉熵可在神经网络中作为损失函数，p表示真实标记的分布，q则为训练后的模型的预测标记分布，交叉熵损失函数可以衡量 p与 q的相似性。
#### 二分类问题交叉熵
当y=1时，即标签值是1，是个正例，加号后面的项为0：loss=-ln（a）

横坐标是预测输出，纵坐标是损失函数值。y=1意味着当前样本标签值是1，当预测输出越接近1时，损失函数值越小，训练结果越准确。当预测输出越接近0时，损失函数值越大，训练结果越糟糕。

当y=0时，即标签值是0，是个反例，加号前面的项为0：loss=-ln（1—a）
此时损失函数（测试代码）：
![](media\11.JPG)
## 总结
本次学习让我学习到了很多知识。我学到了神经网络的梯度下降还有损失函数，而且在学习损失函数的同时知道了不能使用均方差做为分类问题的损失函数。因为回归问题通常用均方差损失函数，可以保证损失函数是个凸函数，即可以得到最优解。而分类问题如果用均方差的话，损失函数的表现不是凸函数，就很难得到最优解。而交叉熵函数可以保证区间内单调。分类问题的最后一层网络，需要分类函数，Sigmoid或者Softmax，如果再接均方差函数的话，其求导结果复杂，运算量比较大。用交叉熵函数的话，可以得到比较简单的计算结果，一个简单的减法就可以得到反向误差。本次学习收获很多，当然也有不少困难，比如好多数学公式，不太容易理解，可能自己的基础没有打好。所以，在后面学习我会更加努力的。


   



