# 第五次作业
本次课学习了神经网络的线性多分类
## 线性多分类
### 多分类函数
#### 定义
假设输入值是：[3,1,-3]，如果取max操作会变成：[1,0,0]，这符合我们的分类需要。但是有两个不足：
1.分类结果是[1，0，0]，只保留的非0即1的信息，没有各元素之间相差多少的信息，可以理解是“Hard-Max”
2.max操作本身不可导，无法用在反向传播中。
所以Softmax加了个"soft"来模拟max的行为，但同时又保留了相对大小的信息。
![](media\31.png)
### 线性多分类神经网络实现
运行结果：从趋势上来看，loss值还有进一步下降的可能，以提高模型精度。
![](media\32.png)
### 线性多分类原理
#### 1.多分类过程
1. 线性计算

$$z_1 = x_1 w_{11} + x_2 w_{21} + b_1 \tag{1}$$
$$z_2 = x_1 w_{12} + x_2 w_{22} + b_2 \tag{2}$$
$$z_3 = x_1 w_{13} + x_2 w_{23} + b_3 \tag{3}$$

2. 分类计算

$$
a_1=\frac{e^{z_1}}{\sum_i e^{z_i}}=\frac{e^{z_1}}{e^{z_1}+e^{z_2}+e^{z_3}}  \tag{4}
$$
$$
a_2=\frac{e^{z_2}}{\sum_i e^{z_i}}=\frac{e^{z_2}}{e^{z_1}+e^{z_2}+e^{z_3}}  \tag{5}
$$
$$
a_3=\frac{e^{z_3}}{\sum_i e^{z_i}}=\frac{e^{z_3}}{e^{z_1}+e^{z_2}+e^{z_3}}  \tag{6}
$$

3. 损失函数计算

单样本时，$n$表示类别数，$j$表示类别序号：

$$
loss(w,b)=-(y_1 \ln a_1 + y_2 \ln a_2 + y_3 \ln a_3)
$$
$$
=-\sum_{j=1}^{n} y_j \ln a_j  \tag{7}
$$

批量样本时，$m$表示样本数，$i$表示样本序号：

$$J(w,b) =- \sum_{i=1}^m (y_{i1} \ln a_{i1} + y_{i2} \ln a_{i2} + y_{i3} \ln a_{i3})$$
$$ =- \sum_{i=1}^m \sum_{j=1}^n y_{ij} \ln a_{ij} \tag{8}$$
### 多分类结果可视化
理想中一对多方式：
![](media\33.png)
现实中代码实现：
![](media\34.png)
## 总结
本次课学习了神经网络的多线性分类，在此之前我已经了解到线性二分类的概念和基本原理，学习多线性分类之后，我收获到很多知识，也认识到之间没有学到的知识.在学习的过程中，我学到了神经网络线性多分类，首先了解了分类函数和各个运算公式，并在最后用神经网络实现，然后又学到了多分类结果的可视化，也知道了理想中一对多的方式还有现实中的情况。本次的学习让我受益匪浅，虽然还有一些理论知识不太懂，但在老师讲解之下慢慢了解了。总之来说，收获很多。