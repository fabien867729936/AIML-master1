 # 第六次作业
本次学习了深度神经网络，并学会怎样搭建深度神经网络框架等等，还学到了网络优化等知识
## 一.深度神经网络
#### 简介
DNN是指深度神经网络。与RNN循环神经网络、CNN卷积神经网络的区别就是DNN特指全连接的神经元结构，并不包含卷积单元或是时间上的关联。
#### 基本结构
按不同层的位置划分，DNN内部的神经网络层可以分为：输入层，隐藏层和输出层，一般第一层是输入层，最后一层是输出层，而中间的层数都是隐藏层。层与层之间是全连接的，即第i层的任意一个神经元一定与第i+1层的任意一个神经元相连。
![](media\35.png)
### 回归任务功能测试
运行结果：
![](media\2.jpg)
![](media\3.jpg)
#### 真实案例——房价预测
运行结果：
![](media\4.jpg)

![](media\5.jpg)
### 反向传播四大公式推导
DNN的反向传播算法（Back Propagation，BP）通过对损失函数用梯度下降法进行迭代优化求极小值，找到合适的隐藏层和输出层对应的线性系数矩阵W,偏倚向量b,让所有的训练样本输入计算出的输出尽可能的等于或接近样本标签。

著名的反向传播四大公式是：

  $$\delta^{L} = \nabla_{a}C \odot \sigma_{'}(Z^L) \tag{1}$$
  $$\delta^{l} = ((W^{l + 1})^T\delta^{l+1})\odot\sigma_{'}(Z^l) \tag{2}$$
  $$\frac{\partial{C}}{\partial{b_j^l}} = \delta_j^l \tag{3}$$
  $$\frac{\partial{C}}{\partial{w_{jk}^{l}}} = a_k^{l-1}\delta_j^l \tag{4}$$
### 二分类任务
#### 数据处理

数据分析和数据处理实际上是一门独立的课，超出类本书的范围，所以我们只做一些简单的数据处理，以便神经网络可以用之训练。

对于连续值，我们可以直接使用原始数据。对于枚举型，我们需要把它们转成连续值。以性别举例，Female=0，Male=1即可。对于其它枚举型，都可以用从0开始的整数编码。
#### 测试——双弧形非线性分类
![](media\6.jpg)
![](media\7.jpg)
#### 实例——居民收入
代码实现：
![](media\8.jpg)
### 多分类任务
#### 分类函数——softmax
公式：![](media\36.png)
#### 测试——“铜钱孔分类问题”
模型一：

![](media\9.jpg)
![](media\10.jpg)
模型二：
![](media\12.jpg)
![](media\13.jpg)
#### 真实案例——MNIST手写数字识别
![](media\43.png)

## 二.网络优化
原因：
- 参数多
- 数据量大
- 梯度消失
- 损失函数坡度平缓

措施：
- 权重矩阵初始化
- 批量归一化
- 梯度下降优化算法
- 自适应学习率算法
### 权重矩阵初始化
模型权重的初始化对于网络的训练很重要, 不好的初始化参数会导致梯度传播问题, 降低训练速度; 而好的初始化参数, 能够加速收敛, 并且更可能找到较优解. 如果权重一开始很小，信号到达最后也会很小；如果权重一开始很大，信号到达最后也会很大。不合适的权重初始化会使得隐藏层的输入的方差过大,从而在经过sigmoid这种非线性层时离中心较远(导数接近0),因此过早地出现梯度消失.如使用均值0,标准差为1的正态分布初始化在隐藏层的方差仍会很大. 不初始化为0的原因是若初始化为0,所有的神经元节点开始做的都是同样的计算,最终同层的每个神经元得到相同的参数.

方式：
1. 初始化为小的随机数,如均值为0,方差为0.01的高斯分布:
W=0.01 * np.random.randn(D,H) 然而只适用于小型网络,对于深层次网络,权重小导致反向传播计算中梯度也小,梯度"信号"被削弱.
2. 上面1中的分布的方差随着输入数量的增大而增大,可以通过正则化方差来提高权重收敛速率,初始权重的方式为正态分布: w = np.random.randn(n) / sqrt(n). 这会使得中间结果z=∑iwixi+bz=∑iwixi+b的方差较小,神经元不会饱和,学习速度不会减慢.
3. Xavier (均匀分布)
4. MSRA
   
#### 代码运行结果：
![](media\44.png)
![](media\51.png)
![](media\52.png)
![](media\53.png)
### 梯度下降优化算法
神经网络最基本的优化算法是反向传播算法加上梯度下降法。通过梯度下降法，使得网络参数不断收敛到全局（或者局部）最小值，但是由于神经网络层数太多，需要通过反向传播算法，把误差一层一层地从输出传播到输入，逐层地更新网络参数。由于梯度方向是函数值变大的最快的方向，因此负梯度方向则是函数值变小的最快的方向。沿着负梯度方向一步一步迭代，便能快速地收敛到函数最小值。这就是梯度下降法的基本思想。

梯度下降法的迭代公式如下：
![](media\37.png)

其中w是待训练的网络参数，α是学习率，是一个常数，dw是梯度。以上是梯度下降法的最基本形式，在此基础上，研究人员提出了其他多种变种，使得梯度下降法收敛更加迅速和稳定，其中最优秀的代表便是Mommentum, RMSprop和Adam等。
### 梯度加速算法NAG
![](media\54.png)
![](media\55.png)
![](media\56.png)
![](media\57.png)
![](media\58.png)
![](media\59.png)
![](media\60.png)
![](media\61.png)



## 总结
本次课学习了很多关于神经网络的知识，对神经网络的知识更加巩固，并且也新认识了深度神经网络的相关知识，也学会了如何搭建深度神经网络框架。并在基本概念基础上，并用代码测试运行成功了解回归任务的功能测试，还有在此基础上弄懂了真实案例。然后学会了二分类和多分类的基本知识，通过代码解析和老师讲解，明白了其中的道理。后面也学到了网络优化。优化有很多方式，在本次学习中先讲了权重矩阵初始化和梯度下降优化算法。总之，本次的学习让我更加了解神经网络，也让我明白很多的理论知识，收益颇多。

