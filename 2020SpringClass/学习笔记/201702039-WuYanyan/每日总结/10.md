# 第十次作业
本次课程又学习到一个新的神经网络知识：循环神经网络。还有NLP模型
## 循环神经网络
#### 概念
循环神经网络（Recurrent Neural Network, RNN）是一类以序列（sequence）数据为输入，在序列的演进方向进行递归（recursion）且所有节点（循环单元）按链式连接的递归神经网络（recursive neural network）。循环神经网络具有记忆性、参数共享并且图灵完备（Turing completeness），因此在对序列的非线性特征进行学习时具有一定优势。循环神经网络在自然语言处理（Natural Language Processing, NLP），例如语音识别、语言建模、机器翻译等领域有应用，也被用于各类时间序列预报
#### 模型结构
![](media\45.png)
如图1所示, 我们可以看到RNN层级结构较之于CNN来说比较简单, 它主要有输入层,Hidden Layer, 输出层组成.并且会发现在Hidden Layer 有一个箭头表示数据的循环更新, 这个就是实现时间记忆功能的方法.
### 前向传播
RNN的前向传播其实和其他人工神经网络的前向传播是一样的都是乘法、加法操作以及集合操作的集合。
从循环神经网络的原理，我们可以知道对于一个有序序列，假设有t个时刻，那么他会经过t次隐藏层的计算,以其中任意一次为案例进行说明。当前隐藏层状态h(t)由当前的输入x(t)和上一层的输出h(t-1)决定。
![](media\47.png)
其中，代表中激活函数，其实在RNN中就两种，一种是tanh函数用于将值转化到0-1之间，一种是softmax函数，用于输出概率值，在隐藏层中选择的是tanh函数。
### 后向传播
后向传播算法的核心思想就是采用梯度下降算法进行一步步的迭代，直到得到最终需要的参数U、V、W、b、c，反向传播算法也被称为BPTT(back-propagation through time)。
  首先来定义RNN的损失函数，最常使用的是交叉熵损失函数。对于某一时刻t交叉熵损失函数可以表示为：

![](media\48.png)
### 长短时记忆网络LSTM
#### 概述
Longshort term memory，循环神经网络的变形结构，在普通RNN基础上，在隐藏层各神经单元中增加记忆单元，从而使时间序列上的记忆信息可控，每次在隐藏层各单元间传递时通过几个可控门（遗忘门、输入门、候选门、输出门），可以控制之前信息和当前信息的记忆和遗忘程度，从而使RNN网络具备了长期记忆功能，对于RNN的实际应用，有巨大作用。
#### 前向计算
门实际上就是一层全连接层，它的输入是一个向量，输出是一个0到1之间的实数向量。假设W是门的权重向量，是偏置项，那么门可以表示为：
![](media\49.png)
门的使用，就是用门的输出向量按元素乘以我们需要控制的那个向量。因为门的输出是0到1之间的实数向量，那么，当门输出为0时，任何向量与之相乘都会得到0向量，这就相当于啥都不能通过；输出为1时，任何向量与之相乘都不会有任何改变，这就相当于啥都可以通过。因为（也就是sigmoid函数）的值域是(0,1)，所以门的状态都是半开半闭的。

结果图：
![](media\50.png)
### 反向传播
是通过梯度下降法迭代更新我们所有的参数，关键点在于计算所有参数基于损失函数的偏导数。和RNN一样，LSTM也采用基于时间的反向传播算法（Backpropagation Through Time, BPTT）。LSTM的误差项也是沿两个方向传播：
1. 沿时间的反向传播
2. 向上一层网络的反向传播
### LSTM的实现
## NLP模型
## 总结
本次学习了循环神经网络，在学习这个之前，我们已经学到了深度学习的卷积神经网络和深度神经网络的相关知识和计算原理，对循环神经网络的理解更深一层，没有像之前学习那么吃力。当然，在学习的过程中，我认识到神经网络的魅力，也学习到循环神经网络的基本概念和结构，并在此基础上学习到了它的前向传播和反向传播的基本原理。然后，还学习到了LSTM模型，并且也学到了他的前向计算和反向计算等等知识。这个课程让我在这段时间里学习到了很多的知识，也知道了神经网络的很多理论知识，本次的学习让我收获很多，希望自己在以后的日子里可以灵活运用自己学到的知识。


