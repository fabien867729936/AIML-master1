# 第三次作业
本次课程学习了线性回归和多入单出单层—多变量线性回归。
## 线性回归
### 单变量线性回归
#### 一元线性回归模型
回归分析是一种数学模型。当因变量和自变量为线性关系时，它是一种特殊的线性模型。最简单的情形是一元线性回归，由大体上有线性关系的一个自变量和一个因变量组成，模型是：  Y=a+bX+ε

X是自变量，Y是因变量，ε是随机误差，a和b是参数，在线性回归模型中，a和b是我们要通过算法学习出来的。
#### 线性与非线性区别
![](media\12.png)
左侧为线性模型，可以看到直线穿过了一组三角形所形成的区域的中心线，并不要求这条直线穿过每一个三角形。右侧为非线性模型，一条曲线穿过了一组矩形所形成的区域的中心线。
### 解决左侧线性回归问题
#### 1.最小二乘法
##### 概念
最小二乘法，也叫做最小平方法（Least Square），它通过最小化误差的平方和寻找数据的最佳函数匹配。利用最小二乘法可以简便地求得未知的数据，并使得这些求得的数据与实际数据之间误差的平方和为最小。最小二乘法还可用于曲线拟合。其他一些优化问题也可通过最小化能量或最小二乘法来表达。
##### 数学原理
线性回归试图学得：
![](media\13.png)

使得：
![](media\14.png)
其中xi是样本特征值，yi是样本标签值，zi是模型预测值。
#### 2.梯度下降法
##### 数学原理
在下面的公式中，我们规定x是样本特征值（单特征），y是样本标签值，z是预测值，下标i表示其中一个样本。

- 预设函数为线性函数：![](media\15.png)
- 损失函数为均方差函数：![](media\16.png)
  
与最小二乘法比较可以看到，梯度下降法和最小二乘法的模型及损失函数是相同的，都是一个线性模型加均方差损失函数，模型用于拟合，损失函数用于评估效果。
区别在于，最小二乘法从损失函数求导，直接求得数学解析解，而梯度下降以及后面的神经网络，都是利用导数传递误差，再通过迭代方式一步一步（用近似解）逼近真实解。
#### 3.神经网络法
##### 定义神经网络结构
首次尝试建立神经网络，先用一个最简单的单层单点神经元，如图所示
![](media\17.png)
下面，我们用这个最简单的线性回归的例子，来说明神经网络中最重要的反向传播和梯度下降的概念、过程以及代码实现。

- 输入层

此神经元在输入层只接受一个输入特征，经过参数w,b的计算后，直接输出结果。这样一个简单的“网络”，只能解决简单的一元线性回归问题，而且由于是线性的，我们不需要定义激活函数，这就大大简化了程序，而且便于大家循序渐进地理解各种知识点。
严格来说输入层在神经网络中并不能称为一个层。

- 权重w/b:因为是一元线性问题，所以w/b都是一个标量。
- 输出层:输出层1个神经元，线性预测公式是：
  ![](media\18.png)
### 梯度下降的三种形式
#### 单样本随机梯度下降SDG(Stochastic Grident Descent)
样本访问示意图：
![](media\19.png)
  特点：

 - 训练样本：每次使用一个样本数据进行一次训练，更新一次梯度，重复以上过程。
- 优点：训练开始时损失值下降很快，随机性大，找到最优解的可能性大。
- 缺点：受单个样本的影响最大，损失函数值波动大，到后期徘徊不前，在最优解附近震荡。不能并行计算。

单批量样本方式训练情况：
![](media\20.png)
#### 小批量样本梯度下降Mini-Batch Gradient Descent
样本访问示意图：
![](media\21.png)
特点:
- 训练样本：选择一小部分样本进行训练，更新一次梯度，然后再选取另外一小部分样本进行训练，再更新一次梯度。
- 优点：不受单样本噪声影响，训练速度较快。
- 缺点：batch size的数值选择很关键，会影响训练结果。
#### 全批量样本梯度下降Full Batch Gradient Descent
样本访问示意图：
![](media\22.png)
特点：
- 训练样本：每次使用全部数据集进行一次训练，更新一次梯度，重复以上过程。
- 优点：受单个样本的影响最小，一次计算全体样本速度快，损失函数值没有波动，到达最优点平稳。方便并行计算。
- 缺点：数据量较大时不能实现（内存限制），训练过程变慢。初始值不同，可能导致获得局部最优解，并非全局最优解。
## 二.多变量线性回归
### 正规方程法
到目前为止，我们都在使用梯度下降算法，但是对于某些线性回归问题，正规方程方法是更好的解决方案。如：
![](media\23.png)
 正规方程解出向量： ![](media\24.png) 
### 神经网络法
与单特征值的线性回归问题类似，多变量（多特征值）的线性回归可以被看做是一种高维空间的线性拟合。以具有两个特征的情况为例，这种线性拟合不再是用直线去拟合点，而是用平面去拟合点。
### 样本特征数据归一化
深度学习中的数据分布偏移：深度神经网络涉及到很多层的叠加，而每一层的参数更新会导致上层的输入数据分布发生变化，通过层层叠加，高层的输入分布变化会非常剧烈。虽然神经网络的各层的输入信号分布不同，但最终“指向“的样本标记是不变的，即边缘概率不同而条件概率一致。 

为了降低分布变化的影响，可使用归一化策略Normalization，把数据分布映射到一个确定的区间。 神经网络中，常用的归一化策略有BN(Batch Normalization)， WN(Weight Normalization)， LN(Layer Normalization)， IN(Instance Normalization).
## 总结
本次的学习让我对神经网络有了进一步的了解，知识也得到了很大的升华。经过慕课学习和老师讲解，我知道了神经网络线性回归中的很多算法，比如最小二乘法、梯度下降法、神经网络法等等。这样在解决回归问题可以用这些方法解决。当然还学习了多变量线性回归，有正规方程法和神经网络法。还学习了样本特征数据归一化，并知道了归一化的后遗症和正确的推理方法。这次学习知识很多，但弄懂了就不难。在之后的学习，我会更加努力，并充实自己。

