# 第七次作业
本次课程学习了网络优化的自适应学习率算法和批量归一化，然后还学习了神经网络的正规化
## 自适应学习率算法
### 1.AdaGrad
AdaGrad算法，独立地使用所有模型参数的学习率，缩放每个参数反比于其所有梯度历史平方值总和的平方根。具有损失最大偏导的参数相应地有一个快速下降的学习率，而具有小偏导的参数在学习率上有相对较小的下降。净效果是在参数空间中更为平缓的斜率方向会取得更大的进步。在凸优化背景中，AdaGrad算法具有一些令人满意的理论性质。然而，经验上已经发现，对于深度神经网络模型而言，从训练开始时积累梯度平方会导致有效学习率过早和过量的减少。AdaGrad在某些深度学习模型上效果不错，但不是全部。
### 2、RMSProp
RMSProp算法修改AdaGrad以在非凸设定下效果更好，改变梯度积累为指数加权的移动平均。AdaGrad旨在应用于凸问题时快速收敛。当应用于非凸函数训练神经网络时，学习轨迹可能穿过了很多不同的结构，最终到达一个局部是凸碗的区域。AdaGrad根据平方梯度的整个历史收缩学习率，可能使得学习率在达到这样的凸结构前就变得太小收敛。它就像一个初始化与该碗状结构的AdaGrad算法实例。
## 批量归一化
既然可以把原始训练样本做归一化，那么如果在深度神经网络的每一层，都可以有类似的手段，也就是说把层之间传递的数据移到0点附近，那么训练效果就应该会很理想。这就是批归一化BN的想法的来源。
### 批量归一化原理
验证成功
![](media\64.png)
![](media\62.png)
![](media\63.png)
![](media\65.png)
![](media\66.png)
![](media\67.png)
### 归一化实现
![](media\68.png)
## 正则化
在训练模型时，一般会选择正则化方法来防止过拟合，正则化项的系数可能会选择的过高或过低。
![](media\1.JPG)
- 过大时，求解的结果将使得 ，此时是一条水平直线如上图左所示，模型处于欠拟合情况。
- 过小()时，正则化项对模型的修正效果很小近似于无，如上图右所示，模型处于欠拟合情况。
- 为了取得上图中所示比较好的拟合情况，需要选择一个合适的值，即还是需要考虑和刚才选择多形式模型的次数相类似的问题。

### L1正则化和L2正则化
#### L1正则化
L1正则化可以产生稀疏值矩阵，即产生一个稀疏模型，可以用于特征选择和解决过拟合。稀疏矩阵是矩阵中很多元素为0，只有少数元素是非零值的矩阵，稀疏矩阵的好处就是能够帮助模型找到重要特征，而去掉无用特征或影响甚小的特征。
![](media\69.png)
![](media\70.png)
#### L2正则化
![](media\71.png)
![](media\72.png)
#### DNN之L1和L2正则化
和普通机器学习算法一样，DNN也会遇到过拟合的问题，因此需要考虑泛化。结合我们上面讲到的L1和L2正则化，这里对深度神经网络中的正则化做个总结，其中L1正则化和L2正则化原理类似，这里主要介绍L2正则化方法。通过深度神经网络之前向传播算法的学习，我们知道前向传播过程中损失函数为：
![](media\38.png)

加入L2正则化后，损失函数如下所示。其中λ是正则化参数，实际使用时需要我们进行调参。

![](media\39.png)

如果使用上式的损失函数，进行反向传播算法时，流程和没有正则化时的反向传播算法相同。区别在于进行梯度下降时，W更新公式会进行改变。在深度神经网络之反向传播算法中，W的梯度下降更新公式为:

![](media\40.png)

加入L2正则化后，W迭代更新公式如下所示

![](media\41.png)
## 总结
本次学习让我对神经网络有了更深层的认知，在学习的过程中，通过老师的讲解和网上视频资料，逐渐的了解到神经网络的正则化和网络优化，也知道了神经网络的魅力。神经网络的优化不止老师所讲的内容，还有很多方法，在网上搜索中可以看到各种各样的方法，让我受益匪浅。不过，这些算法知识看起来很难理解，运用到很多数学公式，各种数学理论知识，单看字面和公式很难弄懂，所以得结合老师给的代码实现，才知道各个理论知识的变化和功能。本次的学习让我学习了很多，也对神经网络更加感兴趣，所以希望自己在以后的学习中，学到一门技术。