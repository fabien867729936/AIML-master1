# 第四次作业
本次课学习了神经网络的线性分类
## 一.线性分类
### 线性二分类
#### 逻辑·回归·模型
回归问题可以分为两类：线性回归和逻辑回归。逻辑回归的英文是Logistic Regression，逻辑回归是用来计算“事件=Success”和“事件=Failure”的概率。当因变量的类型属于二元（1 / 0，真/假，是/否）变量时，我们就应该使用逻辑回归。回忆线性回归，使用一条直线拟合样本数据，而逻辑回归是“拟合”0或1两个数值，而不是具体的连续数值，所以它叫广义线性模型。逻辑回归又称logistic回归分析，常用于数据挖掘，疾病自动诊断，经济预测等领域。

直观理解线性二分类与非线性二分类的区别：
![](media\25.png)
### 二分类函数
#### 概念
对率函数Logistic Function，即可以做为激活函数使用，又可以当作二分类函数使用。而在很多不太正规的文字材料中，把这两个概念混用了，比如下面这个说法：“我们在最后使用Sigmoid激活函数来做二分类”，这是不恰当的。我们会根据不同的任务区分激活函数和分类函数这两个概念，在二分类任务中，作为分类函数，叫做Logistic函数，而在作为激活函数时，叫做Sigmoid函数。

- 公式：![](media\26.png)
- 导数：![](media\27.png)

- 函数图像：![](media\28.png)
- 使用方法
  此函数实际上是一个概率计算，它把(−∞,∞)之间的任何数字都压缩到(0,1)之间，返回一个概率值，这个概率值接近1时，认为是正例，否则认为是负例。
  
  训练时，一个样本x在经过神经网络的最后一层的矩阵运算结果作为输入z，经过Logistic计算后，输出一个(0,1)之间的预测值。我们假设这个样本的标签值为0属于负类，如果其预测值越接近0，就越接近标签值，那么误差越小，反向传播的力度就越小。
  
  推理时，我们预先设定一个阈值比如0.5，则当推理结果大于0.5时，认为是正类；小于0.5时认为是负类；等于0.5时，根据情况自己定义。阈值也不一定就是0.5，也可以是0.65等等，阈值越大，准确率越高，召回率越低；阈值越小则相反，准确度越低，召回率越高。
#### 用神经网络实现线性二分类
代码实现：![](media\29.png)
分类的方式是，可以指定当A > 0.5时是正例，A <= 0.5时就是反例。有时候正例反例的比例不一样或者有特殊要求时，也可以用不是0.5的数来当阈值。
### 二分类的工作原理
#### 二分类的代数原理
代数方式：通过一个分类函数计算所有样本点在经过线性变换后的概率值，使得正例样本的概率大于0.5，而负例样本的概率小于0.5。
1. 正向计算

$$
z = x_1 w_1+ x_2 w_2 + b  \tag{1}
$$

2. 分类计算

$$
a={1 \over 1 + e^{-z}} \tag{2}
$$

3. 损失函数计算
$$
loss = -[y \ln (a)+(1-y) \ln (1-a)] \tag{3}
$$
#### 几何原理
几何方式：让所有正例样本处于直线的上方，所有负例样本处于直线的下方，尽可能处于双方的中间。
### 二分类结果可视化
代码实现：
![](media\30.png)

把max_epoch从100改成了10000，再跑一次。
![](media\73.png)
## 总结
本次在上次课的基础上又学习到了神经网络的新知识，关于线性分类和线性多分类。线性分类中学习到了二分类的函数的知识，二分类中也学习到了激活函数和分类函数。并在此基础上学习到它的工作原理和数学原理。还有他的可视化，经过多次的代码实现，得到完美的二分类。在学习中，我遇到了很多困难，比如概念太深奥，不能理解，还有代码的功能等等，但是经过老师的讲解还有自己在课外搜索资料，逐渐知道了其中的道理。本次学习收获很多，希望自己能够学到更多关于深度学习这方面的知识，并且能够将自己学到的知识灵活运用。