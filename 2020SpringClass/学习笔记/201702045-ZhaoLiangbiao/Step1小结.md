# Step1 基本概念

## 一、神经网络基本工作原理简介

### 1.1神经元细胞的数学模型

**神经网络由基本的神经元组成：**
### （1）输入 input：
    (x1,x2,x3) 是外界输入信号，一般是一个训练数据样本的多个属性； 
### （2）权重 weights：
    (w1,w2,w3) 是每个输入信号的权重值。 
### （3）偏移 bias：
    从生物学上解释，在脑神经细胞中，一定是输入信号的电平/电流大于某个临界值时，神经元细胞才会处于兴奋状态，这个b实际就是那个临界值。 
### （4）求和计算 sum：
    $$Z = w1 \cdot x1 + w2 \cdot x2 + w3 \cdot x3 + b = \sum_{i=1}^m(w_i \cdot x_i) + b$$ 
### （5）激活函数 activation：
    求和之后，神经细胞已经处于兴奋状态了，已经决定要向下一个神经元传递信号了，但是要传递多强烈的信号，要由激活函数来确定。
![数学模型](images\1.png)

### 特性：

    （1）一个神经元可以有多个输入
    （2）一个神经元只能有一个输出，这个输出可以同时输入给多个神经元**
    （3）一个神经元的w的数量和输入的数量一致
    （4）一个神经元只有一个b
    （5）w和b有人为的初始值，在训练过程中被不断修改
    （6）激活函数不是必须有的，亦即A可以等于Z
    （7）一层神经网络中的所有神经元的激活函数必须一致

### 1.2训练过程

**单层神经网络模型**

这是一个单层的神经网络，有m个输入 (这里m=3)，有n个输出 (这里n=2)。在单个神经元里，b是个值。但是在神经网络中，我们把b的值永远设置为1，而用b到每个神经元的权值来表示实际的偏移值，亦即(b1,b2)，这样便于矩阵运算。也有些作者把b写成x0，其实是同一个意思，只不过x0用于等于1。

- $(x1,x2,x3)$是一个样本数据的三个特征值
- $(w11,w12,w13)$是$(x1,x2,x3)$到$n1$的权重
- $(w21,w22,w23)$是$(x1,x2,x3)$到$n2$的权重
- $b1$是$n1$的偏移
- $b2$是$n2$的偏移

**训练步骤**
假设我们有以下训练数据样本：
|Id|x1|x2|x3|Y|
|---|---|---|---|---|
|1|0.5|1.4|2.7|3|
|2|0.4|1.3|2.5|5|
|3|0.1|1.5|2.3|9|
|4|0.5|1.7|2.9|1|
其中，x1，x2，x3是每一个样本数据的三个特征值，Y是样本的真实结果值：

  1. 随机初始化权重矩阵，可以根据高斯分布或者正态分布等来初始化。这一步可以叫做"蒙"，但不是瞎蒙。
  2. 拿一个或一批数据作为输入，带入权重矩阵中计算，再通过激活函数传入下一层，最终得到预测值。在本例中，我们先用Id-1的数据输入到矩阵中，得到一个A值，假设A=5
  3. 拿到Id-1样本的真实值Y=3
  4. 计算损失，假设用均方差函数 $Loss = (A-Y)^2=(5-3)^2=4$
  5. 根据一些神奇的数学公式（反向微分），把Loss=4这个值用大喇叭喊话，告诉在前面计算的步骤中，影响A=5这个值的每一个权重矩阵，然后对这些权重矩阵中的值做一个微小的修改（当然是向着好的方向修改，这一点可以用数学家的名誉来保证）
  6. 用Id-2样本作为输入再次训练（goto 2）
  7. 这样不断地迭代下去，直到以下一个或几个条件满足就停止训练：损失函数值非常小；迭代了指定的次数；

### 1.3激活函数作用
    如果我们不运用激活函数的话，则输出信号将仅仅是一个简单的线性函数。线性函数一个一级多项式。现如今，线性方程是很容易解决的，但是它们的复杂性有限，并且从数据中学习复杂函数映射的能力更小。一个没有激活函数的神经网络将只不过是一个线性回归模型（Linear regression Model）罢了，它功率有限，并且大多数情况下执行得并不好。

## 2.基本函数导数公式

**基本函数及其导数：**
|序号|函数|导数|备注|
|---|---|---|---|
|1|$y=c$|$y'=0$|
|2|$y=x^a$|$y'=ax^{a-1}$|
|3|$y=log_ax$|$y'=\frac{1}{x}log_ae=\frac{1}{xlna}$|
|4|$y=lnx$|$y'=\frac{1}{x}$|
|5|$y=a^x$|$y'=a^xlna$|
|6|$y=e^x$|$y'=e^x$|
|7|$y=e^{-x}$|$y'=-e^{-x}$|
|8|$y=sin(x)$|$y'=cos(x)$|正弦函数|
|9|$y=cos(x)$|$y'=-sin(x)$|余弦函数|
|10|$y=tg(x)$|$y'=sec^2(x)=\frac{1}{cos^2x}$| 正切函数 |
|11|$y=ctg(x)$|$y'=-csc^2(x)$| 余切函数 |
|12|$y=arcsin(x)$|$y'=\frac{1}{\sqrt{1-x^2}}$| 反正弦函数 |
|13|$y=arccos(x)$|$y'=-\frac{1}{\sqrt{1-x^2}}$| 反余弦函数 |
|14|$y=arctan(x)$|$y'=\frac{1}{1+x^2}$| 反正切函数 |
|15|$y=arcctg(x)$|$y'=-\frac{1}{1+x^2}$| 反余切函数 |
|16|$y=sinh(x)=(e^x-e^{-x})/2$|$y'=cosh(x)$|双曲正弦函数 |
|17|$y=cosh(x)=(e^x+e^{-x})/2$|$y'=sinh(x)$|双曲余弦函数 |
|18|$y=tanh(x)=(e^x-e^{-x})/(e^x+e^{-x})$|$y'=sech^2(x)=1-tanh^2(x)$|双曲正切函数|
|19|$y=coth(x)=(e^x+e^{-x})/(e^x-e^{-x})$|$y'=-csch^2(x)$|双曲余切函数|
|20|$y=sech(x)=2/(e^x+e^{-x})$|$y'=-sech(x)*tanh(x)$|双曲正割函数|
|21|$y=csch(x)=2/(e^x-e^{-x})$|$y'=-csch(x)*coth(x)$| 双曲余割函数
**导数四则运算：**
$$[u(x) + v(x)]' = u'(x) + v'(x) \tag{30}$$
$$[u(x) - v(x)]' = u'(x) - v'(x) \tag{31}$$
$$[u(x)*v(x)]' = u'(x)*v(x) + v'(x)*u(x) \tag{32}$$
$$[\frac{u(x)}{v(x)}]'=\frac{u'(x)v(x)-v'(x)u(x)}{v^2(x)} \tag{33}$$

# 二.反向传播与梯度下降

**反向传播与梯度下降的基本工作原理：**

  1. 初始化
  2. 正向计算
  3. 损失函数为我们提供了计算损失的方法
  4. 梯度下降是在损失函数基础上向着损失最小的点靠近而指引了网络权重调整的方向
  5. 反向传播把损失值反向传给神经网络的每一层，让每一层都根据损失值反向调整权重
  6. goto 2，直到精度足够好（比如损失函数值小于0.001）

## 1.非线性反向传播
误差一次性地传递给了初始值w和b，即，只经过一步，直接修改w和b的值，就能做到误差校正。因为从它的计算图看，无论中间计算过程有多么复杂，它都是线性的，所以可以一次传到底。缺点是这种线性的组合最多只能解决线性问题，不能解决更复杂的问题。这个我们在神经网络基本原理中已经阐述过了，需要有激活函数连接两个线性单元。

## 2.梯度下降三要素
  1. 当前点
  2. 方向
  3. 步长

“梯度下降”包含了两层含义：

   1. 梯度：函数当前位置的最快上升点
   2. 下降：与导数相反的方向，用数学语言描述就是那个减号

# 三.损失函数
## 1、概念

在各种材料中经常看到的中英文词汇有：误差，偏差，Error，Cost，Loss，损失，代价......意思都差不多，在本系列文章中，使用损失函数和Loss Function这两个词汇，具体的损失函数符号用J()来表示，误差值用loss表示。

**损失**就是所有样本的**误差**的总和，亦即：
$$损失 = \sum^m_{i=1}误差_i$$
$$J = \sum_{i=1}^m loss$$

## 2.损失函数的作用
损失函数的作用，就是计算神经网络每次迭代的前向计算结果与真实值的差距，从而指导下一步的训练向正确的方向进行。
## 3.损失函数图像理
### 3.1用二维函数图像理解单变量对损失函数的影响

![](images/2.png)

上图中，纵坐标是损失函数值，横坐标是变量。不断地改变变量的值，会造成损失函数值的上升或下降。而梯度下降算法会让我们沿着损失函数值下降的方向前进。

  1. 假设我们的初始位置在A点，X=x0，Loss值（纵坐标）较大，回传给网络做训练
  2. 经过一次迭代后，我们移动到了B点，X=x1，Loss值也相应减小，再次回传重新训练
  3. 以此节奏不断向损失函数的最低点靠近，经历了x2 x3 x4 x5
  4. 直到损失值达到可接受的程度，就停止训练

### 3.2用等高线图理解双变量对损失函数影响

![](images/3.png)

上图中，横坐标是一个变量w，纵坐标是另一个变量b。两个变量的组合形成的损失函数值，在图中对应处于等高线上的唯一的一个坐标点。所有的不同的值的组合会形成一个损失函数值的矩阵，我们把矩阵中具有相同（相近）损失函数值的点连接起来，可以形成一个不规则椭圆，其圆心位置，是损失值为0的位置，也是我们要逼近的目标。

这个椭圆如同平面地图的等高线，来表示的一个洼地，中心位置比边缘位置要低，通过对损失函数的计算，对损失函数的求导，会带领我们沿着等高线形成的梯子一步步下降，无限逼近中心点。
## 4.均方差损失函数
函数就是最直观的一个损失函数了，计算预测值和真实值之间的欧式距离。预测值和真实值越接近，两者的均方差就越小。

均方差函数常用于线性回归(linear regression)，即函数拟合(function fitting)。

### 4.1公式

$$
loss = {1 \over 2}(z-y)^2 \tag{单样本}
$$

$$
J=\frac{1}{2m} \sum_{i=1}^m (z_i-y_i)^2 \tag{多样本}
$$
### 4.2工作原理

要想得到预测值a与真实值y的差距，最朴素的想法就是用$Error=a_i-y_i$。

对于单个样本来说，这样做没问题，但是多个样本累计时，$a_i-y_i$有可能有正有负，误差求和时就会导致相互抵消，从而失去价值。所以有了绝对值差的想法，即$Error=|a_i-y_i|$。

## 5.交叉熵损失函数
交叉熵（Cross Entropy）是Shannon信息论中一个重要概念，主要用于度量两个概率分布间的差异性信息。在信息论中，交叉熵是表示两个概率分布p,q的差异，其中p表示真实分布，q表示非真实分布，那么H(p,q)就称为交叉熵：

$$H(p,q)=\sum_i p_i \cdot log {1 \over q_i} = - \sum_i p_i \log q_i$$

交叉熵可在神经网络中作为损失函数，p表示真实标记的分布，q则为训练后的模型的预测标记分布，交叉熵损失函数可以衡量p与q的相似性。
**交叉熵函数常用于逻辑回归(logistic regression)，也就是分类(classification)。**

# 四.总结
    本次学习是我们第一次接触神经网络，讲述神经网络的基本概念。让我认识学习了神经网络基本的训练和工作原理，导数公式和反向传播公式，包括矩阵求导，反向传播和梯度下降。这次学习了拓宽了我对AI技术的视野，认识到了很多的东西。
    课程的知识点我发现很多都是数学问题，发现了数学对这门课会有很多的应用，所有课后我还得将我的数学补一补。其中我发现，梯度下降和损失函数对于神经网络很重要，也是两个难点。梯度下降是神经网络的基本学习方法，损失函数着重说明了神经网络中目前最常用的均方差损失函数（用于回归）和交叉熵损失函数（用于分类）。而这两个函数的应用课后需要多加练习才能很好的掌握。