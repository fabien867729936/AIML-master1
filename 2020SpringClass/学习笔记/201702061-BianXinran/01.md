# 人工智能与机器学习

## 深度学习入门
#### 9步学习法：
  基本概念
  线性回归
  线性分类
  非线性回归
  非线性分类
  模型的推理与应用部署
  深度神经网络
  卷积神经网络
  循环神经网络
## 课程概要
#### 一、人工智能
##### 1.人工智能发展简史
1.研究（包括技术）取得进展。
2.研究的进展让人们看到人工智能的潜力，产生非常乐观的期望
    例如在1958年到1970年间科学家对人工智能各种突破的预计，当然他们的绝大多数预计都太乐观了。
3.过高的期望让产业界开始热情地开发各种应用。
4.但应用未能全部满足期望，于是人工智能行业进入低谷
5.直到下一波研究和技术取得突破性进展。在2007年之后，是大规模的数据和廉价的计算能力，让神经网络技术再度兴起，成为AI领域的明星技术。

##### 2.人工智能的定义
从人们对人工智能的期待定义
从技术特点定义
##### 3.范式的演化
范式演化的四个阶段：
第一阶段：经验
第二阶段：理论
第三阶段：计算仿真
第四阶段：数据探索
##### 4.范式各阶段的应用
经验归纳
理论推导
数据模拟
数据探索


#### 二、神经网络
##### 1、神经网络的基本工作原理简介
神经元细胞的数学模型
神经网络的训练过程
神经网络中的矩阵运算
神经网络的主要功能
为什么需要激活函数
为什么需要深度神经网络与深度学习
Deep Learning的训练过程简介
##### 2.神经网络中的三个基本概念
反向传播
梯度下降
损失函数

反向传播与梯度下降的基本工作原理：
1.初始化；
2.正向计算；
3.损失函数为我们提供了计算损失的方法；
4.梯度下降是在损失函数基础上向着损失最小的点靠近而指引了网络权重调整的方向；
5.反向传播把损失值反向传给神经网络的每一层，让每一层都根据损失值反向调整权重；
6.goto 2，直到精度足够好（比如损失函数值小于0.001）
##### 3.线性反向传播
##### 4.非线性反向传播
##### 5.梯度下降
单变量函数的梯度下降
双变量函数的梯度下降

梯度下降的理解：
从自然现象中理解梯度下降
从数学中理解梯度下降

梯度下降三要素：
1.当前点
2.方向
3.步长

梯度下降含义：
梯度：函数当前位置的最快上升点；
下降：与导数相反的方向，用数学语言描述就是那个减号。与上升相反的方向运动，就是下降。

#### 三、损失函数
##### 1.损失函数的概念

损失函数的作用：
损失函数的作用，就是计算神经网络每次迭代的前向计算结果与真实值的差距，从而指导下一步的训练向正确的方向进行。

损失函数的使用：
1.用随机值初始化前向计算公式的参数；
2.代入样本，计算输出的预测值；
3.用损失函数计算预测值和标签值（真实值）的误差；
4.根据损失函数的导数，沿梯度最小方向将误差回传，修正前向计算公式中的各个权重值；
5.goto 2, 直到损失函数值达到一个满意的值就停止迭代。

##### 2.机器学习常用损失函数
绝对值损失函数
铰链/折页损失函数或最大边界损失函数
对数损失函数，又叫交叉熵损失函数
均方差损失函数
指数损失函数

##### 3.损失函数图像理解
用二维函数图像理解单变量对损失函数的影响
用等高线图理解双变量对损失函数影响

##### 4.神经网络中常用的损失函数
1.均方差函数，主要用于回归
2.交叉熵函数，主要用于分类
二者都是非负函数，极值在底部，用梯度下降法可以求解。

##### 5.均方差函数
工作原理
实际案例

##### 6.损失函数的可视化
损失函数值的3D示意图
损失函数值的2D示意图

##### 7.交叉熵损失函数
交叉熵的由来：
信息量
熵
相对熵(KL散度)
交叉
熵

##### 8.二分类问题交叉熵
##### 9.多分类问题交叉熵

    为什么不能使用均方差做为分类问题的损失函数？

    回归问题通常用均方差损失函数，可以保证损失函数是个凸函数，即可以得到最优解。而分类问题如果用均方差的话，损失函数的表现不是凸函数，就很难得到最优解。而交叉熵函数可以保证区间内单调。

    分类问题的最后一层网络，需要分类函数，Sigmoid或者Softmax，如果再接均方差函数的话，其求导结果复杂，运算量比较大。用交叉熵函数的话，可以得到比较简单的计算结果，一个简单的减法就可以得到反向误差。