# 第八章作业

## 题目

简要说明采用Seq2Seq模型实现机器翻译的原理

## 回答

### 一、Seq2Seq模型简介

Seq2Seq，全称Sequence to Sequence。它是一种通用的编码器——解码器框架，可用于机器翻译、文本摘要、会话建模、图像字幕等场景中。Seq2Seq并不是GNMT（Google Neural Machine Translation）系统的官方开源实现。框架的目的是去完成更广泛的任务，而神经机器翻译只是其中之一。在循环神经网络中我们了解到如何将一个序列转化成定长输出。在本文中，我们将探究如何将一个序列转化成一个不定长的序列输出（如机器翻译中，源语言和目标语言的句子往往并没有相同的长度）。

目前Seq2Seq模型在机器翻译，语音识别，文本摘要，问答系统等领域取得了巨大的成功。如图1所示，Seq2Seq其实就是Encoder-Decoder结构的网络，它的输入是一个序列，输出也是一个序列。在Encoder中，将序列转换成一个固定长度的向量，然后通过Decoder将该向量转换成我们想要的序列输出出来。

### 二、复杂一点的Seq2Seq模型

Teacher Forcing
在基础的模型中，Decoder的每一次解码又会作为下一次解码的输入，这样就会导致一个问题就是错误累计，如果其中一个RNN单元解码出现误差了，那么这个误差就会传递到下一个RNN单元，使训练结果误差越来越大。Teacher Forcing在一定程度上解决了这个问题，它的流程如图3所示，在训练过程中，使用要解码的序列作为输入进行训练，但是在inference阶段是不能使用的，因为你不知道要预测的序列是个啥，当然只在训练过程中效果就很不错了，它帮助模型加速收敛。
Attention
Attention是注意力机制，是2015年Bahdanau等人提出的，大概意思就是让Encoder编码出的c向量跟Decoder解码过程中的每一个输出进行加权运算，在解码的每一个过程中调整权重取到不一样的  c向量。
Beam Search
在inference阶段，不能使用Teacher Forcing，那么只能使用上一时刻解码的输出作为下一个解码的输入，刚才也说过了这样会导致误差传递，Beam Search可怎么解决这个问题。

### 三、原理

解码器部分的结构与语言模型几乎完全相同：输入为单词的词向量，输出为softmax层产生的单词概率，损失函数为 log perplexity。可以理解为是一个以输入编码为前提的语言模型（Conditional Language Model）。
编码器部分更为简单。与解码器一样拥有词向量层和循环神经网络，但是由于在编码阶段并未输出，因此不需要softmax层。
在训练过程中，编码器顺序读入每个单词的词向量，然后将最终的隐藏状态复制到解码器作为初始阶段。解码器的第一个输入是一个特殊的（Start-Of-Sentence）字符，每一步预测的单词时训练数据的目标句子，预测序列的最后一个单词是与语言模型相同的（End-Of-Sentence）字符。
