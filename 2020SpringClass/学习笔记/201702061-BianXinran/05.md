# 深度神经网络

## 搭建深度神经网络框架

### 回归任务功能测试

实验测试结果：
![](./14.1.png)

![](./14.11.png)


超参数说明：

1. 输入层1个神经元，因为只有一个x值
2. 隐层4个神经元，对于此问题来说应该是足够了，因为特征很少
3. 输出层1个神经元，因为是拟合任务
4. 学习率=0.5
5. 最大epoch=10000轮
6. 批量样本数=10
7. 拟合网络类型
8. Xavier初始化
9. 绝对损失停止条件=0.001


损失函数值在一段平缓期过后，开始陡降，这种现象在神经网络的训练中是常见的，最有可能的是当时处于一个梯度变化的平缓地带，算法在艰难地寻找下坡路，然后忽然就找到了。

## 回归任务功能真实案例

测试结果：
![](./14.2.png)
![](./14.22.png)

数据处理
原始数据只有一个数据集，所以需要我们自己把它分成训练集和测试集，比例大概为4:1。此数据集为csv文件格式，为了方便，我们把它转换成了两个扩展名为npz的numpy压缩形式：

1. house_Train.npz，训练数据集
2. house_Test.npz，测试数据集

搭建模型
在不知道一个问题的实际复杂度之前，把模型设计得复杂一些。这个模型包含了四组全连接层-Relu层的组合，最后是一个单输出做拟合。

## 二分类任务功能

![](./14.3.png)

超参数说明：

1. 输入层神经元数为2
2. 隐层的神经元数为3，使用Sigmoid激活函数
3. 由于是二分类任务，所以输出层只有一个神经元，用Logistic做二分类函数
4. 最多训练1000轮
5. 批大小=5
6. 学习率=0.1
7. 绝对误差停止条件=0.02

![](./14.32.png)

案例代码实现：

![](./14.4.png)

数据处理
数据分析和数据处理实际上是一门独立的课，超出类本书的范围，所以我们只做一些简单的数据处理，以便神经网络可以用之训练。
对于连续值，我们可以直接使用原始数据。对于枚举型，我们需要把它们转成连续值。以性别举例，Female=0，Male=1即可。对于其它枚举型，都可以用从0开始的整数编码。
一个小技巧是利用python的list功能，取元素下标，即可以作为整数编码：

```Python

sex_list = ["Female", "Male"]
array_x[0,9] = sex_list.index(row[9].strip())

```

strip()是trim掉前面的空格，因为是csv格式，读出来会是这个样子："_Female"，前面总有个空格。index是取列表下标，这样对于字符串"Female"取出的下标为0，对于字符串"Male"取出的下标为1。

把所有数据按行保存到numpy数组中，最后用npz格式存储：

```Python
np.savez(data_npz, data=self.XData, 
label=self.YData)
```

原始数据已经把train data和test data分开了，所以我们针对两个数据集分别调用数据处理过程一次，保存为Income_Train.npz和Income_Test.npz。

超参数说明：

1. 学习率=0.1
2. 最大epoch=100
3. 批大小=16
4. 二分类网络类型
5. MSRA初始化
6. 相对误差停止条件1e-3

net.train()函数是一个阻塞函数，只有当训练完毕后才返回。

## 多分类任务

### 搭建模型一

实验结果：

![](./14.5.png)
![](./14.52.png)

### 搭建模型二

实验结果：
![](./14.53.png)
![](./14.54.png)

比较：
可以看到第一个的边界要平滑许多，这也就是Relu和Sigmoid的区别，Relu是用分段线性拟合曲线，Sigmoid有真正的曲线拟合能力。但是Sigmoid也有缺点，看分类的边界，使用Relu函数的分类边界比较清晰，而使用Sigmoid函数的分类边界要平缓一些，过渡区较宽。
用一句简单的话来描述二者的差别：Relu能直则直，对方形边界适用；Sigmoid能弯则弯，对圆形边界适用。

### MNIST手写体识别

![](./14.56.png)
![](./14.55.png)

### 总结

深度神经网络可以理解为有很多隐藏层的神经网络，又被称为深度前馈网络，多层感知机。通过这次实验让我对深度神经网络有了更加清楚的认识。在运行代码的过程中，虽然也遇到了困难，但是通过查阅资料和帮助，最后得以解决，深度学习网络在机器学习中占有重要地位，可以说学好了深度神经网络就可以弄清深度学习的本质。这次试验和学习，让我有很大的收获。




