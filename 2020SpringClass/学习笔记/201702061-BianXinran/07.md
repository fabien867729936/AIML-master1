# 正则化

正则化的作用是什么？
正则化用于防止过拟合。
过拟合是什么？
所谓过拟合，就是模型在测试集上的表现要和训练集上一样好。模型过度拟合了训练数据，而不能反映真实世界的情况。解决过度拟合的手段和过程，就叫做泛化。

出现过拟合的原因：
1. 训练集的数量和模型的复杂度不匹配，样本数量级小于模型的参数
2. 训练集和测试集的特征分布不一致
3. 样本噪音大，使得神经网络学习到了噪音，正常样本的行为被抑制
4. 迭代次数过多，过分拟合了训练数据，包括噪音部分和一些非重要特征

解决过拟合问题；
1. 数据扩展
2. 正则
3. 丢弃法
4. 早停法
5. 集成学习法
6. 特征工程（属于传统机器学习范畴，不在此处讨论）
7. 简化模型，减小网络的宽度和深度

代码分析：
在main过程中，设置一些超参数，然后调用刚才建立的Net进行训练：

```Python
if __name__ == '__main__':

    dataReader = LoadData()
    num_feature = dataReader.num_feature
    num_example = dataReader.num_example
    num_input = num_feature
    num_hidden = 30
    num_output = 10
    max_epoch = 200
    batch_size = 100
    learning_rate = 0.1
    eps = 1e-5

    params = CParameters(
      learning_rate, max_epoch, batch_size, eps,
      LossFunctionName.CrossEntropy3, 
      InitialMethod.Xavier, 
      OptimizerName.SGD)

    Net(dataReader, num_input, num_hidden, num_hidden, num_hidden, num_hidden, num_output, params)
```

在超参数中指定了：

1. 每个隐层30个神经元（4个隐层在Net函数里指定）
2. 最多训练200个epoch
3. 批大小为100个样本
4. 学习率为0.1
5. 多分类交叉熵损失函数(CrossEntropy3)
6. Xavier权重初始化方法
7. 随机梯度下降算法

## L2正则

假设只有两个参数需要学习，那么这两个参数的损失函数就构成了上面的等高线图。由于样本数据量比较小（这是造成过拟合的原因之一），所以神经网络在训练过程中沿着箭头方向不断向最优解靠近，最终达到了过拟合的状态。也就是说在这个等高线图中的最优解，实际是针对有限的样本数据的最优解，而不是针对这个特点问题的最优解。

实验结果：
![](./16.2.png)
![](./16.22.png)

代码分析：
正则项在方向传播过程中，唯一影响的就是求W的梯度时，所以，我们可以修改FullConnectionLayer.py中的反向传播函数如下：

```Python
    def backward(self, delta_in, idx):
        dZ = delta_in
        m = self.x.shape[1]
        if self.regular == RegularMethod.L2:
            self.weights.dW = (np.dot(dZ, self.x.T) + self.lambd * self.weights.W) / m
        else:
            self.weights.dW = np.dot(dZ, self.x.T) / m
        # end if
        self.weights.dB = np.sum(dZ, axis=1, keepdims=True) / m

        delta_out = np.dot(self.weights.W.T, dZ)

        if len(self.input_shape) > 2:
            return delta_out.reshape(self.input_shape)
        else:
            return delta_out
```

当regular == RegularMethod.L2时，走一个特殊分支，完成正则项的惩罚机制。

## L1正则

实验结果：

![](./16.3.png)
![](./16.32.png)

从图上看，无论是损失函数值还是准确率，在训练集上都没有表现得那么夸张了，不会极高（到100%）或者极低（到0.001）。这说明过拟合的情况得到了抑制，而且准确率提高到了99.18%。
从输出结果分析：

权重值的绝对值和等于391.26，远小于过拟合时的1719
较小的权重值（小于0.01）的数量为22935个，远大于过拟合时的2810个
趋近于0的权重值（小于0.0001）的数量为12384个，大于过拟合时的25个。

代码分析：

py中的反向传播函数如下：

```Python
def backward(self, delta_in, idx):
    dZ = delta_in
    m = self.x.shape[1]
    if self.regular == RegularMethod.L2:
        self.weights.dW = (np.dot(dZ, self.x.T) + self.lambd * self.weights.W) / m
    elif self.regular == RegularMethod.L1:
        self.weights.dW = (np.dot(dZ, self.x.T) + self.lambd * np.sign(self.weights.W)) / m
    else:
        self.weights.dW = np.dot(dZ, self.x.T) / m
    # end if
    self.weights.dB = np.sum(dZ, axis=1, keepdims=True) / m
    ......
```

符号函数的效果如下：

```Python
a=np.array([1,-1,2,0])
np.sign(a)
array([ 1, -1,  1,  0])
```

当w为正数时，符号为正，值为1，相当于直接乘以w的值；当w为负数时，符号为负，值为-1，相当于乘以(-w)的值。最后的效果就是乘以w的绝对值。

## 