# 人工智能与机器学习

### 概念与编程实现

#### 一、梯度下降

1、梯度下降一是根据梯度（导数）的符号来判断最小值点x在哪，二是让函数值下降（变小）。
2、梯度就是导数（对于多维就是偏导数）。
3、梯度下降作用是找到函数的最小值所对应的自变量的值（曲线最低点对应x的值）。

简单来说，梯度下降是一种寻找目标函数最小化的方法。

在code里，我们把学习率定义为learning_rate，或者eta。当初始值为-0.8，学习率为1时，迭代不断在一条水平线上跳来跳去，永远也不能下降。
当学习率=0.8时，会有这种左右跳跃的情况发生，这不利于神经网络的训练。
当学习率=0.6时，也会有跳跃，幅度偏小。
当学习率=0.4时，损失值会从单侧下降，4步以后基本接近了理想值。
当学习率=0.2时，损失值会从单侧下降，但下降速度较慢，8步左右接近极值。
当学习率=0.1时，损失值会从单侧下降，但下降速度非常慢，10步了还没有到达理想状态。

用python代码实现结果如下：

![](./01.png)
![](./02.png)
![](./03.png)


#### 损失函数

损失函数一定要是连续的，这是寻找损失函数的第一个条件。此外，根本性的问题在于，我们想优化的根本不是经验风险，所谓的经验风险，如同上式，是定义在训练集上的样本上损失；我们真正想优化的是期望风险，定义在全部样本（包含测试集）的损失，用期望来表示：
minE[L(f(0,x)y)]
但是概率分布P(x,y)未知的话，我们连期望风险都无法计算，更无法将其最小化。但我们使用的数据越多，根据大数定律，期望风险也就越接近于经验风险，注意到，我们的联合分布还可以写为：
P(x,y)=P(x)P(y|x)
联合分布可以被拆为先验概率和条件概率。但是当我们可以假设模型的概率分布时，比如线性回归假设了高斯分布，logistic回归假设了伯努利分布，我们就可以利用极大似然估计来逼近期望风险，这也叫做一致性（consistency ），这个是我们寻找损失的函数第二个条件。
如果是连续的凸函数，在0处可导，且导数小于零，就具备与0-1损失函数的一致性，我们把这些损失函数叫做替代损失（Surrogate loss），值得注意的是，这是我们选用凸函数的最重要的原因，虽然凸函数具备局部最小值就是全局最小值的性质，但主要是为了计算上的便利，而非本质意义上的。

神经网络中常用的损失函数:
均方差函数，主要用于回归
交叉熵函数，主要用于分类
二者都是非负函数，极值在底部，用梯度下降法可以求解。

#### 三、交叉熵损失函数

交叉熵是信息论中的一个重要概念，主要用于度量两个概率分布间的差异性。

##### 信息量
信息是用来消除随机不确定性的东西”，也就是说衡量信息量的大小就是看这个信息消除不确定性的程度。
信息量的大小与信息发生的概率成反比。概率越大，信息量越小。概率越小，信息量越大。

##### 信息熵

信息熵也被称为熵，用来表示所有信息量的期望。
期望是试验中每次可能结果的概率乘以其结果的总和。

##### 相对熵（KL散度）
如果对于同一个随机变量有两个单独的概率分布P(x)和Q(x)，则我们可以使用KL散度来衡量这两个概率分布之间的差异。

##### 交叉熵
在机器学习训练网络时，输入数据与标签常常已经确定。那么真实概率分布P(x)也就确定下来了，所以信息熵在这里就是一个常量。由于KL散度的值表示真实概率分布P(x)与预测概率分布Q(x）之间的差异，值越小表示预测的结果越好，所以需要最小化KL散度，而交叉熵等于KL散度加上一个常量（信息熵），且公式相比KL散度更加容易计算，所以在机器学习中常常使用交叉熵损失函数来计算就行了。
—

交叉熵损失函数的代码实现：

![](./04.png)