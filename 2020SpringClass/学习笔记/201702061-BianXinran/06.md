# 网络优化

为了解决参数多、数据量大、梯度消失、损失函数坡度平缓等问题，科学家们在深入研究网络表现的前提下，发现在下面这些方向上经过一些努力，可以给深度网络的训练带来或多或少的改善：
权重矩阵初始化
批量归一化
梯度下降优化算法
自适应学习率算法

## 权重矩阵初始化

实验结果：

![](./15.1.png)
![](./15.12.png)
![](./15.13.png)

随着层的加深，使用ReLU时激活值逐步向0偏向，同样会导致梯度消失问题。于是He Kaiming等人研究出了MSRA初始化法，又叫做He初始化法。

![](./15.14.png)

激活值从0到1的分布，在各层都非常均匀，不会由于层的加深而梯度消失，所以，在使用ReLU时，推荐使用MSRA法初始化。

![](./15.15.png)

## 梯度下降优化算法

随机梯度下降算法，在当前点计算梯度，根据学习率前进到下一点。到中点附近时，由于样本误差或者学习率问题，会发生来回徘徊的现象，很可能会错过最优解。

实验结果：

![](./15.2.png)

![](./15.21.png)
![](./15.22.png)
![](./15.23.png)

使用同等的超参数设置，普通梯度下降算法经过epoch=10000次没有到达预定0.001的损失值；动量算法经过2000个epoch迭代结束。

在损失函数历史数据图中，中间有一大段比较平坦的区域，梯度值很小，或者是随机梯度下降算法找不到合适的方向前进，只能慢慢搜索。而下侧的动量法，利用惯性，判断当前梯度与上次梯度的关系，如果方向相同，则会加速前进；如果不同，则会减速，并趋向平衡。所以很快地就达到了停止条件。

![](./15.24.png)

![](./15.25.png)

## 自适应学习率算法

AdaGrad是一个基于梯度的优化算法，它的主要功能是：它对不同的参数调整学习率，具体而言，对低频出现的参数进行大的更新，对高频出现的参数进行小的更新。因此，他很适合于处理稀疏数据。
随着算法不断迭代，r会越来越大，整体的学习率会越来越小。所以，一般来说AdaGrad算法一开始是激励收敛，到了后面就慢慢变成惩罚收敛，速度越来越慢。

这是因为随着更新次数的增大，我们希望学习率越来越慢。因为我们认为在学习率的最初阶段，我们距离损失函数最优解还很远，随着更新次数的增加，越来越接近最优解，所以学习率也随之变慢。

但是当某个参数梯度较小时，累积和也会小，那么更新速度就大。

实验结果：

![](./15.3.png)
![](./15.32.png)
![](./15.33.png)
![](./15.34.png)
![](./15.35.png)
![](./15.36.png)
![](./15.37.png)

## 算法在等高线图上的效果比较

实验结果：

![](./15.4.png)
![](./15.42.png)
![](./15.43.png)
![](./15.44.png)
![](./15.45.png)
![](./15.46.png)
![](./15.47.png)

SGD当学习率为0.1时，需要很多次迭代才能逐渐向中心靠近。
SGD当学习率为0.5时，会比较快速地向中心靠近，但是在中心的附近有较大震荡。
由于惯性存在，一下子越过了中心点，但是很快就会得到纠正。
Nag是Momentum的改进，有预判方向功能。
AdaGrad的学习率在开始时可以设置大一些，因为会很快衰减。
AdaDelta即使把学习率设置为0，也不会影响，因为有内置的学习率策略。
RMSProp解决AdaGrad的学习率下降问题，即使学习率设置为0.1，收敛也会快。
Adam到达中点的路径比较直接。

![](./15.48.png)

训练轨迹：
SGD：在较远的地方，沿梯度方向下降，越靠近中心的地方，抖动得越多，似乎找不准方向，得到loss值等于0.005迭代了148次。
Momentum：由于惯性存在，一下子越过了中心点，但是很快就会得到纠正，得到loss值等于0.005迭代了128次。
RMSProp：与SGD的行为差不多，抖动大，得到loss值等于0.005迭代了130次。
Adam：与Momentum一样，越过中心点，但后来的收敛很快，得到loss值等于0.005迭代了107次。


SGD：接近中点的过程很曲折，步伐很慢，甚至有反方向的，容易陷入局部最优。
Momentum：快速接近中点，但中间跳跃较大。
RMSProp：接近中点很曲折，但是没有反方向的，用的步数比SGD少，跳动较大，有可能摆脱局部最优解的。
Adam：快速接近中点，难怪很多人喜欢用这个优化器。

## 批量归一化的原理

既然可以把原始训练样本做归一化，那么如果在深度神经网络的每一层，都可以有类似的手段，也就是说把层之间传递的数据移到0点附近，那么训练效果就应该会很理想。这就是批归一化BN的想法的来源。

深度神经网络随着网络深度加深，训练起来越困难，收敛越来越慢，这是个在DL领域很接近本质的问题。很多论文都是解决这个问题的，比如ReLU激活函数，再比如Residual Network。BN本质上也是解释并从某个不同的角度来解决这个问题的。

BN就是在深度神经网络训练过程中使得每一层神经网络的输入保持相同的分布，致力于将每一层的输入数据正则化成$N(0,1)$的分布。因次，每次训练的数据必须是mini-batch形式，一般取32，64等数值。

代码结果：

![](./15.5.png)
![](./15.52.png)
![](./15.53.png)
![](./15.54.png)
![](./15.55.png)
![](./15.56.png)

批量归一化的优点

1. 可以选择比较大的初始学习率，让你的训练速度提高。以前还需要慢慢调整学习率，甚至在网络训练到一定程度时，还需要想着学习率进一步调小的比例选择多少比较合适，现在我们可以采用初始很大的学习率，因为这个算法收敛很快。当然这个算法即使你选择了较小的学习率，也比以前的收敛速度快，因为它具有快速训练收敛的特性；
2. 减少对初始化的依赖。一个不太幸运的初始化，可能会造成网络训练实际很长，甚至不收敛。
3. 减少对正则的依赖。在第16章中，我们将会学习正则化知识，以增强网络的泛化能力。采用BN算法后，我们会逐步减少对正则的依赖，比如令人头疼的dropout、L2正则项参数的选择问题，或者可以选择更小的L2正则约束参数了，因为BN具有提高网络泛化能力的特性；

## 批量归一化的实现

### 初始化类

```Python
class BnLayer(CLayer):
    def __init__(self, input_size, momentum=0.9):
        self.gamma = np.ones((1, input_size))
        self.beta = np.zeros((1, input_size))
        self.eps = 1e-5
        self.input_size = input_size
        self.output_size = input_size
        self.momentum = momentum
        self.running_mean = np.zeros((1,input_size))
        self.running_var = np.zeros((1,input_size))
```

后面三个变量，momentum、running_mean、running_var，是为了计算/记录历史方差均差的。

### 正向计算

```Python
    def forward(self, input, train=True):
        assert(input.ndim == 2 or input.ndim == 4)  # fc or cv
        self.x = input

        if train:
            # 公式6
            self.mu = np.mean(self.x, axis=0, keepdims=True)
            # 公式7
            self.x_mu  = self.x - self.mu
            self.var = np.mean(self.x_mu**2, axis=0, keepdims=True) + self.eps
            # 公式8
            self.std = np.sqrt(self.var)
            self.norm_x = self.x_mu / self.std
            # 公式9
            self.z = self.gamma * self.norm_x + self.beta
            # mean and var history, for test/inference
            self.running_mean = self.momentum * self.running_mean + (1.0 - self.momentum) * self.mu
            self.running_var = self.momentum * self.running_var + (1.0 - self.momentum) * self.var
        else:
            self.mu = self.running_mean
            self.var = self.running_var
            self.norm_x = (self.x - self.mu) / np.sqrt(self.var + self.eps)
            self.z = self.gamma * self.norm_x + self.beta
        # end if
        return self.z
```

前向计算完全按照上一节中的公式6到公式9实现。要注意在训练/测试阶段的不同算法，用train是否为True来做分支判断。

### 更新参数

```Python
    def update(self, learning_rate=0.1):
        self.gamma = self.gamma - self.d_gamma * learning_rate
        self.beta = self.beta - self.d_beta * learning_rate
```

打印输出的最后几行如下：

```
......
epoch=4, total_iteration=4267
loss_train=0.079916, accuracy_train=0.968750
loss_valid=0.117291, accuracy_valid=0.967667
time used: 19.44783306121826
save parameters
testing...
0.9663
```

使用BN后，迭代速度提升，但是花费时间多了2秒，这是因为BN的正向和反向计算过程还是比较复杂的，需要花费一些时间，但是BN确实可以帮助网络快速收敛。如果使用GPU的话，花费时间上的差异应该可以忽略。

在准确率上的差异可以忽略，由于样本误差问题和随机初始化参数的差异，会造成最后的训练结果有细微差别。

### 总结

这一章节的主要内容是学习网络的优化。优化算法就是一种能够帮我们最小化或者最大化目标函数（有时候也叫损失函数）的一类算法。而目标函数往往是模型参数和数据的数学组合。为了实现网络的优化引出了许多种类的算法，这一章的学习让我们弄清楚了每一种算法的原理和优缺点。同时也横向对比了每一种算法适用于不同的场合。这一章我学会了很多，收获也很多。
