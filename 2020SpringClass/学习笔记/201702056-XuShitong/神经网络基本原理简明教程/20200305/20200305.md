>>>>># 卷积神经网络
+ 卷积神经网络
    + 卷积神经网络的能力:
        + 神经网络的类型之一，在图像识别和分类领域中取得了非常好的效果。
    + 卷积网络的典型结构：
        + ![](1.png)
        + 层级结构：
            + 原始的输入是一张图片，可以是彩色的，也可以是灰度的或黑白的。
            + 第一层卷积，我们使用了4个卷积核，得到了4张feature map；激活函数层没有单独画出来，这里我们紧接着卷积操作使用了Relu激活函数；
            + 第二层是池化，使用了Max Pooling方式，把图片的高宽各缩小一倍，但仍然是4个feature map；
            + 第三层卷积，我们使用了4x6个卷积核，其中4对应着输入通道，6对应着输出通道，从而得到了6张feature map，当然也使用了Relu激活函数；
            + 第四层再次做一次池化，现在得到的图片尺寸只是原始尺寸的四分之一左右；
            + 第五层把第四层的6个图片展平成一维，成为一个fully connected层；
            + 第六层再接一个小一些的fully connected层；
            + 最后接一个softmax函数，判别10个分类。
        + 在神经网络中，一定会有以下几个层：
            + 卷积层
            + 激活函数层
            + 池化层
            + 全连接分类层
    + 卷积核作用：
        + 锐化：如果一个像素点比周围像素点亮，则此算子会令其更亮
        + 检测竖边：检测出了十字线中的竖线，由于是左侧和右侧分别检查一次，所以得到两条颜色不一样的竖线
        + 周边:把周边增强，把同色的区域变弱，形成大色块
        + SObel-Y:纵向亮度差分可以检测出横边，与横边检测不同的是，它可以使得两条横线具有相同的颜色，具有分割线的效果
        + Identity:中心为1四周为0的过滤器，卷积后与原图相同
        + 横边检测：检测出了十字线中的横线，由于是上侧和下侧分别检查一次，所以得到两条颜色不一样的横线
        + 模糊：通过把周围的点做平均值计算而“杀富济贫”造成模糊效果
        + Sobel-X：横向亮度差分可以检测出竖边，与竖边检测不同的是，它可以使得两条竖线具有相同的颜色，具有分割线的效果
        + 浮雕：形成大理石浮雕般的效果
    + 卷积神经网络的学习
        + 在卷积-池化等一些列操作的后面，要接全连接层，这里的全连接层和我们在前面学习的深度网络的功能一模一样，都是做为分类层使用
        + 在最后一层的池化后面，把所有特征数据变成一个一维的全连接层，然后就和普通的深度全连接网络一样了，通过在最后一层的softmax分类函数，以及多分类交叉熵函数，对比图片的OneHot编码标签，回传误差值，从全连接层传回到池化层，通过激活函数层再回传给卷积层，对卷积核的数值进行梯度更新，实现卷积核数值的自我学习。
        + 平移不变性：卷积核的权值共享。但是特征处于不同的位置，由于距离差距较大，即使经过多层池化后，也不能处于近似的位置。此时，后续的全连接层会通过权重值的调整，把这两个相同的特征看作同一类的分类标准之一。如果是小距离的平移，通过池化层就可以处理了。
        + 旋转不变性：卷积网络对于小角度旋转是可以容忍的，但是对于较大的旋转，需要使用数据增强来增加训练样本。
        + 尺度不变性：这就是Inception的想法，用不同尺寸的卷积核去同时寻找同一张图片上的特征。
+ 运行结果：


### 卷积的前向计算：
+ 卷积的数学定义：
    + 连续定义：
        + $$h(x)=(f*g)(x) = \int_{-\infty}^{\infty} f(t)g(x-t)dt \tag{1}$$
    + 离散定义：
        + $$h(x) = (f*g)(x) = \sum^{\infty}_{t=-\infty} f(t)g(x-t) \tag{2}$$
    + 单入单出的二维卷积：二维卷积一般用于图像处理上
        + 在二位图片上做卷积，如果把图像Image简写为$I$，把卷积核Kernal简写为$K$，则目标图片的第$(i,j)$个像素的卷积值为：
            + $$ h(i,j) = (I*K)(i,j)=\sum_m \sum_n I(m,n)K(i-m,j-n) \tag{3} $$
        + 自相关函数和互相关函数的定义：
            + 自相关：设原函数是f(t)，则$h=f(t) \star f(-t)$，其中$\star$表示卷积
            + 互相关：设两个函数分别是f(t)和g(t)，则$h=f(t) \star g(-t)$。[是两个序列滑动相乘，两个序列都不翻转]
    + 单入多出的升维卷积：
        + 原始输入是一维的图片，但是我们可以用多个卷积核分别对其计算，从而得到多个特征输出
    + 多入单出的降维卷积：
        + 一张图片，通常是彩色的，具有红绿蓝三个通道。两个选择：
            + 变成灰度的，每个像素只剩下一个值，就可以用二维卷积
            + 对于三个通道，每个通道都使用一个卷积核，分别处理红绿蓝三种颜色的信息
    + 多入多出的同维卷积：
        + ![](2.png)
        + 第一个过滤器Filter-1为棕色所示，它有三卷积核(Kernal)，命名为Kernal-1，Keanrl-2，Kernal-3，分别在红绿蓝三个输入通道上进行卷积操作，生成三个2x2的输出Feature-1,n。然后三个Feature-1,n相加，并再加上b1偏移值，形成最后的棕色输出Result-1。对于灰色的过滤器Filter-2也是一样，先生成三个Feature-2,n，然后相加再加b2，最后得到Result-2。之所以Feature-m,n还用红绿蓝三色表示，是因为在此时，它们还保留着红绿蓝三种色彩的各自的信息，一旦相加后得到Result，这种信息就丢失了。
+ 卷及编程模型：
    + 五个概念关系：
        + 输入 Input Channel
        + 卷积核组 WeightsBias
        + 过滤器 Filter
        + 卷积核 kernal
        + 输出 Feature Map
    + 三维卷积的特点：
        + 预先定义输出的feature map的数量，而不是根据前向计算自动计算出来
        + 对于每个输出，都有一个对应的过滤器Filter
        + 每个Filter内都有一个或多个卷积核Kernal，对应每个输入通道(Input Channel)
        + 每个Filter只有一个Bias值
        + 卷积核Kernal的大小一般是奇数
    + 步长
    + 填充
+ 代码运算结果：
2


### 卷积层训练：同全连接层一样，卷积层的训练也需要从上一层回传的误差矩阵，然后计算：本层的权重矩阵的误差项；本层的需要回传到下一层的误差矩阵。
+ 计算反向传播的梯度矩阵
    + 正向公式：
        + $$Z = W*A+b \tag{0}$$（W是卷积核，*表示卷积（互相关）计算，A为当前层的输入项，b是偏移（未在图中画出），Z为当前层的输出项，但尚未经过激活函数处理。）
    + 最后统一成为一个简洁公式：
        + $$\delta_{out} = \delta_{in} * W^{rot180} \tag{8}$$
    + 当Weights是3x3时，$\delta_{in}$需要padding=2，即加2圈0，才能和Weights卷积后，得到正确尺寸的$\delta_{out}$
    + 当Weights是5x5时，$\delta_{in}$需要padding=4，即加4圈0，才能和Weights卷积后，得到正确尺寸的$\delta_{out}$
    + 以此类推：当Weights是NxN时，$\delta_{in}$需要padding=N-1，即加N-1圈0
+ 步长不为1时的梯度矩阵还原：
    + 以此类推，如果步长为3时，需要补一个双线的十字。所以，当知道当前的卷积层步长为S（S>1）时：
        + 得到从上层回传的误差矩阵形状，假设为$M \times N$
        + 初始化一个$(M \cdot S) \times (N \cdot S)$的零矩阵
        + 把传入的误差矩阵的第一行值放到零矩阵第0行的0,S,2S,3S...位置
        + 然后把误差矩阵的第二行的值放到零矩阵第S行的0,S,2S,3S...位置
        + ......
+  有多个卷积核时的梯度计算：有多个卷积核也就意味着有多个输出通道。
+  有多个输入时的梯度计算：当输入层是多个图层时，每个图层必须对应一个卷积核。
+  权重（卷积核）梯度计算：
      +  总结成一个公式：
        + $$\delta_w = A * \delta_{in} \tag{11}$$
+ 偏移的梯度计算：每个卷积核W可能会有多个filter，或者叫子核，但是一个卷积核只有一个偏移，无论有多少子核
+ 代码运行：Ch17 3
    + Level4_Col2Img_Test.py中有两个方法：
        + understand_4d_col2img_simple - 用单样本单通道理解反向传播
        + understand_4d_col2img_complex - 用多样本多通道理解反向传播
        + Level4_BackwardTest.py用来测试两种方法的性能。

### 池化的前向计算与反向传播
+ 池化层：
    + 常用池化方法：
        + 池化 pooling，又称为下采样，downstream sampling or sub-sampling。
        + 池化方法分为两种：
            + 一种是最大值池化 Max Pooling：是取当前池化视野中所有元素的最大值，输出到下一层特征图中。
            + 一种是平均值池化 Mean/Average Pooling：取当前池化视野中所有元素的平均值，输出到下一层特征图中。
        + 目的：
            + 扩大视野：就如同先从近处看一张图片，然后离远一些再看同一张图片，有些细节就会被忽略
            + 降维：在保留图片局部特征的前提下，使得图片更小，更易于计算
            + 平移不变性，轻微扰动不会影响输出：比如上如中最大值池化的4，即使向右偏一个像素，其输出值仍为4
            + 维持同尺寸图片，便于后端处理：假设输入的图片不是一样大小的，就需要用池化来转换成同尺寸图片
    + 池化的其他方式：
        + size=2x2，stride=2的模式，这是常用的模式
    + 池化层的训练：
        + 对于最大值池化，残差值会回传到当初最大值的位置上（请对照图14.3.1），而其它三个位置的残差都是0。
        + 对于平均值池化，残差值会平均到原始的4个位置上。
    + Max Pooling：
        + 正向公式：
            + $$w = max(a,b,e,f)$$
        + 反向公式（假设Input Layer中的最大值是b）：
            + $${\partial w \over \partial a} = 0$$ $${\partial w \over \partial b} = 1$$
            + $${\partial w \over \partial e} = 0$$ $${\partial w \over \partial f} = 0$$
    + Mean Pooling：
        + 正向公式：
            + $$w = \frac{1}{4}(a+b+e+f)$$
        + 反向公式（假设Layer-1中的最大值是b）：
            + $${\partial w \over \partial a} = \frac{1}{4}$$ $${\partial w \over \partial b} = \frac{1}{4}$$ $${\partial w \over \partial e} = \frac{1}{4}$$ $${\partial w \over \partial f} = \frac{1}{4}$$
    + 实现发放1：按照标准公式来实现池化的正向和反向代码。
    + 实现方法2：池化也有类似与卷积优化的方法来计算
+ 代码运算：
    + 5