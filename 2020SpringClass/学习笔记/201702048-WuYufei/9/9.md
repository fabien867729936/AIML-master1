## Step9 - RNN知识总结

学习具有两个时间步的DNN组成的简单RNN网络，用于拟合功能。

验证万能近似定理时有几点不同：

1. 不是连续值，而是时间序列的离散值
2. 完全随机的离散值，而不是满足一定的规律
3. 测试数据不在样本序列里，完全独立

使用DNN技术中曲线拟合技术得到了一个拟合网络，也不能正确地预测不在样本序列里的测试集数据。但是，我们可以把DNN做一个变形，让它能够处理时间序列数据：

<img src="C:/Users/Pangzi/Desktop/Mark/9/image/random_number_echo_net.png" width="600" />
图：两个时间步的DNN

图中含有两个简单的DNN网络，t1和t2，每个节点上都只有一个神经元，其中，各个节点的名称和含义是：

|名称|含义|在t1,t2上的取值|
|---|---|---|
|x|输入层样本|根据样本值|
|U|x到h的权重值|相同|
|h|隐层|不同|不同|
|bh|h节点的偏移值|相同|
|tanh|激活函数|函数相同|
|s|隐层激活状态|不同|
|V|s到z的权重值|相同|
|z|输出层|不同|
|bz|z的偏移值|相同|
|loss|损失函数|函数相同|
|y|标签值|根据标签值|

搭建多个时序的网络从DNN的结构扩展到含有4个时序的网络结构：

<img src="C:/Users/Pangzi/Desktop/Mark/9/image/binary_number_minus_net.png"/>
搭建环境  前向技术  反向传播  代码实现

### 长短时记忆网络的基本原理

循环神经网络的提出，使神经网络可以训练和解决带有时序信息的任务，大大拓宽了神经网络的使用范围。但是原始的RNN有明显的缺陷，不管是双向RNN，还是深度RNN，都有一个严重的缺陷：在训练过程冲经常会出现梯度爆炸和梯度消失的问题，以至于原始的RNN很难处理长距离的依赖。
### LSTM的误差项也是沿两个方向传播：
1. 沿时间的反向传播
2. 向上一层网络的反向传播