## Step3-LinearClassification知识总结

Logistic Regression，其原因是使用了线性回归中的线性模型。还有就是最简单的线性二分类开始学习，包括其原理，实现，训练过程，推理过程等等。逻辑非门的学习。还有就是Softmax函数的学习，从而理解神经网络的工作方式。

### 线性二分类

提出问题

我们经常看到中国象棋棋盘中，用楚河汉界分割开了两个阵营的棋子。回忆历史，公元前206年前后，楚汉相争，当时刘邦和项羽麾下的城池，在中原地区的地理位置示意图如下：

<img src="C:/Users/Pangzi/Desktop/Mark/3/image/binary_data.png" ch="500" />

利用逻辑回归模型解决此类问题。

### 二分类函数


- 公式

$$a(z) = \frac{1}{1 + e^{-z}}$$

- 导数

$$a^{'}(z) = a(z)(1 - a(z))$$

具体求导过程可以参考8.1节。

- 输入值域

$$(-\infty, \infty)$$

- 输出值域

$$(0,1)$$

- 函数图像

<img src="C:/Users/Pangzi/Desktop/Mark/3/image/logistic.png" ch="500" />

### 正向传播

#### 矩阵运算

$$
z=x \cdot w + b \tag{1}
$$

#### 分类计算

$$
a = Logistic(z)={1 \over 1 + e^{-z}} \tag{2}
$$

#### 损失函数计算

二分类交叉熵损失函数：

$$
loss(w,b) = -[y \ln a+(1-y)\ln(1-a)] \tag{3}
$$

### 反向传播

#### 求损失函数loss对a的偏导

$$
\frac{\partial loss}{\partial a}=-[{y \over a}+{-(1-y) \over 1-a}]=\frac{a-y}{a(1-a)} \tag{4}
$$

#### 求损失函数a对z的偏导

$$
\frac{\partial a}{\partial z}= a(1-a) \tag{5}
$$

#### 求损失函数loss对z的偏导

使用链式法则链接公式4和公式5：

$$
\frac{\partial loss}{\partial z}=\frac{\partial loss}{\partial a}\frac{\partial a}{\partial z}
$$
$$
=\frac{a-y}{a(1-a)} \cdot a(1-a)=a-y \tag{6}
$$
满足以下条件：
1. 损失函数满足二分类的要求，无论是正例还是反例，都是单调的；
2. 损失函数可导，以便于使用反向传播算法；
3. 让计算过程非常简单，一个减法就可以搞定。

### 二分类过程

1. 正向计算

2. 分类计算

3. 损失函数计算

### 线性多分类

多分类问题的解法

一对一

<img src="C:/Users/Pangzi/Desktop/Mark/3/image/one_vs_one.png" />

多对多

