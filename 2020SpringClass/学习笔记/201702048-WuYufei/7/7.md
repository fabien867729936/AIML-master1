## Step7-DNN知识总结
本章节主要是对深度学习的一些细节，如权重矩阵初始化、梯度下降优化算法、批量归一化等高级知识。
### 搭建深度神经网络框架
神经网络结构和功能：

- Layers - 神经网络各层的容器，按添加顺序维护一个列表
- Parameters - 基本参数，包括普通参数和超参
- Loss Function - 提供计算损失函数值，存储历史记录并最后绘图的功能
- LayerManagement() - 添加神经网络层
- ForwardCalculation() - 调用各层的前向计算方法
- BackPropagation() - 调用各层的反向传播方法
- PreUpdateWeights() - 预更新各层的权重参数
- UpdateWeights() - 更新各层的权重参数
- Train() - 训练
- SaveWeights() - 保存各层的权重参数
- LoadWeights() - 加载各层的权重参数
### 回归任务功能测试

回归实验-万能近似定理

一个双层的神经网络，第一层后面接一个Sigmoid激活函数，第二层直接输出拟合数据：<img src="C:/Users/Pangzi/Desktop/Mark/7/image/ch09_net.png" />

### 直观理解反向传播四大公式
<img src="C:/Users/Pangzi/Desktop/Mark/7/image/bp.png" />

### 网络优化
原因是：
- 参数多
- 数据量大
- 梯度消失
- 损失函数坡度平缓

改善措施：
- 权重矩阵初始化
Xavier初始化方法
条件：正向传播时，激活值的方差保持不变；反向传播时，关于状态值的梯度的方差保持不变。
Xavier初始化方法比直接用高斯分布进行初始化W的优势所在： 

一般的神经网络在前向传播时神经元输出值的方差会不断增大，而使用Xavier等方法理论上可以保证每层神经元输入输出方差一致。
MSRA初始化方法
条件：正向传播时，状态值的方差保持不变；反向传播时，关于激活值的梯度的方差保持不变。

|ID|网络深度|初始化方法|激活函数|说明|
|---|---|---|---|---|
|1|单层|Zero|无|可以|
|2|双层层|Zero|Sigmoid|错误，不能进行正确的反向传播|
|3|双层|Normal|Sigmoid|可以|
|4|多层|Normal|Sigmoid|激活值分布成凹形，不利于反向传播|
|5|多层|Xavier|Sigmoid|正确|
|6|多层|Xavier|Relu|激活值分布偏向0，不利于反向传播|
|7|多层|MSRA|Relu|正确|

matplotlib的绘图知识：
1. 确定x_axis值的范围：w = np.arange(1,3,0.01)，因为w的准确值是2
2. 确定y_axis值的范围：b = np.arange(2,4,0.01)，因为b的准确值是3
3. 生成网格数据：W,B = np.meshgrid(w, b)
4. 计算每个网点上的损失函数值Z
5. 所以(W,B,Z)形成了一个3D图，最后用ax.coutour(W,B,Z)来绘图
6. levels参数是控制等高线的精度或密度，norm控制颜色的非线性变化

- 批量归一化

<img src="C:/Users/Pangzi/Desktop/Mark/7/image/bn6.png" ch="500" />

1. 数据在训练过程中，在网络的某一层会发生Internal Covariate Shift，导致数据处于激活函数的饱和区；
2. 经过均值为0、方差为1的变换后，位移到了0点附近。但是只做到这一步的话，会带来两个问题：
   
   a. 在[-1,1]这个区域，Sigmoid激活函数是近似线性的，造成激活函数失去非线性的作用；
   
   b. 在二分类问题中我们学习过，神经网络把正类样本点推向了右侧，把负类样本点推向了左侧，如果再把它们强行向中间集中的话，那么前面学习到的成果就会被破坏；

3. 经过$\gamma、\beta$的线性变换后，把数据区域拉宽，则激活函数的输出既有线性的部分，也有非线性的部分，这就解决了问题a；而且由于$\gamma、\beta$也是通过网络进行学习的，所以以前学到的成果也会保持，这就解决了问题b。
- 梯度下降优化算法
通过手工推导迭代，我们得到两个结论：

1. 可以看到两种方式的第9步结果是相同的，即公式(1)(2)等同于(3)(4)
2. 与普通SGD的算法$W_3 = W_2 - \eta dW_2$相比，动量法不但每次要减去当前梯度，还要减去历史梯度$W_0、W_1$乘以一个不断减弱的因子$\alpha$，因为$\alpha$小于1，所以$\alpha^2$比$\alpha$小，$\alpha^3$比$\alpha^2$小。这种方式的学名叫做指数加权平均。
- 自适应学习率算法
RMSprop也将学习率除以了一个指数衰减的衰减均值。为了进一步优化损失函数在更新中存在摆动幅度过大的问题，并且进一步加快函数的收敛速度，RMSProp算法对权重W和偏置b的梯度使用了微分平方加权平均数。