## Step4 - NonLinearRegression学习总结

通过两层神经网络的学习，从而解决非线性问题。中间需要激活函数连接且加入非线性因素，提高神经网络能力。

### 激活函数
激活函数的基本性质：

+ 非线性：线性的激活函数和没有激活函数一样
+ 可导性：做误差反向传播和梯度下降，必须要保证激活函数的可导性
+ 单调性：单一的输入会得到单一的输出，较大值的输入得到较大值的输出
<img src="C:/Users/Pangzi/Desktop/Mark/4/image/step.png" ch="500" />

1. 神经网络最后一层不需要激活函数

2. 激活函数只用于连接前后两层神经网络

#### 挤压型激活函数
##### 公式
$$a(z) = \frac{1}{1 + e^{-z}}$$
##### 数值计算：

第一步，求出当前输入的值

第二步，求出当前梯度

第三步，根据梯度更新当前输入值

第四步，判断当前函数值是否接近0.5

第五步，重复步骤2-3直到当前函数值接近0.5
如图，

<img src="C:/Users/Pangzi/Desktop/Mark/4/image/decay_sigmoid.png" ch="500" />

#### 半线性激活函数

Rectified Linear Unit，修正线性单元，线性整流函数，斜坡函数。

##### 公式

$$a(z) = max(0,z) = \begin{Bmatrix} 
  z & (z \geq 0) \\ 
  0 & (z < 0) 
\end{Bmatrix}$$

##### 导数

$$a'(z) = \begin{cases} 1 & z \geq 0 \\ 0 & z < 0 \end{cases}$$

##### 值域

输入值域：$[-\infty, \infty]$

输出值域：$[0,\infty]$

导数值域：$[0,1]$

<img src="C:/Users/Pangzi/Desktop/Mark/4/image/relu.png" ch="500" />

##### 优点

- 反向导数恒等于1，更加有效率的反向传播梯度值，收敛速度快
- 避免梯度消失问题
- 计算简单，速度快
- 活跃度的分散性使得神经网络的整体计算成本下降

##### 缺点

无界。

梯度很大的时候可能导致的神经元“死”掉。
### 非线性回归的工作原理
多项式回归法，它成功地用于正弦曲线和复合函数曲线的拟合，其基本工作原理是把单一特征值的高次方做为额外的特征值加入，使得神经网络可以得到附加的信息用于训练。
多项式回归方法的示意图：

<img src="C:/Users/Pangzi/Desktop/Mark/4/image/polynomial_concept.png" ch="506" />

神经网络的非线性拟合工作原理

1. 隐层把x拆成不同的特征，根据问题复杂度觉得神经元数量；
2. 隐层做一次激活函数的非线性变换；
3. 输出层使用多变量线性回归，把隐层的输出当作输入特征值，再做一次线性变换，得出拟合结果。

比较多项式回归和双层神经网络解法

||多项式回归|双层神经网络|
|---|---|---|
|增加特征方式|特征值的高次方|线性变换拆分|
|特征值数量级|高几倍的数量级|数量级与原特征值相同|
|训练效率|低，需要迭代次数多|高，比前者少好几个数量级|