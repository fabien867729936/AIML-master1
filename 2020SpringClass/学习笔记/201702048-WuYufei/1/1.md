## Step1-BasicKnowledge知识总结

初步接触神经网络，了解了神经网络基本的训练和工作原理，以及对导数公式和反向传播公式的学习，包括矩阵求导。还有反向传播和梯度下降的学习。梯度下降是神经网络的基本学习方法。最后是对损失函数的学习，着重说明了神经网络中目前最常用的均方差损失函数（用于回归）和交叉熵损失函数（用于分类）。


### 前期准备

  环境：
  
  - Windows 10 version 1809
  - Visual Studio 2017 Community or above
  - Python 3.6.6
  - Jupyter Notebook (可选)
  
### 一、深度学习的方法
1. 基本概念
2. 线性回归
3. 线性分类
4. 非线性回归
5. 非线性分类
6. 模型的推理与应用部署
7. 深度神经网络
8. 卷积神经网络
9. 循环神经网络

### 二、神经网络基本的训练和原理

#### 神经元细胞的数学模型
<img src="C:/Users/Pangzi/Desktop/Mark/1/image/NeuranCell.png" ch="500" />

#### 输入 input

#### 权重 weights

#### 偏移 bias

#### 求和计算 sum

#### 激活函数 activation

<img src="C:/Users/Pangzi/Desktop/Mark/1/image/activation.png" />

### 三、导数公式和反向传播公式的学习

#### 1.基本函数及其导数

|公式序号|函数|导数|备注|
|---|---|---|---|
|1|$y=c$|$y'=0$|
|2|$y=x^a$|$y'=ax^{a-1}$|
|3|$y=log_ax$|$y'=\frac{1}{x}log_ae=\frac{1}{xlna}$|
|4|$y=lnx$|$y'=\frac{1}{x}$|
|5|$y=a^x$|$y'=a^xlna$|
|6|$y=e^x$|$y'=e^x$|
|7|$y=e^{-x}$|$y'=-e^{-x}$|
|8|$y=sin(x)$|$y'=cos(x)$|正弦函数|
|9|$y=cos(x)$|$y'=-sin(x)$|余弦函数|
|10|$y=tg(x)$|$y'=sec^2(x)=\frac{1}{cos^2x}$| 正切函数 |
|11|$y=ctg(x)$|$y'=-csc^2(x)$| 余切函数 |
|12|$y=arcsin(x)$|$y'=\frac{1}{\sqrt{1-x^2}}$| 反正弦函数 |
|13|$y=arccos(x)$|$y'=-\frac{1}{\sqrt{1-x^2}}$| 反余弦函数 |
|14|$y=arctan(x)$|$y'=\frac{1}{1+x^2}$| 反正切函数 |
|15|$y=arcctg(x)$|$y'=-\frac{1}{1+x^2}$| 反余切函数 |
|16|$y=sinh(x)=(e^x-e^{-x})/2$|$y'=cosh(x)$|双曲正弦函数 |
|17|$y=cosh(x)=(e^x+e^{-x})/2$|$y'=sinh(x)$|双曲余弦函数 |
|18|$y=tanh(x)=(e^x-e^{-x})/(e^x+e^{-x})$|$y'=sech^2(x)=1-tanh^2(x)$|双曲正切函数|
|19|$y=coth(x)=(e^x+e^{-x})/(e^x-e^{-x})$|$y'=-csch^2(x)$|双曲余切函数|
|20|$y=sech(x)=2/(e^x+e^{-x})$|$y'=-sech(x)*tanh(x)$|双曲正割函数|
|21|$y=csch(x)=2/(e^x-e^{-x})$|$y'=-csch(x)*coth(x)$| 双曲余割函数|

#### 2.导数四则运算

$$[u(x) + v(x)]' = u'(x) + v'(x) \tag{30}$$
$$[u(x) - v(x)]' = u'(x) - v'(x) \tag{31}$$
$$[u(x)*v(x)]' = u'(x)*v(x) + v'(x)*u(x) \tag{32}$$
$$[\frac{u(x)}{v(x)}]'=\frac{u'(x)v(x)-v'(x)u(x)}{v^2(x)} \tag{33}$$
#### 3.偏导数
#### 4.复合函数求导（链式法则）
#### 5.矩阵求导
#### 6.标量对矩阵导数的定义
#### 7.反向传播与梯度下降的基本工作原理：
##### a. 初始化
##### b. 正向计算
##### c. 损失函数为我们提供了计算损失的方法
##### d. 梯度下降是在损失函数基础上向着损失最小的点靠近而指引了网络权重调整的方向
##### e. 反向传播把损失值反向传给神经网络的每一层，让每一层都根据损失值反向调整权重
##### f. goto 2，直到精度足够好（比如损失函数值小于0.001）
### 四、神经网络的基本学习方法
#### 1.梯度下降的数学公式

$$\theta_{n+1} = \theta_{n} - \eta \cdot \nabla J(\theta) \tag{1}$$

其中：
- $\theta_{n+1}$：下一个值
- $\theta_n$：当前值
- $-$：梯度的反向
- $\eta$：学习率或步长，控制每一步走的距离，不要太快以免错过了最佳景点，不要太慢以免时间太长
- $\nabla$：梯度，函数当前位置的最快上升点
- $J(\theta)$：函数
#### 2.梯度下降的三要素

##### a. 当前点
##### b. 方向
##### c. 步长
### 五、损失函数的学习
具体步骤：

a. 用随机值初始化前向计算公式的参数
b. 代入样本，计算输出的预测值
c. 用损失函数计算预测值和标签值（真实值）的误差
d. 根据损失函数的导数，沿梯度最小方向将误差回传，修正前向计算公式中的各个权重值
e. goto 2, 直到损失函数值达到一个满意的值就停止迭代
- 均方差函数，主要用于回归

- 交叉熵函数，主要用于分类
二者都是非负函数，极值在底部，用梯度下降法可以求解。